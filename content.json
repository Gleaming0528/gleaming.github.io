{"meta":{"title":"小管的blog","subtitle":"","description":"","author":"gleaming","url":"https://gleaming.cn","root":"/"},"pages":[{"title":"所有分类","date":"2022-07-28T08:45:23.901Z","updated":"2022-07-28T07:52:51.995Z","comments":true,"path":"categories/index.html","permalink":"https://gleaming.cn/categories/index.html","excerpt":"","text":""},{"title":"公开的秘密","date":"2022-07-28T08:45:23.899Z","updated":"2022-07-28T07:52:51.995Z","comments":true,"path":"about/index.html","permalink":"https://gleaming.cn/about/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2022-07-28T08:45:23.901Z","updated":"2022-07-28T07:52:51.996Z","comments":true,"path":"friends/index.html","permalink":"https://gleaming.cn/friends/index.html","excerpt":"","text":"如何添加友链 请先添加本站链接 名称：Gleaming 链接：https://gleaming.cn 头像：https://oss.gleaming.cn/shige.jpg 描述：`` 标签：Java，ACM 博客截图（若有）：https://7.dusays.com/2020/12/18/b7bc8a721998f.png 下方评论区按此格式申请友链 1 2 3 4 5 6 COPY- title: # 名称 avatar: # 头像 url: # 链接 screenshot: # 截图 keywords: # 关键词 description: # 描述（可选） 为了提高图片加载速度，建议优化图片尺寸： 将自己的头像图片尺寸调整到 96px。 将压缩后的图片上传到图床并使用此图片链接作为头像。 重复上述步骤，压缩网站截图并把尺寸调整到 540x360 以下。 等待本站添加贵站 友链原则 合法的、非营利性、无商业广告站点。 有实质性原创内容的个人博客或组织。 必须是 HTTPS 独立域名。 没有特殊要求，只要是 符合大众道德观念、符合普适价值观 的网站我们都欢迎！ 互添成功 已经添加的友链不会轻易删除。如果您已经移除本站，本站也将移除友链 感谢支持，鞠躬🙇"},{"title":"所有标签","date":"2022-07-28T08:45:23.900Z","updated":"2022-07-28T07:52:51.996Z","comments":true,"path":"tags/index.html","permalink":"https://gleaming.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubernetes总结","slug":"Kubernetes总结","date":"2022-07-28T08:01:35.000Z","updated":"2022-07-28T08:11:32.169Z","comments":true,"path":"kubernetes-summary/","link":"","permalink":"https://gleaming.cn/kubernetes-summary/","excerpt":"","text":"概述微服务架构是替代以单个进程或几个进程运行在服务器上为部署方式的一种架构，它将单体应用分解成若干个可独立运行的组件。微服务的解耦性确保他们可以独立开发、部署、升级、伸缩。 如何部署、管理这些微服务，并充分利用宿主机的硬件资源，诞生了K8S。它将人员分为开发人员和系统管理员，系统管理员负责处理硬件、集群相关的事务，开发人员只需要提交自己的应用和描述，K8S就会按照描述，把应用启动起来，并暴露定义的接口。 一、Kubernetes介绍1.1 容器技术虚拟机可以隔离不同的微服务环境，Linux容器技术也可以。容器和虚拟机相比开销要小得多，容器里运行的进程实际上运行在宿主机上，但是和其他进程隔离，开销仅是容器消耗的资源。 容器使用两个机制来隔离运行在同一个操作系统上的多个进程 Linux命名空间：可以在某个命名空间运行一个进程，进程只能看到这个命名空间下的资源。当然，会存在多种类型的命名空间，所以一个进程不单单只属于某一个命名空间，而属于每个类型的一个命名空间。 内核的cgroups：限制进程能使用的资源量(CPU、内存、网络带宽等)不能超过被分配的量。 1.2 Kubernetes架构 主节点 主节点承载着K8S控制和管理整个系统的控制面板。控制面板的组件持有并控制集群状态，但它们不运行应用，应用由工作节点运行。 API服务器：应用和其他控制面板组件都要和它通信 Scheduler：调度应用 Controller Manager：执行集群级别的功能，如复制组件、秩序跟踪工作节点，处理失败节点 etcd：分布式数据存储，持久化存储集群配置 工作节点 运行用户部署的应用 Docker、rkt或其他容器类型 Kubelet：与API服务器通信，管理它所在节点的容器 kube-proxy：负责组件之间的负载均衡网络流量 K8S采用声明式的控制流，所有的资源声明都保存在etcd，所有的组件通过API Server来声明或监听。 二、Pod类型 自主式Pod 控制器管理的Pod 字段1234567891011apiVersion: v1kind: Podmetadata: name: kubia-manualspec: containers: - image: luksa/kubia name: kubia ports: - containerPort: 8080 protocol: TCP 在k8s中，一般使用yaml格式文件来创建符合我们期望的Pod，常用字段如下 参数名 字段类型 说明 version String K8S API版本 kind String yaml文件定义的资源类型和角色，例如：Pod metadata Object 元数据对象， metadata.name String 元数据对象的名字 metadata.namespace String 元数据对象的命名空间 Spec Object 详细定义对象 spec.containers[] list Spec对象的容器列表定义 spec.containers[].name String 定义容器名字 spec.containers[].image String 定义容器镜像 spec.containers[].imagePullPolicy String 定义镜像拉去策略：Always，Never，IfNotPresent spec.containers[].command[] List 指定容器启动命令 spec.containers[].args[] List 指定容器启动命令参数 spec.containers[].workingDir String 指定容器的工作目录 spec.containers[].volumeMounts[] List 指定容器内部的存储卷配置 spec.containers[].volumeMounts[].name String 指定可以被容器挂载的存储卷的名称 spec.containers[].volumeMounts[].mountPath String 指定可以被容器挂载的存储卷的路径 spec.containers[].volumelMounts[].readOnly String 默认为读写模式 spec.containers[].ports[] List 指定容器需要用到的端口列表 spec.containers[].ports[].name String 指定端口名称 spec.containers[].ports[].containerPort String 指定容器需要监听的端口号 spec.containers[].ports[].hostPort String 指定容器所在主机需要监听的端口号，默认跟上面containerPort相同,注意设置了hostPort spec.containers[].ports[].protocol String 指定端口协议，支持TCP和UDP，默认值为TCP spec.containers[].env[] List 指定容器运行前需设置的环境变量列表 spec.containers[].env[].name String 指定环境变量名称 spec.containers[].env[].value String 指定环境变量值 spec.containers[].resources Object 设置容器的资源上限 spec.containers[].resources.limits Object 指定设置容器运行时资源的运行上限 spec.containers[].resources.requests Object 指定容器启动和调度时的限制设置 spec.restartPolicy String 定义Pod的重启策略，可选值为Always、OnFailure、Never，默认值为Always. spec.nodeSelector Object 定义Node的Label过滤标签，以key:value格式指定 spec.imagePullSecrets Object 定义pul镜像时使用secret名称,以name:secretkey格式指定 spec.hostNetwork Boolean 定义是否使用主机网络模式,默认值为false。设置true表示使用宿主机网络,不使用docker网桥，同时设置了true将无法在同一台宿主机上启动第二个副本。 控制器 ReplicationController 用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代；而如果出现多余的异常容器也会被自动回收。新版本的K8S建议市容RS来替代RC ReplicaSet和RC没有本质的不同，只是名字不一样，RS支持集合式的selector Deployment为Pod和RS提供了一个声明式定义方法，用来替代以前的RC来方便管理应用。典型的应用场景包括： 定义Deployment来创建Pod和RS 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment Horizontal Pod Auto scaling仅适用于Deployment和RS，在V1版本中仅支持根据Pod的CPU利用率扩容，在V1alpha版本中，支持根据内存和用户自定义的metric扩缩容 StatefulSet为了解决有状态服务的问题，其应用场景包括 稳定的持久化存储，即Pod重新调度后还能访问到相同的持久化数据 稳定的网络标识，即Pod重新调度后其Podname和HostName不变 有序部署，有序扩展，即Pod是有顺序的，部署和扩展的时候根据定义的顺序依次进行（0~n-1） 有序收缩，有序删除（n-1~0） DaemonSet确保全部（或者一些）Node上运行一个Pod的副本，当有Node加入集群时，也会为他们创建一个Pod。当有Node从集群中移除时，Pod会收回。删除DaemonSet也会删除它创建的Pod。用法 在每个Node上运行日志收集daemon 在每个Node上运行监控daemon Job负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束 Cron Job管理基于时间Job，即周期性的运行Job 探针探针是由kubelet对容器执行的定期诊断。要执行诊断，kubelet调用由容器实现。有三种类型的处理程序: ExecAction:在容器内执行指定命令。如果命令退出时返回码为0则认为诊断成功。 TCPSocketAction:对指定端口上的容器的IP地址进行TCP检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction:对指定的端口和路径上的容器的IP地址执行HTTP Get请求。如果响应状态码大于等于200且小于400，则诊断被认为是成功的 每次探测都将获得以下三种结果之一: 成功:容器通过了诊断。 失败:容器未通过诊断。 未知:诊断失败，因此不会采取任何行动 livenessProbe:指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其重启策略的影响。如果容器不提供存活探针，则默认状态为Success readinessProbe:指示容器是否准备好服务请求。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有Service的端点中删除该Pod 的IP地址。初始延迟之前的就绪状态默认为Failure。如果容器不提供就绪探针，则默认状态为Success 三、Servicepod会在node间被调度，一组功能相同的pod需要对外提供一个稳定地址，而service就是pod对外的门户。 Service会通过selector绑定多个pod，service通过clusterIP对外接收请求，然后分配给绑定的pod。 12345678910apiVersion: v1kind: Servicemetadata: name: kubiaspec: ports: - port: 80 # 该服务的可用端口 targetPort: 8080 # 服务将连接转发到的容器端口 selector: app: kubia # 具有app=kubia标签的pod都属于该服务 Service类型 ClusterIP:默认类型，自动分配一个仅Cluster内部可以访问的虚拟IP NodePort: 在 ClusterlP基础上为Service在每台机器上绑定一个端口，这样就可以通过:NodePort 来访问该服务 LoadBalancer:在NodePort的基础上，借助cloud provider创建一个外部负载均衡器，并将请求转发到: NodePort 服务并不是和pod直接相连的，service创建endpoint，并将流量导向endpoint endpoint包含了所有后端Pod的IP:Port对 Client -&gt; Service -&gt; Pod 为了实现以上的功能，主要需要以下几个组件的协同工作: apiserver 用户通过kubectl命令向apiserver发送创建service的命令，apiserver接收到请求后将数据存储到etcd中 kube-proxy kubernetes的每个节点中都有一个叫做kube-porxy的进程，这个进程负责感知service,pod的变化，并将变化的信息写入本地的iptables规则中 iptables使用NAT等技术将virtualIP的流量转至endpoint中 四、configMap和Secret4.1 configMapConfigMap功能在Kubernetes1.2版本中引入，许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息。ConfigMap API给我们提供了向容器中注入配置信息的机制，ConfigMap可以被用来保存单个属性，也可以用来保存整个配置文件或者JSON二进制大对象 1234kubectl create configmap my-config--from-file=foo.json //单独的文件--from-file=config-opts/ //完整的文件夹--from-literal=some=thing //字面量 使用 使用configMap来替代环境变量 可以通过envFrom属性字段将所有条目暴露作为环境变量。 传递 ConfigMap 条目作为命令行参数 以利用configMap条目初始化某个环境变量，然后再在参数字段中引用该环境变量，形如 “$ENV_VAR” 4.2 Secretsecret和configMap最大的区别在于，secrets用于保存敏感的数据。 secret的数据都不会被写入磁盘，而是挂载在内存盘中。 Secret的大小限于1MB。 Secret最好不要通过环境变量进入container（日志、报错、子进程继承环境等，可能导致泄密），建议使用secret volume来访问Secret 五、卷pod中的每个容器都有自己的独立文件系统，因为文件系统来自容器镜像。 k8s通过在pod中定义卷，使得存储持久化，和pod共享生命周期，而不会随着容器的重启消失。 emptyDir：用于存储临时数据的简单目录，可以设置为Memory medium，直接存储在内存中 hostPath：用于将目录从工作节点的文件系统挂载到pod中 gitRepo：通过git仓库的内容初始化的卷，不会实时同步，只有在Pod创建时同步 nfs：挂载到pod中的NFS共享卷 云磁盘 网络存储 k8s内部资源卷 configMap Secret downwardAPI PersistentVolume和PersistentVolumeClaim，将对存储的需求和底层的存储技术解耦；将底层存储技术的配置交给k8s集群管理员，而应用开发人员（即k8s使用者）只需要提需求即可 注：PersistentVolume是集群级别的资源，不受namespace限制 PersistentVolume(PV)是由管理员设置的存储，它是群集的一部分。就像节点是集群中的资源一样，PV也是集群中的资源。PV是Volume之类的卷插件，但具有独立于使用PV的Pod的生命周期。 PersistentVolumeClaim(PVC)是用户存储的请求。它与Pod相似。Pod消耗节点资源，PVC消耗PV 资源。Pod可以请求特定级别的资源(CPU和内存)。声明可以请求特定的大小和访问模式（例如，可以以读/写一次或只读多次模式挂载) 持久化卷声明的保护 PVC保护的目的是确保由pod 正在使用的PVC不会从系统中移除，因为如果被移除的话可能会导致数据丢失当启用PVC保护 alpha功能时，如果用户删除了一个pod 正在使用的PVC，则该PVC不会被立即删除。PVC的删除将被推迟，直到PVC 不再被任何pod使用 PV访问模式 ReadWriteonce——该卷可以被单个节点以读/写模式挂载 ReadOnlyMany——该卷可以被多个节点以只读模式挂载 ReadWriteMany——该卷可以被多个节点以读/写模式挂载 StorageClass管理员可以手动通过置备程序创建PV，或者直接创建对应的StorageClass，然后用户创建PVC时，会自动根据StorageClass的设置调用置备程序创建出可供使用的PV。 六、调度简介Scheduler是kubernetes的调度器，主要的任务是把定义的pod分配到集群的节点上。 公平:如何保证每个节点都能被分配资源 资源高效利用:集群所有资源最大化被使用 效率:调度的性能要好，能够尽快地对大批量的pod完成调度工作 灵活:允许用户根据自己的需求控制调度的逻辑 Sheduler是作为单独的程序运行的，启动之后会一直监听API Server,获取PodSpec.NodeName为空的pod，对每个pod都会创建一个binding,表明该pod应该放到哪个节点上 调度过程预选和优选 调度分为几个部分:首先是过滤掉不满足条件的节点，这个过程称为predicate ;然后对通过的节点按照优先级排序，这个是priority ;最后从中选择优先级最高的节点。如果中间任何一步骤有错误，就直接返回错误 Predicate有一系列的算法可以使用: PodFitsResources :节点上剩余的资源是否大于pod请求的资源 PodFitsHost :如果pod指定了NodeName,检查节点名称是否和NodeName匹配 PodFitsHostPorts :节点上已经使用的port是否和pod申请的port冲突 PodSelectorMatches :过滤掉和pod指定的label不匹配的节点 NoDiskConflict :已经mount的volume和pod指定的volume不冲突，除非它们都是只读 如果在predicate过程中没有合适的节点，pod 会一直在pending 状态，不断重试调度,直到有节点满足条件。 经过这个步骤，如果有多个节点满足条件,就继续priorities过程:按照优先级大小对节点排序 优先级由一系列键值对组成，键是该优先级项的名称，值是它的权重(该项的重要性)。这些优先级选项包括: LeastRequestedPriority :通过计算CPU和Memory的使用率来决定权重，使用率越低权重越高。换句话说，这个优先级指标倾向于资源使用比例更低的节点 BalancedResourceAllocation :节点上CPU和Memory使用率越接近，权重越高。这个应该和上面的一起使用，不应该单独使用 ImageLocalityPriority :倾向于已经有要使用镜像的节点，镜像总大小值越大，权重越高 通过算法对所有的优先级项目和权重进行计算,得出最终的结果 节点亲和性pod.spec.nodeAfinity preferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 键值运算关系 In：label的值在某个列表中 NotIn：label的值不在某个列表中 Gt：label的值大于某个值 Lt：label的值小于某个值 Exists：某个label存在 DoesNotExist：某个label不存在 Pod亲和性pod.spec.afinity.podAfinity/podAntiAfinity preferredDuringSchedulingIgnoredDuringExecution：软策略 requiredDuringSchedulingIgnoredDuringExecution：硬策略 Taint和TolerationTaint和toleration相互配合，可以用来避免pod被分配到不合适的节点上。每个节点上都可以应用一个或多个taint，这表示对于那些不能容忍这些taint的pod，是不会被该节点接受的。如果将toleration应用于pod上，则表示这些pod可以（但不要求）被调度到具有匹配taint的节点上 每个污点有一个key和value作为污点的标签，其中value可以为空，effect描述污点的作用。当前taintefect支持如下三个选项： NoSchedule：表示k8s将不会将Pod调度到具有该污点的Node上 PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上 NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去 设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。但我们可以在Pod上设置容忍( Toleration )，意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上 指定调度节点 Pod.spec.nodeName将Pod直接调度到指定的Node节点上，会跳过Scheduler的调度策略，该匹配规则是强制匹配 Pod.spec.nodeSelector：通过kubernetes的label-selector机制选择节点，由调度器调度策略匹配label，而后调度Pod到目标节点，该匹配规则属于强制约束","categories":[{"name":"K8S","slug":"K8S","permalink":"https://gleaming.cn/categories/K8S/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://gleaming.cn/tags/Go/"}]},{"title":"春招:数据库","slug":"春招-数据库","date":"2022-03-03T09:25:48.000Z","updated":"2022-07-28T07:52:51.995Z","comments":true,"path":"databese/","link":"","permalink":"https://gleaming.cn/databese/","excerpt":"","text":"数据库知识点ACID原子性：事务被视为不可分割的最小单元，事务的所有操作要么全部提交成功，要么全部回滚。 一致性：数据库在事务执行前后都保持一致性状态。在一致性状态下，所有事务对一个数据的读取结果都是相同的。 隔离性：一个事务所做的修改在最终提交以前，对其他事务是不可见的。 持久性：一旦事务提交，则其所做的修改将永远保存到数据库中。其实系统发生崩溃，事务执行的结果也不能丢失。 隔离级别未提交读，提交读，可重复读，可串行化。 MySQL存储引擎InnoDB默认事务型存储引擎，实现了四个标准的隔离级别，默认级别为可重复读。在可重复读隔离级别下，通过MVCC+间隙锁防止幻影读。 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。 内部做了很多优化，从磁盘读取数据时采用的可预测性读，能够加快读操作并且自动创建自适应哈希索引，能够加速插入操作的插入缓冲区。 支持真正的在线热备份， 索引(B+树)1.数据结构B tree指的是 Balance Tree，也就是平衡树。平衡树是一颗查找树，并且所有叶子节点位于同一层。 B+ Tree 是基于B Tree 和叶子节点顺序访问指针进行实现，它具有B Tree的平衡性，并且通过顺序访问指针来提高区间查询的性能。 在B+ Tree中，一个节点中的key从左到右非递减排列，如果某个指针的左右相邻key分别是$key_i$和$key_{i+1}$，且不为null，则该指针指向节点的所有key大于等于$key_i$且小于等于$key_{i+1}$ 2.操作进行查找操作时，首先在根节点进行二分查找，找到一个 key 所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出 key 所对应的 data。 插入删除操作记录会破坏平衡树的平衡性，因此在插入删除操作之后，需要对树进行一个分裂、合并、旋转等操作来维护平衡性。 3.与红黑树的比较红黑树等平衡树也可以用来实现索引，但是文件系统及数据库系统普遍采用B+ Tree作为主要有以下两个原因。 更少的查找次数 平衡树查找操作的时间复杂度等于树高h，而树高大致为$O(h)=O(log_{d}N)$,其中d为每个节点的出度。 红黑树的出度为2，而B+ Tree的出度一般都非常大，所以红黑书的树高h很明显比B+ Tree大非常多，检索的次数也就更多。 利用计算机预读特性 为了减少磁盘I/O，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道，并且只需要很短的旋转时间，因此速度会非常快。 操作系统一般将内存和磁盘分割成固态大小的块，没一块称为一页，内存和磁盘以页为单位减缓数据。数据库系统将索引的一个节点的大小设置为页的大小，是的一次I/O就能完全载入一个节点，并且可以利用预读特性，相邻的节点也能够被预先载入。 MySQL-主从复制与读写分离主从复制","categories":[{"name":"春招","slug":"春招","permalink":"https://gleaming.cn/categories/%E6%98%A5%E6%8B%9B/"}],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://gleaming.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"春招:网络","slug":"春招-网络","date":"2022-03-01T02:06:18.000Z","updated":"2022-07-28T07:52:51.995Z","comments":true,"path":"network/","link":"","permalink":"https://gleaming.cn/network/","excerpt":"","text":"TCP协议详解TCP状态 对于建立链接的3次握手，主要是初始化 Sequence Number 的初始值这个号要作为以后的数据通信的序号，以保证应用层接收到的数据不会因为网络上的传输的问题而乱序。 对于四次挥手，其实仔细看就是两次，因为 TCP 是全双工的，所以发送方和接收方都需要 Fin 和 Ack 。只不过有一方是被动的，所以看上去是所谓的4此挥手。 关于建立链接时SYN超时。如果server端接到了client发的SYN后回了SYN-ACK后client掉线了，server端没有收到client回来的ACK，那么，这个连接处于一个中间状态，既没成功也没失败。于是，server端如果在一定时间内没有收到的TCP会重发SYN-ACK。在Linux下，默认重试次数为5次，重试的间隔时间从1s开始每次都翻倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP才会把断开这个连接。 关于SYN Flood攻击。给服务器发一个SYN后下线，服务器需要默认等63s才会断开连接，就可以把服务器的SYN连接的队列耗尽。Linux下给了一个tcp_syncookies的参数，当SYN队列满了之后，TCP会通过源地址端口、目标地址端口和时间戳打造出一个特别的Sequence Number发过去，如果是攻击者，就不会有回应，正常连接就会把它发回来，就可以通过这个代码通信。 快速重传机制包没有连续到达，就ack最后那个可能被丢了的包，如果发送方连续收到3次相同的ack，就重传 SACK方法 这样，在发送端就可以根据回传的SACK来知道哪些数据到了，哪些没有到。 TCP滑动窗口窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 如果发送窗口变成零时，使用ZWP技术，发送ZWP包给接收方，让接收方ack它的窗口尺寸，一般这个值会射程三次，第一次大约30-60秒。如果三次之后，还是0，有的TCP实现就会把链接中断。 拥塞避免 UDP协议详解UDP提供不可靠服务，具有TCP所没有的优势： UDP无连接，时间上不存在建立连接需要的时延。空间上，TCP需要在端系统中维护连接状态，需要一定的开销。此连接装入包括接收和发送缓存，拥塞控制参数和序号与确认号的参数。UCP不维护连接状态，也不跟踪这些参数，开销小。空间和时间上都具有优势。 举个例子： DNS如果运行在TCP之上而不是UDP，那么DNS的速度将会慢很多。 HTTP使用TCP而不是UDP，是因为对于基于文本数据的Web网页来说，可靠性很重要。 同一种专用应用服务器在支持UDP时，一定能支持更多的活动客户机。 分组首部开销小，TCP首部20字节，UDP首部8字节。 UDP没有拥塞控制，应用层能够更好的控制要发送的数据和发送时间，网络中的拥塞控制也不会影响主机的发送速率。某些实时应用要求以稳定的速度发送，能容 忍一些数据的丢失，但是不能允许有较大的时延（比如实时视频，直播等） UDP提供尽最大努力的交付，不保证可靠交付。所有维护传输可靠性的工作需要用户在应用层来完成。没有TCP的确认机制、重传机制。如果因为网络原因没有传送到对端，UDP也不会给应用层返回错误信息 UDP是面向报文的，对应用层交下来的报文，添加首部后直接乡下交付为IP层，既不合并，也不拆分，保留这些报文的边界。对IP层交上来UDP用户数据报，在去除首部后就原封不动地交付给上层应用进程，报文不可分割，是UDP数据报处理的最小单位。 正是因为这样，UDP显得不够灵活，不能控制读写数据的次数和数量。比如我们要发送100个字节的报文，我们调用一次sendto函数就会发送100字节，对端也需要用recvfrom函数一次性接收100字节，不能使用循环每次获取10个字节，获取十次这样的做法。 UDP常用一次性传输比较少量数据的网络应用，如DNS,SNMP等，因为对于这些应用，若是采用TCP，为连接的创建，维护和拆除带来不小的开销。UDP也常用于多媒体应用（如IP电话，实时视频会议，流媒体等）数据的可靠传输对他们而言并不重要，TCP的拥塞控制会使他们有较大的延迟，也是不可容忍的 UDP 支持一对一、一对多、多对一和多对多的交互通信。 HTTP协议详解Session使用 Session 维护用户登录状态的过程如下: 用户进行登录时，用户提交包含用户名和密码的表单，放入 HTTP 请求报文中； 服务器验证该用户名和密码； 如果正确则把用户信息存储到 Redis 中，它在 Redis 中的 Key 称为 Session ID； 服务器返回的响应报文的 Set-Cookie 首部字段包含了这个 Session ID，客户端收到响应报文之后将该 Cookie 值存入浏览器中； 客户端之后对同一个服务器进行请求时会包含该 Cookie 值，服务器收到之后提取出 Session ID，从 Redis 中取出用户信息，继续之前的业务操作。 HTTPSHTTPs 并不是新协议，而是让 HTTP 先和 SSL通信，再由 SSL 和 TCP 通信，也就是说 HTTPs 使用了隧道进行通信。 对称加密:加密解密使用同一密钥 非对称加密:通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。 HTTPS采用混合加密机制，通过非对称加密用与传输对称密钥，用对称密钥进行通信。 HTTPS具有完整性保护。HTTP 也提供了 MD5 报文摘要功能，但不是安全的。例如报文内容被篡改之后，同时重新计算 MD5 的值，通信接收方是无法意识到发生了篡改。HTTPs 的报文摘要功能之所以安全，是因为它结合了加密和认证这两个操作。 HTTP 1.1对比HTTP 1.0 长连接 HTTP1.1可以在一个TCP连接上传送多个HTTP请求。 节约带宽 HTTP1.1支持只发送header信息，收到100信息之后，再把请求body发送到服务器 HTTP 2.0 多路复用 同一个域名下的所有通信都在单个连接上完成。 单个连接可以承载任意数量的双向数据流 数据流以消息的形式发送，而消息由一个或多个帧组成，可以乱序发送，因为根据帧首部的流标识可以重新组装。 请求头信息压缩 二进制分帧 HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装。 缺点 TCP 以及 TCP+TLS 建立连接的延时 TCP 的队头阻塞并没有彻底解决 多路复用导致服务器压力上升 多路复用容易 Timeout HTTP 3.0队头阻塞问题队头阻塞是计算机网络中一种性能受限的现象，就是一个数据包影响了一堆数据包 队头阻塞分为HTTP层和TCP层。HTTP2.0协议的多路复用解决了HTTP层的队头阻塞问题。 TCP协议在收到数据包之后，这部分数据可能是乱序到达的，但是TCP必须将所有数据收集排序整合后给上层使用，如果其中某个包丢失了，就必须等待重传，从而出现某个丢包数据阻塞整个连接的数据使用 QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势。 QUIC 一个连接上的多个 stream 之间没有依赖。这样假如 stream2 丢了一个 udp packet，也只会影响 stream2 的处理。不会影响 stream2 之前及之后的 stream 的处理 0RTT连接RTT是指数据包一来一回的时间消耗。包括传播时延，链路时延，应用处理时延 一般来说HTTPS协议要建立完整链路包括TCP握手和TLS握手，总计至少要2-3个RTT，普通HTTP也要1个RTT才可以完成握手。 QUIC协议可以在第一个包就可以包含有效的应用数据，从而实现0RTT 但是对已第一个交互的客户端和服务端0RTT也是做不到的。 因此QUIC分为首次连接和非首次连接。 使用QUIC协议的客户端和服务端要使用1RTT进行密钥交换客户端和服务端首次连接时服务端传递了config包，里面包含了服务端公钥和两个随机数，客户端会将config存储下来，后续再连接时可以直接使用，从而跳过这个1RTT，实现0RTT的业务数据交互。客户端保存config是有时间期限的，在config失效之后仍然需要进行首次连接时的密钥交换。 输入URL到页面加载过程详解 输入URL DNS域名解析IP 通过域名查找IP过程：浏览器缓存 -&gt; 系统缓存 -&gt; 本地DNS服务器缓存 浏览器搜索自己的DNS缓存 搜索操作系统中的DNS缓存 搜索操作系统中的host文件 操作系统将域名发送到本地区域服务器（LNDS），进行查找，成功则返回结果（递归查询），失败则发起一个迭代DNS请求 本地域名服务器LDNS将得到的IP地址返回给操作系统，同时也将IP地址缓存起来 操作系统将IP地址返回给浏览器，同时将IP地址缓存起来 请求和响应数据 TCP连接建立 发送http 请求 服务端处理 返回http 结果 TCP连接关闭。 浏览器加载，解析和渲染 浏览器加载 解析html，生成dom树 解析css，生成cssom树 将dom树和cssom树合并，生成渲染树 遍历渲染树，开始布局和计算 绘制渲染树，显示到屏幕","categories":[{"name":"春招","slug":"春招","permalink":"https://gleaming.cn/categories/%E6%98%A5%E6%8B%9B/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://gleaming.cn/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"计算机网络","slug":"计算机网络","date":"2021-06-15T02:55:35.000Z","updated":"2022-07-28T07:52:51.995Z","comments":true,"path":"computer-network/","link":"","permalink":"https://gleaming.cn/computer-network/","excerpt":"","text":"1. 概述网络网络把主机连接起来，而互连网（Internet）是把多种不同的网络连接起来，因此互连网是网络的网络。而互联网（Internet）是全球范围的互连网。 ISP互联网服务提供商 ISP 可以从互联网管理机构获得许多 IP 地址，同时拥有通信线路以及路由器等联网设备，个人或机构向 ISP 缴纳一定的费用就可以接入互联网。 主机之间的通信方式 客户-服务器（C/S）：客户是服务的请求方，服务器是服务的提供方。 对等（P2P）：不区分客户和服务器 电路交换和分组交换 电路交换 电路交换用于电话通信系统，两个用户要通信之前需要建立一条专用的物理链路，并且在整个通信过程中始终占用该链路。由于通信的过程中不可能一直在使用传输线路，因此电路交换对线路的利用率很低，往往不到10% 分组交换 每个分组都有首部和尾部，包含了源地址和目的地址等控制信息，在同一个传输线路上同时传输多个分组互相不会影响，因此在同一条传输线路上允许同时传输多个分组，也就是说分组交换不需要占用传输线路。 时延总时延 = 排队时延 + 处理时延 + 传输时延 + 传播时延 排队时延 分组在路由器的输入队列和输出队列中排队等待的时间，取决于网络当前的通信量 处理时延 主机或路由器收到分组进行处理所需要的时间，例如分析首部、从分组中提取数据、进行差错检验或查找适当的路由等。 传输时延 主机或路由器传输数据帧所需要的时间。$$delay=\\frac{l(bit)}{v(bit/s)}$$ 其中 l 表示数据帧的长度，v 表示传输速率。 传播时延 电磁波在信道中传播所花费的时间。$$delay=\\frac{l(m)}{v(m/s)}$$ 其中 l 表示信道长度，v 表示电磁波在信道上的传播速度。 计算机网络体系结构 1. 五层协议 应用层 ：为特定应用程序提供数据传输服务，例如 HTTP、DNS 等协议。数据单位为报文。 传输层 ：为进程提供通用数据传输服务。由于应用层协议很多，定义通用的传输层协议就可以支持不断增多的应用层协议。运输层包括两种协议：传输控制协议 TCP，提供面向连接、可靠的数据传输服务，数据单位为报文段；用户数据报协议 UDP，提供无连接、尽最大努力的数据传输服务，数据单位为用户数据报。TCP 主要提供完整性服务，UDP 主要提供及时性服务。 网络层 ：为主机提供数据传输服务。而传输层协议是为主机中的进程提供数据传输服务。网络层把传输层传递下来的报文段或者用户数据报封装成分组。 数据链路层 ：网络层针对的还是主机之间的数据传输服务，而主机之间可以有很多链路，链路层协议就是为同一链路的主机提供数据传输服务。数据链路层把网络层传下来的分组封装成帧。 物理层 ：考虑的是怎样在传输媒体上传输数据比特流，而不是指具体的传输媒体。物理层的作用是尽可能屏蔽传输媒体和通信手段的差异，使数据链路层感觉不到这些差异。 2. OSi其中表示层和会话层用途如下： 表示层 ：数据压缩、加密以及数据描述，这使得应用程序不必关心在各台主机中数据内部格式不同的问题。 会话层 ：建立及管理会话。 五层协议没有表示层和会话层，而是将这些功能留给应用程序开发者处理。 3. TCP/IP它只有四层，相当于五层协议中数据链路层和物理层合并为网络接口层。 TCP/IP 体系结构不严格遵循 OSI 分层概念，应用层可能会直接使用 IP 层或者网络接口层。 4. 数据传递在向下的过程中，需要添加下层协议所需要的首部或者尾部，而在向上的过程中不断拆开首部和尾部。 路由器只有下面三层协议，因为路由器位于网络核心中，不需要为进程或者应用程序提供服务，因此也就不需要传输层和应用层。 2. 物理层通信方式根据信息在传输线上的传送方向，分为以下三种通信方式： 单工通信：单向传输 半双工通信：双向交替传输 全双工通信：双向同时传输 带通调制模拟信号是连续的信号，数字信号是离散的信号。带通调制把数字信号转换为模拟信号。 3.链路层基本问题1. 封装成帧将网络层传下来的分组添加首部和尾部，用于标记帧的开始和结束 2. 透明传输帧使用首部和尾部进行界定，但是如果帧的数据部分含有首部和尾部相同的内容，就需要在首部尾部相同内容的前面插入转义字符。如果数据部分出现转义字符，就需要在转义字符前面再加个转义字符。接收端处理之后可以还原出原始数据。 3. 差错检测目前数据链路层广泛使用循环冗余检验来检查比特差错。 信道分类1. 广播信道一对多通信，一个节点发送的数据能够被广播信道上所有的节点接收到。 所有的节点都在同一个广播信道上发送数据，因此需要有专门的控制方法进行协调，避免发生冲突（冲突也叫碰撞）。 主要有两种控制方法进行协调，一个是使用信道复用技术，一是使用 CSMA/CD 协议。 2.点对点信道一对一通信。 因为不会发生碰撞，因此也比较简单，使用 PPP 协议进行控制。 信道复用技术1. 频分复用频分复用的所有主机在相同的时间占用不同的频率带宽资源。 2. 时分复用时分复用的所有主机在不同的时间占用相同的频率带宽资源。 使用频分复用和时分复用进行通信，在通信的过程中主机会一直占用一部分信道资源。但是由于计算机数据的突发性质，通信过程没必要一直占用信道资源而不让出给其它用户使用，因此这两种方式对信道的利用率都不高。 3. 统计时分复用是对时分复用的一种改进，不固定每个用户在时分复用帧中的位置，只要有数据就集中起来组成统计时分复用帧然后发送。 4. 波分复用光的频分复用。由于光的频率很高，因此习惯上用波长而不是频率来表示所使用的光载波。 CSMA/CD 协议CSMA/CD 表示载波监听多点接入 / 碰撞检测。 多点接入 ：说明这是总线型网络，许多主机以多点的方式连接到总线上。 载波监听 ：每个主机都必须不停地监听信道。在发送前，如果监听到信道正在使用，就必须等待。 碰撞检测 ：在发送中，如果监听到信道已有其它主机正在发送数据，就表示发生了碰撞。虽然每个主机在发送数据之前都已经监听到信道为空闲，但是由于电磁波的传播时延的存在，还是有可能会发生碰撞。 记端到端的传播时延为 τ，最先发送的站点最多经过 2τ 就可以知道是否发生了碰撞，称 2τ 为 争用期 。只有经过争用期之后还没有检测到碰撞，才能肯定这次发送不会发生碰撞。 当发生碰撞时，站点要停止发送，等待一段时间再发送。这个时间采用 截断二进制指数退避算法 来确定。从离散的整数集合$${0,1…,(2^k-1)}$$中随机取出一个数，记作 r，然后取 r 倍的争用期作为重传等待时间。 PPP 协议互联网用户通常需要连接到某个 ISP 之后才能接入到互联网，PPP 协议是用户计算机和 ISP 进行通信时所使用的数据链路层协议。 PPP 的帧格式： F 字段为帧的定界符 A 和 C 字段暂时没有意义 FCS 字段是使用 CRC 的检验序列 信息部分的长度不超过 1500 MAC地址MAC 地址是链路层地址，长度为 6 字节（48 位），用于唯一标识网络适配器（网卡）。 一台主机拥有多少个网络适配器就有多少个 MAC 地址。例如笔记本电脑普遍存在无线网络适配器和有线网络适配器，因此就有两个 MAC 地址。 4. 网络层IP数据包格式 版本 : 有 4（IPv4）和 6（IPv6）两个值； 首部长度 : 占 4 位，因此最大值为 15。值为 1 表示的是 1 个 32 位字的长度，也就是 4 字节。因为固定部分长度为 20 字节，因此该值最小为 5。如果可选字段的长度不是 4 字节的整数倍，就用尾部的填充部分来填充。 区分服务 : 用来获得更好的服务，一般情况下不使用。 总长度 : 包括首部长度和数据部分长度。 生存时间 ：TTL，它的存在是为了防止无法交付的数据报在互联网中不断兜圈子。以路由器跳数为单位，当 TTL 为 0 时就丢弃数据报。 协议 ：指出携带的数据应该上交给哪个协议进行处理，例如 ICMP、TCP、UDP 等。 首部检验和 ：因为数据报每经过一个路由器，都要重新计算检验和，因此检验和不包含数据部分可以减少计算的工作量。 标识 : 在数据报长度过长从而发生分片的情况下，相同数据报的不同分片具有相同的标识符。 片偏移 : 和标识符一起，用于发生分片的情况。片偏移的单位为 8 字节。 IP地址编址方式IP 地址的编址方式经历了三个历史阶段： 分类 子网划分 无分类 1.分类由两部分组成，网络号和主机号，其中不同分类具有不同的网络号长度，并且是固定的。 IP 地址 ::= {&lt; 网络号 &gt;, &lt; 主机号 &gt;} 2.子网划分通过在主机号字段中拿一部分作为子网号，把两级 IP 地址划分为三级 IP 地址。 IP 地址 ::= {&lt; 网络号 &gt;, &lt; 子网号 &gt;, &lt; 主机号 &gt;} 要使用子网，必须配置子网掩码。一个 B 类地址的默认子网掩码为 255.255.0.0，如果 B 类地址的子网占两个比特，那么子网掩码为 11111111 11111111 11000000 00000000，也就是 255.255.192.0。 注意，外部网络看不到子网的存在。 3.无分类无分类编址 CIDR 消除了传统 A 类、B 类和 C 类地址以及划分子网的概念，使用网络前缀和主机号来对 IP 地址进行编码，网络前缀的长度可以根据需要变化。 IP 地址 ::= {&lt; 网络前缀号 &gt;, &lt; 主机号 &gt;} CIDR 的记法上采用在 IP 地址后面加上网络前缀长度的方法，例如 128.14.35.7/20 表示前 20 位为网络前缀。 CIDR 的地址掩码可以继续称为子网掩码，子网掩码首 1 长度为网络前缀的长度。 一个 CIDR 地址块中有很多地址，一个 CIDR 表示的网络就可以表示原来的很多个网络，并且在路由表中只需要一个路由就可以代替原来的多个路由，减少了路由表项的数量。把这种通过使用网络前缀来减少路由表项的方式称为路由聚合，也称为 构成超网 。 在路由表中的项目由“网络前缀”和“下一跳地址”组成，在查找时可能会得到不止一个匹配结果，应当采用最长前缀匹配来确定应该匹配哪一个。 地址解析协议 ARP网络层实现主机之间的通信，而链路层实现具体每段链路之间的通信。因此在通信过程中，IP 数据报的源地址和目的地址始终不变，而 MAC 地址随着链路的改变而改变。 ARP 实现由 IP 地址得到 MAC 地址。 每个主机都有一个 ARP 高速缓存，里面有本局域网上的各主机和路由器的 IP 地址到 MAC 地址的映射表。 如果主机 A 知道主机 B 的 IP 地址，但是 ARP 高速缓存中没有该 IP 地址到 MAC 地址的映射，此时主机 A 通过广播的方式发送 ARP 请求分组，主机 B 收到该请求后会发送 ARP 响应分组给主机 A 告知其 MAC 地址，随后主机 A 向其高速缓存中写入主机 B 的 IP 地址到 MAC 地址的映射。 网际控制报文协议 ICMPICMP 是为了更有效地转发 IP 数据报和提高交付成功的机会。它封装在 IP 数据报中，但是不属于高层协议。 ICMP 报文分为差错报告报文和询问报文。 1.PingPing 是 ICMP 的一个重要应用，主要用来测试两台主机之间的连通性。 Ping 的原理是通过向目的主机发送 ICMP Echo 请求报文，目的主机收到之后会发送 Echo 回答报文。Ping 会根据时间和成功响应的次数估算出数据包往返时间以及丢包率。 2.TracerouteTraceroute 是 ICMP 的另一个应用，用来跟踪一个分组从源点到终点的路径。 Traceroute 发送的 IP 数据报封装的是无法交付的 UDP 用户数据报，并由目的主机发送终点不可达差错报告报文。 源主机向目的主机发送一连串的 IP 数据报。第一个数据报 P1 的生存时间 TTL 设置为 1，当 P1 到达路径上的第一个路由器 R1 时，R1 收下它并把 TTL 减 1，此时 TTL 等于 0，R1 就把 P1 丢弃，并向源主机发送一个 ICMP 时间超过差错报告报文； 源主机接着发送第二个数据报 P2，并把 TTL 设置为 2。P2 先到达 R1，R1 收下后把 TTL 减 1 再转发给 R2，R2 收下后也把 TTL 减 1，由于此时 TTL 等于 0，R2 就丢弃 P2，并向源主机发送一个 ICMP 时间超过差错报文。 不断执行这样的步骤，直到最后一个数据报刚刚到达目的主机，主机不转发数据报，也不把 TTL 值减 1。但是因为数据报封装的是无法交付的 UDP，因此目的主机要向源主机发送 ICMP 终点不可达差错报告报文。 之后源主机知道了到达目的主机所经过的路由器 IP 地址以及到达每个路由器的往返时间。 5. 传输层网络层只把分组发送到目的主机，但是真正通信的并不是主机而是主机中的进程。传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。 UDP 和 TCP 的特点 用户数据报协议 UDP（User Datagram Protocol）是无连接的，尽最大可能交付，没有拥塞控制，面向报文（对于应用程序传下来的报文不合并也不拆分，只是添加 UDP 首部），支持一对一、一对多、多对一和多对多的交互通信。 传输控制协议 TCP（Transmission Control Protocol）是面向连接的，提供可靠交付，有流量控制，拥塞控制，提供全双工通信，面向字节流（把应用层传下来的报文看成字节流，把字节流组织成大小不等的数据块），每一条 TCP 连接只能是点对点的（一对一）。 UDP 首部格式 首部字段只有 8 个字节，包括源端口、目的端口、长度、检验和。12 字节的伪首部是为了计算检验和临时添加的。 TCP 首部格式 序号 ：用于对字节流进行编号，例如序号为 301，表示第一个字节的编号为 301，如果携带的数据长度为 100 字节，那么下一个报文段的序号应为 401。 确认号 ：期望收到的下一个报文段的序号。例如 B 正确收到 A 发送来的一个报文段，序号为 501，携带的数据长度为 200 字节，因此 B 期望下一个报文段的序号为 701，B 发送给 A 的确认报文段中确认号就为 701。 数据偏移 ：指的是数据部分距离报文段起始处的偏移量，实际上指的是首部的长度。 确认 ACK ：当 ACK=1 时确认号字段有效，否则无效。TCP 规定，在连接建立后所有传送的报文段都必须把 ACK 置 1。 同步 SYN ：在连接建立时用来同步序号。当 SYN=1，ACK=0 时表示这是一个连接请求报文段。若对方同意建立连接，则响应报文中 SYN=1，ACK=1。 终止 FIN ：用来释放一个连接，当 FIN=1 时，表示此报文段的发送方的数据已发送完毕，并要求释放连接。 窗口 ：窗口值作为接收方让发送方设置其发送窗口的依据。之所以要有这个限制，是因为接收方的数据缓存空间是有限的。 TCP 的三次握手 假设 A 为客户端，B 为服务器端。 首先 B 处于 LISTEN（监听）状态，等待客户的连接请求。 A 向 B 发送连接请求报文，SYN=1，ACK=0，选择一个初始的序号 x。 B 收到连接请求报文，如果同意建立连接，则向 A 发送连接确认报文，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 y。 A 收到 B 的连接确认报文后，还要向 B 发出确认，确认号为 y+1，序号为 x+1。 B 收到 A 的确认后，连接建立。 三次握手的原因第三次握手是为了防止失效的连接请求到达服务器，让服务器错误打开连接。 客户端发送的连接请求如果在网络中滞留，那么就会隔很长一段时间才能收到服务器端发回的连接确认。客户端等待一个超时重传时间之后，就会重新请求连接。但是这个滞留的连接请求最后还是会到达服务器，如果不进行三次握手，那么服务器就会打开两个连接。如果有第三次握手，客户端会忽略服务器之后发送的对滞留连接请求的连接确认，不进行第三次握手，因此就不会再次打开连接。 TCP的四次挥手 以下描述不讨论序号和确认号，因为序号和确认号的规则比较简单。并且不讨论 ACK，因为 ACK 在连接建立之后都为 1。 A 发送连接释放报文，FIN=1。 B 收到之后发出确认，此时 TCP 属于半关闭状态，B 能向 A 发送数据但是 A 不能向 B 发送数据。 当 B 不再需要连接时，发送连接释放报文，FIN=1。 A 收到后发出确认，进入 TIME-WAIT 状态，等待 2 MSL（最大报文存活时间）后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文，就进入了 CLOSE-WAIT 状态。这个状态是为了让服务器端发送还未传送完毕的数据，传送完毕之后，服务器会发送 FIN 连接释放报文。 TIME_WAIT客户端接收到服务器端的 FIN 报文后进入此状态，此时并不是直接进入 CLOSED 状态，还需要等待一个时间计时器设置的时间 2MSL。这么做有两个理由： 确保最后一个确认报文能够到达。如果 B 没收到 A 发送来的确认报文，那么就会重新发送连接释放请求报文，A 等待一段时间就是为了处理这种情况的发生。 等待一段时间是为了让本连接持续时间内所产生的所有报文都从网络中消失，使得下一个新的连接不会出现旧的连接请求报文。 TCP可靠传输TCP 使用超时重传来实现可靠传输：如果一个已经发送的报文段在超时时间内没有收到确认，那么就重传这个报文段。 一个报文段从发送再到接收到确认所经过的时间称为往返时间 RTT，加权平均往返时间 RTTs 计算如下：$$RTTs=(1-a)\\ast(RTTs)+a\\ast RTT$$其中，0 ≤ a ＜ 1，RTTs 随着 a 的增加更容易受到 RTT 的影响。 超时时间 RTO 应该略大于 RTTs，TCP 使用的超时时间计算如下：$$RTO=RTTs+4\\ast RTT_d$$其中$$RTT_d$$为偏差的加权平均值。 TCP滑动窗口窗口是缓存的一部分，用来暂时存放字节流。发送方和接收方各有一个窗口，接收方通过 TCP 报文段中的窗口字段告诉发送方自己的窗口大小，发送方根据这个值和其它信息设置自己的窗口大小。 发送窗口内的字节都允许被发送，接收窗口内的字节都允许被接收。如果发送窗口左部的字节已经发送并且收到了确认，那么就将发送窗口向右滑动一定距离，直到左部第一个字节不是已发送并且已确认的状态；接收窗口的滑动类似，接收窗口左部字节已经发送确认并交付主机，就向右滑动接收窗口。 接收窗口只会对窗口内最后一个按序到达的字节进行确认，例如接收窗口已经收到的字节为 {31, 34, 35}，其中 {31} 按序到达，而 {34, 35} 就不是，因此只对字节 31 进行确认。发送方得到一个字节的确认之后，就知道这个字节之前的所有字节都已经被接收。 TCP流量控制流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 TCP拥塞控制如果网络出现拥塞，分组将会丢失，此时发送方会继续重传，从而导致网络拥塞程度更高。因此当出现拥塞时，应当控制发送方的速率。这一点和流量控制很像，但是出发点不同。流量控制是为了让接收方能来得及接收，而拥塞控制是为了降低整个网络的拥塞程度。 TCP 主要通过四个算法来进行拥塞控制：慢开始、拥塞避免、快重传、快恢复。 发送方需要维护一个叫做拥塞窗口（cwnd）的状态变量，注意拥塞窗口与发送方窗口的区别：拥塞窗口只是一个状态变量，实际决定发送方能发送多少数据的是发送方窗口。 为了便于讨论，做如下假设： 接收方有足够大的接收缓存，因此不会发生流量控制； 虽然 TCP 的窗口基于字节，但是这里设窗口的大小单位为报文段。 1.慢开始与拥塞避免发送的最初执行慢开始，令 cwnd = 1，发送方只能发送 1 个报文段；当收到确认后，将 cwnd 加倍，因此之后发送方能够发送的报文段数量为：2、4、8 … 注意到慢开始每个轮次都将 cwnd 加倍，这样会让 cwnd 增长速度非常快，从而使得发送方发送的速度增长速度过快，网络拥塞的可能性也就更高。设置一个慢开始门限 ssthresh，当 cwnd &gt;= ssthresh 时，进入拥塞避免，每个轮次只将 cwnd 加 1。 如果出现了超时，则令 ssthresh = cwnd / 2，然后重新执行慢开始。 2.快重传与快恢复在接收方，要求每次接收到报文段都应该对最后一个已收到的有序报文段进行确认。例如已经接收到 M1 和 M2，此时收到 M4，应当发送对 M2 的确认。 在发送方，如果收到三个重复确认，那么可以知道下一个报文段丢失，此时执行快重传，立即重传下一个报文段。例如收到三个 M2，则 M3 丢失，立即重传 M3。 在这种情况下，只是丢失个别报文段，而不是网络拥塞。因此执行快恢复，令 ssthresh = cwnd / 2 ，cwnd = ssthresh，注意到此时直接进入拥塞避免。 慢开始和快恢复的快慢指的是 cwnd 的设定值，而不是 cwnd 的增长速率。慢开始 cwnd 设定为 1，而快恢复 cwnd 设定为 ssthresh。 6.应用层域名系统DNS 是一个分布式数据库，提供了主机名和 IP 地址之间相互转换的服务。这里的分布式数据库是指，每个站点只保留它自己的那部分数据。 域名具有层次结构，从上到下依次为：根域名、顶级域名、二级域名。 DNS 可以使用 UDP 或者 TCP 进行传输，使用的端口号都为 53。大多数情况下 DNS 使用 UDP 进行传输，这就要求域名解析器和域名服务器都必须自己处理超时和重传从而保证可靠性。在两种情况下会使用 TCP 进行传输： 如果返回的响应超过的 512 字节（UDP 最大只支持 512 字节的数据）。 区域传送（区域传送是主域名服务器向辅助域名服务器传送变化的那部分数据）。 文件传输协议FTPFTP 使用 TCP 进行连接，它需要两个连接来传送一个文件： 控制连接：服务器打开端口号 21 等待客户端的连接，客户端主动建立连接后，使用这个连接将客户端的命令传送给服务器，并传回服务器的应答。 数据连接：用来传送一个文件数据。 根据数据连接是否是服务器端主动建立，FTP 有主动和被动两种模式： 主动模式：服务器端主动建立数据连接，其中服务器端的端口号为 20，客户端的端口号随机，但是必须大于 1024，因为 0~1023 是熟知端口号。 被动模式：客户端主动建立数据连接，其中客户端的端口号由客户端自己指定，服务器端的端口号随机。 主动模式要求客户端开放端口号给服务器端，需要去配置客户端的防火墙。被动模式只需要服务器端开放端口号即可，无需客户端配置防火墙。但是被动模式会导致服务器端的安全性减弱，因为开放了过多的端口号。 动态主机配置协议DHCPDHCP (Dynamic Host Configuration Protocol) 提供了即插即用的连网方式，用户不再需要手动配置 IP 地址等信息。 DHCP 配置的内容不仅是 IP 地址，还包括子网掩码、网关 IP 地址。 DHCP 工作过程如下： 客户端发送 Discover 报文，该报文的目的地址为 255.255.255.255:67，源地址为 0.0.0.0:68，被放入 UDP 中，该报文被广播到同一个子网的所有主机上。如果客户端和 DHCP 服务器不在同一个子网，就需要使用中继代理。 DHCP 服务器收到 Discover 报文之后，发送 Offer 报文给客户端，该报文包含了客户端所需要的信息。因为客户端可能收到多个 DHCP 服务器提供的信息，因此客户端需要进行选择。 如果客户端选择了某个 DHCP 服务器提供的信息，那么就发送 Request 报文给该 DHCP 服务器。 DHCP 服务器发送 Ack 报文，表示客户端此时可以使用提供给它的信息。","categories":[{"name":"复习","slug":"复习","permalink":"https://gleaming.cn/categories/%E5%A4%8D%E4%B9%A0/"}],"tags":[{"name":"网络","slug":"网络","permalink":"https://gleaming.cn/tags/%E7%BD%91%E7%BB%9C/"}]},{"title":"Redis详解(三):布隆过滤器","slug":"Redis布隆过滤器","date":"2021-04-28T13:17:01.000Z","updated":"2022-07-28T07:52:51.994Z","comments":true,"path":"Redis-bloom-filter/","link":"","permalink":"https://gleaming.cn/Redis-bloom-filter/","excerpt":"","text":"一、概述我们假设一种场景:刷抖音 你有刷到过重复的推荐内容吗?这么多的推荐内容要推荐给这么多的用户,他是怎么保证每个用户在看推荐内容时,保证不会出现之前已经看过的推荐视频呢?也就是说,抖音时如何实现推送去重的呢? 你会想到服务器 记录 了用户看过的 所有历史记录,当推荐系统推荐短视频时会从每个用户的历史记录里进行 筛选,过滤掉那些已经存在的记录.问题是当 用户量很大,每个用户看过的短视频又很多的情况下,这种方式,推荐系统的去重工作 在性能上跟的上么？ 实际上,如果历史记录存储在关系数据库里,去重就需要频繁地对数据库进行 exists 查询,当系统并发量很高时,数据库是很难抗住压力的. 你可能又想到了 缓存,但是这么多用户这么多的历史记录,如果全部缓存起来,那得需要 浪费多大的空间 啊.. (可能老板看一眼账单,看一眼你..) 并且这个存储空间会随着时间呈线性增长,就算你用缓存撑得住一个月,但是又能继续撑多久呢？不缓存性能又跟不上,咋办呢？ 布隆过滤器(Bloom Filter) 就是这样一种专门用来解决去重问题的高级数据结构.但是跟 HyperLogLog 一样,它也一样有那么一点点不精确,也存在一定的误判概率,但它能在解决去重的同时,在 空间上能节省 90% 以上,也是非常值得的. 1.1 什么是布隆过滤器布隆过滤器（Bloom filter）是一种非常节省空间的概率数据结构（space-efficient probabilistic data structure）,运行速度快（时间效率）,占用内存小（空间效率）,但是有一定的误判率且无法删除元素.本质上由一个很长的二进制向量和一系列随机映射函数组成. 1.2 布隆过滤器特性 检查一个元素是否在集成中; 检查结果分为2种：一定不在集合中、可能在集合中; 布隆过滤器支持添加元素、检查元素,但是不支持删除元素; 检查结果的“可能在集合中”说明存在一定误判率; 已经添加进入布隆过滤器的元素是不会被误判的,仅未添加过的元素才可能被误判; 相比set、Bitmaps非常节省空间：因为只存储了指纹信息,没有存储元素本身; 添加的元素超过预设容量越多,误报的可能性越大. 需要注意的是,虽然使用了无偏hash函数,使得hash值尽可能均匀,但是不同的item计算出的hash值依旧可能重复,所以布隆过滤器返回元素存在,实际是有可能不存在的. 二、布隆过滤器原理解析布隆过滤器本质上是由长度为m的位向量或位列表（仅包含 0 或 1 位值的列表）组成,最初所有的值均设置为 0,所以我们先来创建一个稍微长一些的位向量用作展示： 当我们向布隆过滤器中添加数据时,会使用 多个 hash 函数对 key 进行运算,算得一个证书索引值,然后对位数组长度进行取模运算得到一个位置,每个 hash 函数都会算得一个不同的位置.再把位数组的这几个位置都置为 1 就完成了 add 操作,例如,我们添加一个 wmyskxz： 向布隆过滤器查查询 key 是否存在时,跟 add 操作一样,会把这个 key 通过相同的多个 hash 函数进行运算,查看 对应的位置 是否 都 为 1,只要有一个位为 0,那么说明布隆过滤器中这个 key 不存在.如果这几个位置都是 1,并不能说明这个 key 一定存在,只能说极有可能存在,因为这些位置的 1 可能是因为其他的 key 存在导致的. 就比如我们在 add 了一定的数据之后,查询一个 不存在 的 key： 很明显,1/3/5 这几个位置的 1 是因为上面第一次添加的 wmyskxz 而导致的,所以这里就存在 误判.幸运的是,布隆过滤器有一个可以预判误判率的公式,比较复杂,感兴趣的朋友可以自行去阅读,比较烧脑.. 只需要记住以下几点就好了： 使用时 不要让实际元素数量远大于初始化数量; 当实际元素数量超过初始化数量时,应该对布隆过滤器进行 重建,重新分配一个 size 更大的过滤器,再将所有的历史元素批量 add 进行; 三、布隆过滤器底层原理3.1 布隆过滤器的底层结构布隆过滤器本质是一个巨大的bit数组（bit array）+几个不同的无偏hash函数. 布隆过滤器添加一个item(“gleaming”),其操作步骤是： 使用多个无偏哈希函数对item进行hash运算,得到多个hash值hash(gleaming); 每个hash值对bit数组取模得到位数组中的位置index(gleaming); 判断所有index位是否都为1 ; 位都为1则说明该元素可能已经存在了; 任意一位不为1则说明一定不存在,此时会将不为1的位置为1; 需要注意的是,虽然使用了无偏hash函数,使得hash值尽可能均匀,但是不同的item计算出的hash值依旧可能重复,所以布隆过滤器返回元素存在,实际是有可能不存在的. 3.2 最佳hash函数数量与错误率的关系源码中计算hash函数数量的代码 1234567891011121314151617181920# hash函数数量计算公式：# ceil(value)：返回不小于value的最小整数；# log(error)：以10为底的对数函数；# ln(x)：以e为底的对数函数；# ln(2) ≈ 0.693147180559945;# ln(2)^2 ≈ 0.480453013918201;bloom-&amp;gt;hashes = (int)ceil(0.693147180559945 * bloom-&amp;gt;bpe);static double calc_bpe(double error) &#123; static const double denom = 0.480453013918201; // ln(2)^2 double num = log(error); double bpe = -(num / denom); if (bpe &amp;lt; 0) &#123; bpe = -bpe; &#125; return bpe;&#125; 错误率{error_rate} hash函数的最佳数量 0.1 5 0.01 8 0.001 11 0.0001 15 0.00001 18 0.000001 21 0.0000001 25 3.3 所需存储空间与错误率及容量的关系 错误率{error_rate} 元素数量{capacity} 占用内存（单位M） 0.001 10万 0.19 0.001 1百万 1.89 0.001 1千万 18.9 0.001 1亿 188.6 0.0001 10万 0.25 0.0001 1百万 2.5 0.0001 1千万 24.6 0.0001 1亿 245.7 0.00001 10万 0.3 0.00001 1百万 3.01 0.00001 1千万 30.1 0.00001 1亿 302.9 占用内存（单位M） = bytes值/1024/1024. 从上述对比分析可以看出,错误率{error_rate}越小,所需的存储空间越大; 初始化设置的元素数量{capacity}越大,所需的存储空间越大,当然如果实际远多于预设时,准确率就会降低. 在1千万数据场景下,error_rate为0.001、0.0001、0.00001实际占用内存都是30M以下,此时如果对准确性要求高,初始化时将错误率设置低一点是完全无伤大雅的. RedisBloom官方默认的error_rate是 0.01,默认的capacity是 100,源码如下： 1234// RedisBloom/src/rebloom.cstatic double BFDefaultErrorRate = 0.01;static size_t BFDefaultInitCapacity = 100; 四、布隆过滤器的应用场景4.1 邮件黑名单&amp;网站黑名单邮箱地址数十亿计且长度不固定,我们需要从海量的邮箱地址中识别出垃圾邮箱地址.当一个邮箱地址被判定为垃圾邮箱后,就将此地址添加进布隆过滤器中即可. 同理,万维网上的URL地址中包含了大量的非法或恶意URL,利用布隆过滤器也可以快速判断此类URL.当布隆过滤器返回结果为存在时,才对URL进行进一步判定处理. 4.2 新闻去重对于百度新闻、头条新闻等信息推荐平台,为了尽可能提升用户体验,应最大可能保证推荐给用户的新闻不重复,将已推荐给用户的文章ID存入布隆过滤器,再次推荐时先判断是否已推送即可. 4.3 缓存穿透&amp;恶意攻击缓存穿透：是指查询了缓存和数据库中都没有的数据.当此类查询请求量过大时（比如系统被恶意攻击）,缓存系统或数据库的压力将增大,极容易宕机. 方式1：当查询DB中发现某数据不存在时,则将此数据ID存入布隆过滤器,每次查询时先判断是否存在于布隆过滤器,存在则说明数据库无此数据,无需继续查询了.当然此种方式仅能处理同一个ID重复访问的场景. 方式2：如果攻击者恶意构造了大量不重复的且数据库中不存在的数据呢,此时可将数据库中已有的数据的唯一ID放入布隆过滤器,每次查询时先判断是否存在于布隆过滤器,存在才调用后端系统查询,则可有效过滤恶意攻击. 使用方式1需要防止指定ID最初不存在于DB中,遂将此ID存入“数据不存在的过滤器”中,但后续DB又新增了此ID,因为布隆过滤器不支持删除操作,一旦发生此类场景,就肯定会出现误判了. 使用方式2需要注意数据的增量,避免数据库中新增了数据而过滤器中还没有导致无法查询到数据.当然如果此时DB中删除了指定数据,布隆过滤器是无法随之删除指纹标记的. 了解了原理方能如臂使指.此外建议,生产数据的ID应定义生成规则及校验规则（比如身份证的最后一位就是校验位）,这样每次查询先判断这个ID是否有效,有效才进行后续的步骤,这样可以充分过滤外部的恶意攻击. 五、布隆过滤器的优缺点5.1 布隆过滤器的优点 【适合大数据场景】：支持海量数据场景下高效判断元素是否存在; 【节省空间】：不存储数据本身,仅存储hash结果取模运算后的位标记; 【数据保密】：不存储数据本身,适合某些保密场景; 5.2 布隆过滤器的缺点 误判】：由于存在hash碰撞,匹配结果如果是“存在于过滤器中”,实际不一定存在; 【不可删除】：没有存储元素本身,所以只能添加但不可删除; 【空间利用率不高】：创建过滤器时需提前预估创建,当错误率越低时,为了尽可能避免hash碰撞,冗余的空间就越多;需要注意的是,空间利用率不高和节省空间并不冲突; 【容量满时误报率增加】当容量快满时,hash碰撞的概率变大,插入、查询的错误率也就随之增加了. 5.3 布隆过滤器的其他问题 【不支持计数】：同一个元素可以多次插入,但效果和插入一次相同; 【查询速度受错误率影响】：由于错误率影响hash函数的数量,当hash函数越多,每次插入、查询需做的hash操作就越多;","categories":[{"name":"Redis","slug":"Redis","permalink":"https://gleaming.cn/categories/Redis/"}],"tags":[]},{"title":"Redis详解(二):持久化机制","slug":"Redis持久化","date":"2021-04-26T12:24:19.000Z","updated":"2022-07-28T07:52:51.994Z","comments":true,"path":"Redis-Persistence/","link":"","permalink":"https://gleaming.cn/Redis-Persistence/","excerpt":"","text":"很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。 Redis 不同于 Memcached 的很重要一点就是，Redis 支持持久化，而且支持两种不同的持久化操作。Redis 的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。 RDBRDB持久化的原理Redis可以通过穿件快找来获得存储在内存里面的数据在某个时间点上的副本.Redis穿件快照之后,可以对快照进行备份,可以将快照复制到其他服务器从而穿件具有相同数据的服务器副本(Redis主从结构,只要用来提高Redis性能),还可以将快照留在原地以便重启服务器的时候使用. RDB是Redis默认采用的持久化方法. 触发RDB持久化过程分为自动触发和手动出发 自动触发: 使用 save 相关配置，如 save m n，表示 m 秒内数据集存在 n 次修改时，自动触发 bgsave。 12345save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。 如果从节点执行全量复制操作，主节点自动执行 bgsave 生成 RDB 文件并发送给从节点。 执行 debug reload 命令重新加载 Redis 时也会自动触发 save 操作。 默认情况下执行 shutdown 命令时，如果没有开启 AOF 持久化功能则自动执行 bgsave。 手动触发: save: 阻塞当前 Redis 服务器，直到 RDB 过程完成为止，对于内存比较大的实例会造成长时间阻塞，线上环境不建议使用。 bgsave: Redis 进程执行 fork 操作创建子进程，RDB 持久化过程由子进程负责，完成后自动结束。阻塞只发生在 fork 阶段，一般时间很短。bgsave 是针对 save 阻塞问题做的优化，因此 Redis 内部所有涉及 RDB 的操作都采用 bgsave 的方式，而 save 方式已经废弃。 bgsave的原理 执行 bgsave 命令，Redis 父进程判断当前是否存在正在执行的子进程，如 RDB/AOF 子进程，如果存在 bgsave 命令直接返回。 父进程执行 fork 操作创建子进程，fork 操作过程中父进程会阻塞。 父进程 fork 完成后，bgsave 命令返回并不再阻塞父进程，可以继续响应其他命令。 子进程创建 RDB 文件，根据父进程内存生成临时快照文件，完成后对原有文件进行原子替换。 进程发送信号给父进程表示完成，父进程更新统计信息。 RDB的优点RDB 是一个紧凑压缩的二进制文件，代表 Redis 在某个时间点上的数据快照。非常适合于备份，全量复制等场景。例如每 6 个消时执行 bgsave 备份，并把 RDB 文件拷贝到远程机器或者文件系统中，用于灾难恢复。 Redis 加载 RDB 恢复数据远远快于 AOF 的方式。 RDB的缺点RDB 方式数据无法做到实时持久化/秒级持久化，因为 bgsave 每次运行都要执行 fork 操作创建子进程，属于重量级操作，频繁执行成本过高。针对 RDB 不适合实时持久化的问题，Redis 提供了 AOF 持久化方式。 RDB 文件使用特定二进制格式保存，Redis 版本演进过程中有多个格式的 RDB 版本，存在老版本 Redis 服务无法兼容新版 RDB 格式的问题。 AOFAOF持久化的原理AOF 持久化以独立日志的方式记录每次写命令，重启时再重新执行 AOF 文件中的命令达到恢复数据的目的。AOF 的主要作用是解决了数据持久化的实时性，目前是 Redis 持久化的主流方式。 开启 AOF 功能需要设置：appendonly yes，默认不开启。保存路径同 RDB 方式一致，通过 dir 配置指定。 AOF 的工作流程操作：命令写入 append、文件同步 sync、文件重写 rewrite、重启加载 load： 所有的写入命令会追加到 aof_buf 缓冲区中。 AOF 缓冲区根据对应的策略向硬盘做同步操作。 随着 AOF 文件越来越大，需要定期对 AOF 文件进行重写，达到压缩的目的。 当服务器重启时，可以加载 AOF 文件进行数据恢复。 在 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度appendfsync everysec #每秒钟同步一次，显示地将多个写命令同步到硬盘appendfsync no #让操作系统决定何时进行同步 AOF 命令写入的原理？AOF 命令写入的内容直接是文本协议格式，采用文本协议格式的原因： 文本协议具有很好的兼容性。 开启 AOF 后所有写入命令都包含追加操作，直接采用协议格式避免了二次处理开销。 文本协议具有可读性，方便直接修改和处理。 AOF 把命令追加到缓冲区的原因： Redis 使用单线程响应命令，如果每次写 AOF 文件命令都直接追加到硬盘，那么性能完全取决于当前硬盘负载。先写入缓冲区中还有另一个好处，Redis 可以提供多种缓冲区同步硬盘策略，在性能和安全性方面做出平衡。 AOF文件重写的原理文件重写是把 Redis 进程内的数据转化为写命令同步到新 AOF 文件的过程，可以降低文件占用空间，更小的文件可以更快地被加载。 重写后 AOF 文件变小的原因： 进程内已经超时的数据不再写入文件。 旧的 AOF 文件含有无效命令，重写使用进程内数据直接生成，这样新的 AOF 文件只保留最终数据写入命令。 多条写命令可以合并为一个，为了防止单条命令过大造成客户端缓冲区溢出，对于 list、set、hash、zset 等类型操作，以 64 个元素为界拆分为多条。 AOF 重写分为手动触发和自动触发，手动触发直接调用 bgrewriteaof 命令，自动触发根据 auto-aof-rewrite-min-size 和 auto-aof-rewrite-percentage 参数确定自动触发时机。 重写流程： ① 执行 AOF 重写请求，如果当前进程正在执行 AOF 重写，请求不执行并返回，如果当前进程正在执行 bgsave 操作，重写命令延迟到 bgsave 完成之后再执行。 ② 父进程执行 fork 创建子进程，开销等同于 bgsave 过程。 ③ 父进程 fork 操作完成后继续响应其他命令，所有修改命令依然写入 AOF 缓冲区并同步到硬盘，保证原有 AOF 机制正确性。 ④ 子进程根据内存快照，按命令合并规则写入到新的 AOF 文件。每次批量写入数据量默认为 32 MB，防止单次刷盘数据过多造成阻塞。 ⑤ 新 AOF 文件写入完成后，子进程发送信号给父进程，父进程更新统计信息。 ⑥ 父进程把 AOF 重写缓冲区的数据写入到新的 AOF 文件并替换旧文件，完成重写。 Redis4.0对于持久化的优化Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。 如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。 重启加载AOF 和 RDB 文件都可以用于服务器重启时的数据恢复。Redis 持久化文件的加载流程： AOF 持久化开启且存在 AOF 文件时，优先加载 AOF 文件。 AOF 关闭时且存在 RDB 文件时，记载 RDB 文件。 加载 AOF/RDB 文件成功后，Redis 启动成功。 AOF/RDB 文件存在错误导致加载失败时，Redis 启动失败并打印错误信息。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://gleaming.cn/categories/Redis/"}],"tags":[]},{"title":"Redis详解(一):底层数据结构","slug":"Redis详解------底层数据结构","date":"2021-04-26T11:10:20.000Z","updated":"2022-07-28T07:52:51.994Z","comments":true,"path":"Redis-Detailed-Explanation/","link":"","permalink":"https://gleaming.cn/Redis-Detailed-Explanation/","excerpt":"","text":"1. 简单动态字符串我们在上面说到,Redis是用C语言编写的,但是对于Redis的字符串,却不是C语言中的字符串,它是自己构建了一种名为 简单动态字符串的抽象类型,并将SDS作为Redis的默认字符串表示. SDS定义: 123456789struct sdshdr&#123; //记录buf数组中已使用字节的数量 //等于 SDS 保存字符串的长度 int len; //记录 buf 数组中未使用字节的数量 int free; //字节数组,用于保存字符串 char buf[];&#125; 用SDS保存字符串“Redis”具体图示如下: len保存了SDS保存字符串的长度 buf[]数组用来保存字符串的每个元素 free记录了buf数组总未使用的字节数量 那这样有什么好处呢? 常熟复杂度获取字符串长度 由于len属性的存在,我们获取SDS字符串的长度只需要读取len的属性,时间复杂度为O(1).而对于C语言来说,获取字符串的长度通常是通过遍历计数实现的,时间复杂度为O(n).通过strlen命令来获取key的字符串. 杜绝缓冲区溢出 我们知道在C语言中使用strcat 函数来进行两个字符串的拼接,一旦没有分配足够长度的内存空间,就会造成缓冲区溢出.而对于 SDS 数据类型,在进行字符修改的时候,会首先根据记录的 len 属性检查内存空间是否满足需求,如果不满足,会进行相应的空间扩展,然后在进行修改操作,所以不会出现缓冲区溢出. 减少修改字符串的内存重新分配次数 C语言由于不记录字符串的长度,所以如果要修改字符串,必须要重新分配内存（先释放再申请）,因为如果没有重新分配,字符串长度增大时会造成内存缓冲区溢出,字符串长度减小时会造成内存泄露. 而对于SDS,由于len属性和free属性的存在,对于修改字符串SDS实现了空间预分配和惰性空间释放两种策略： 1、空间预分配：对字符串进行空间扩展的时候,扩展的内存比实际需要的多,这样可以减少连续执行字符串增长操作所需的内存重分配次数. 2、惰性空间释放：对字符串进行缩短操作时,程序不立即使用内存重新分配来回收缩短后多余的字节,而是使用 free 属性将这些字节的数量记录下来,等待后续使用.（当然SDS也提供了相应的API,当我们有需要时,也可以手动释放这些未使用的空间.） 二进制安全 因为C字符串以空字符作为字符串结束的标识,而对于一些二进制文件（如图片等）,内容可能包括空字符串,因此C字符串无法正确存取；而所有 SDS 的API 都是以处理二进制的方式来处理 buf 里面的元素,并且 SDS 不是以空字符串来判断是否结束,而是以 len 属性表示的长度来判断字符串是否结束. 兼容部分C字符串函数 虽然SDS是二进制安全的,但是一样遵从每个字符串都是以空字符结尾的惯例,这样可以使用&lt;string.h&gt;中的一部分函数. 总结 2. 链表链表是一种常用的数据结构,C 语言内部是没有内置这种数据结构的实现,所以Redis自己构建了链表的实现. 123456789101112131415161718192021222324typedef struct listNode&#123; //前置节点 struct listNode *prev; //后置节点 struct listNode *next; //节点的值 void *value; &#125;listNode typedef struct list&#123; //表头节点 listNode *head; //表尾节点 listNode *tail; //链表所包含的节点数量 unsigned long len; //节点值复制函数 void (*free) (void *ptr); //节点值释放函数 void (*free) (void *ptr); //节点值对比函数 int (*match) (void *ptr,void *key);&#125;list; Redis链表特性: 双端: 链表具有前置节点和后置节点的引用,获取这两个节点时间复杂度都为O(1). 无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL,对链表的访问都是以 NULL 结束. 带链表长度计数器：通过 len 属性获取链表长度的时间复杂度为 O(1). 多态：链表节点使用 void* 指针来保存节点值,可以保存各种不同类型的值. 3. 字典字典又称为符号表活着关联数组、或映射(map),是一种用于保存键值对的抽象数据结构.字典中的每一个键Key都是唯一的,通过Key可以对值进行查找和修改.C语言中没有内置这种数据结构的实现,所以字典依然是Redis自己构建的. Redis的字典使用哈希表作为底层实现,哈希表结构定义为: 123456789101112typedef struct dictht&#123; //哈希表数组 dictEntry **table; //哈希表大小 unsigned long size; //哈希表大小掩码,用于计算索引值 //总是等于 size-1 unsigned long sizemask; //该哈希表已有节点的数量 unsigned long used;&#125;dictht 哈希表是由数组table组成,table中每个元素都是指向dict.h/dictEntry结构,dictEntry结构定义如下: 12345678910111213typedef struct dictEntry&#123; //键 void *key; //值 union&#123; void *val; uint64_tu64; int64_ts64; &#125;v; //指向下一个哈希表节点,形成链表 struct dictEntry *next;&#125;dictEntry Key用来保存键,val属性用来保存值,值可以是一个指针,也可以是uint64_t整数,也可以是int64_t整数. 注意这里还有一个指向下一个哈希表姐节点的指针,我们知道哈希表最大的问题是存在哈希冲突,这里采用链地址法,通过next指针可以将多个哈希值相同的键值对连接在一起. 哈希算法: Redis计算哈希值和索引值的方法 12341、使用字典设置的哈希函数，计算键 key 的哈希值hash = dict-&gt;type-&gt;hashFunction(key);2、使用哈希表的sizemask属性和第一步得到的哈希值，计算索引值index = hash &amp; dict-&gt;ht[x].sizemask; 解决哈希冲突: 链地址法。通过字典里面的 *next 指针指向下一个具有相同索引值的哈希表节点。 扩容和收缩：当哈希表保存的键值对太多或者太少时，就要通过 rerehash(重新散列）来对哈希表进行相应的扩展或者收缩。具体步骤： 如果执行扩展操作，会基于原哈希表创建一个大小等于 ht[0].used*2的2^n** 的哈希表（也就是每次扩展都是根据原哈希表已使用的空间扩大一倍创建另一个哈希表）。相反如果执行的是收缩操作，每次收缩是基于原哈希表创建一个大小等于ht[0].used的2^n. 重新利用上面的哈希算法，计算索引值，然后将键值对放到新的哈希表位置上。 所有键值对都迁徙完毕后，释放原哈希表的内存空间。 触发扩容的条件 服务器目前没有执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于1。 服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，并且负载因子大于等于5。 ps：负载因子 = 哈希表已保存节点数量 / 哈希表大小。 渐近式 rehash 什么叫渐进式 rehash？也就是说扩容和收缩操作不是一次性、集中式完成的，而是分多次、渐进式完成的。如果保存在Redis中的键值对只有几个几十个，那么 rehash 操作可以瞬间完成，但是如果键值对有几百万，几千万甚至几亿，那么要一次性的进行 rehash，势必会造成Redis一段时间内不能进行别的操作。所以Redis采用渐进式 rehash,这样在进行渐进式rehash期间，字典的删除查找更新等操作可能会在两个哈希表上进行，第一个哈希表没有找到，就会去第二个哈希表上进行查找。但是进行 增加操作，一定是在新的哈希表上进行的。 4. 跳跃表跳跃表是一种有序数据结构,它通过在每个节点中维持多个指向其他节点的指针,从而达到快速访问节点的目的.性质如下: 有很多层结构组成; 每一层都是一个有序的链表,排列顺序由高层到底层,都至少包含两个链表节点,分别是前面的head和后面的nil节点; 最底层的链表包含了所有的元素; 如果一个元素出现在某一层的链表中,那么在该层之下的链表也全都会出现(上一层的元素是当前层的元素的子集); 链表中的每个节点都包含两个指针,一个指向同一层的下一个链表节点,另一个指向下一层的同一个链表节点; 12345678910111213141516171819202122232425262728typedef struct zskiplistNode &#123; //层 struct zskiplistLevel&#123; //前进指针 struct zskiplistNode *forward; //跨度 unsigned int span; &#125;level[]; //后退指针 struct zskiplistNode *backward; //分值 double score; //成员对象 robj *obj; &#125; zskiplistNode typedef struct zskiplist&#123; //表头节点和表尾节点 structz skiplistNode *header, *tail; //表中节点的数量 unsigned long length; //表中层数最大的节点的层数 int level; &#125;zskiplist; 搜索：从最高层的链表节点开始，如果比当前节点要大和比当前层的下一个节点要小，那么则往下找，也就是和当前层的下一层的节点的下一个节点进行比较，以此类推，一直找到最底层的最后一个节点，如果找到则返回，反之则返回空。 插入：首先确定插入的层数，有一种方法是假设抛一枚硬币，如果是正面就累加，直到遇见反面为止，最后记录正面的次数作为插入的层数。当确定插入的层数k后，则需要将新元素插入到从底层到k层。 删除：在各个层中找到包含指定值的节点，然后将节点从链表中删除即可，如果删除以后只剩下头尾两个节点，则删除这一层。 5. 整数集合整数集合（intset）是Redis用于保存整数值的集合抽象数据类型，它可以保存类型为int16_t、int32_t 或者int64_t 的整数值，并且保证集合中不会出现重复元素。 123456789typedef struct intset&#123; //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[]; &#125;intset; 整数集合的每个元素都是 contents 数组的一个数据项，它们按照从小到大的顺序排列，并且不包含任何重复项。 length 属性记录了 contents 数组的大小。 需要注意的是虽然 contents 数组声明为 int8_t 类型，但是实际上contents 数组并不保存任何 int8_t 类型的值，其真正类型有 encoding 来决定。 升级 当我们新增的元素类型比原集合元素类型的长度要大时，需要对整数集合进行升级，才能将新元素放入整数集合中。具体步骤： 根据新元素类型，扩展整数集合底层数组的大小，并为新元素分配空间。 将底层数组现有的所有元素都转成与新元素相同类型的元素，并将转换后的元素放到正确的位置，放置过程中，维持整个元素顺序都是有序的。 将新元素添加到整数集合中（保证有序）。 升级能极大地节省内存。 降级 整数集合不支持降级操作,一旦对数组进行升级,编码就会一直保持升级后的状态. 6. 压缩列表压缩列表(ziplist)是Redis为了节省内存而开发的，是由一系列特殊编码的连续内存块组成的顺序型数据结构，一个压缩列表可以包含任意多个节点(entry),每个节点可以保存一个字节数组或者一个整数值. 压缩列表的原理: 压缩列表并不是对数据利用某种算法进行压缩,而是将数据按照一定的规则编码在一块连续的内存区域,目的是节省内存. 压缩列表的每个节点构成如下: previous_entry_ength：记录压缩列表前一个字节的长度。previous_entry_ength的长度可能是1个字节或者是5个字节，如果上一个节点的长度小于254，则该节点只需要一个字节就可以表示前一个节点的长度了，如果前一个节点的长度大于等于254，则previous length的第一个字节为254，后面用四个字节表示当前节点前一个节点的长度。利用此原理即当前节点位置减去上一个节点的长度即得到上一个节点的起始位置，压缩列表可以从尾部向头部遍历。这么做很有效地减少了内存的浪费。 encoding：节点的encoding保存的是节点的content的内容类型以及长度，encoding类型一共有两种，一种字节数组一种是整数，encoding区域长度为1字节、2字节或者5字节长。 content：content区域用于保存节点的内容，节点内容类型和长度由encoding决定。 7. 总结大多数情况下，Redis使用简单字符串SDS作为字符串的表示，相对于C语言字符串，SDS具有常数复杂度获取字符串长度，杜绝了缓存区的溢出，减少了修改字符串长度时所需的内存重分配次数，以及二进制安全能存储各种类型的文件，并且还兼容部分C函数。 通过为链表设置不同类型的特定函数，Redis链表可以保存各种不同类型的值，除了用作列表键，还在发布与订阅、慢查询、监视器等方面发挥作用（后面会介绍）。 Redis的字典底层使用哈希表实现，每个字典通常有两个哈希表，一个平时使用，另一个用于rehash时使用，使用链地址法解决哈希冲突。 跳跃表通常是有序集合的底层实现之一，表中的节点按照分值大小进行排序。 整数集合是集合键的底层实现之一，底层由数组构成，升级特性能尽可能的节省内存。 压缩列表是Redis为节省内存而开发的顺序型数据结构，通常作为列表键和哈希键的底层实现之一。","categories":[{"name":"Redis","slug":"Redis","permalink":"https://gleaming.cn/categories/Redis/"}],"tags":[]},{"title":"HashMap源码解析","slug":"HashMap源码解析","date":"2021-04-23T13:40:37.000Z","updated":"2022-07-28T07:52:51.992Z","comments":true,"path":"HashMap-source-code/","link":"","permalink":"https://gleaming.cn/HashMap-source-code/","excerpt":"","text":"HashMap的特性Map是Key-Value对映射的接口对象,该映射不包括重复的键,即一个键对应一个值.HashMap是基于哈希表的Map借口的实现,以Key-Value的形式存在.在HashMap中，其会根据hash算法来计算key-value的存储位置并进行快速存取。特点如下 HashMap是可以序列化的.是线程不安全的. HashMap的底层主要是基于数组,链表,红黑树实现的,它之所以有相当快的查询素的主要是因为他是通过计算Hash值来决定存储位置的. 如果发生哈希碰撞就以链表的形式存储,如果链表过长的话,会转换为红黑树存储. 源码重要成员变量1234567891011121314151617181920212223242526272829303132333435363738// 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换）static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;//默认装填因子0.75，如果当前键值对个数 &gt;= HashMap最大容量*装填因子，进行rehash操作static final float DEFAULT_LOAD_FACTOR = 0.75f;//JDK1.8 新加，Entry链表最大长度，当桶中节点数目大于该长度时，将链表转成红黑树存储；static final int TREEIFY_THRESHOLD = 8;//JDK1.8 新加，当桶中节点数小于该长度，将红黑树转为链表存储；static final int UNTREEIFY_THRESHOLD = 6;//桶可能被转化为树形结构的最小容量。当哈希表的大小超过这个阈值，才会把链式结构转化成树型结构，否则仅采取扩容来尝试减少冲突。//应该至少4*TREEIFY_THRESHOLD来避免扩容和树形结构化之间的冲突。static final int MIN_TREEIFY_CAPACITY = 64;//哈希桶数组，分配的时候，table的长度总是2的幂transient Node&lt;K, V&gt;[] table;//HashMap将数据转换成set的另一种存储形式，这个变量主要用于迭代功能transient Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet;//实际存储的数量，则HashMap的size()方法，实际返回的就是这个值，isEmpty()也是判断该值是否为0transient int size;//hashmap结构被改变的次数，fail-fast机制transient int modCount;//HashMap的扩容阈值，在HashMap中存储的Node键值对超过这个数量时，自动扩容容量为原来的二倍int threshold;//HashMap的负加载因子，可计算出当前table长度下的扩容阈值：threshold = loadFactor * table.lengthfinal float loadFactor; 重点属性 table在JDK1.8中我们了解到HashMap是由数组加链表加红黑树来组成的结构其中table就是HashMap中的数组 size为HashMap中K-V的实时数量 loadFactor加载因子，是用来衡量 HashMap 满的程度，计算HashMap的实时加载因子的方法为：size/capacity，而不是占用桶的数量去除以capacity。capacity 是桶的数量，也就是 table 的长度length。 threshold计算公式：capacity * loadFactor。这个值是当前已占用数组长度的最大值。过这个数目就重新resize(扩容)，扩容后的 HashMap 容量是之前容量的两倍 构造方法无参数构造函数1234//使用指定的初始化容量（16）和默认加载因子DEFAULT_LOAD_FACTOR（0.75）构造一个空HashMappublic HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted&#125; HashMap(int initialCapacity)1234//使用指定的初始化容量initial capacity和默认加载因子DEFAULT_LOAD_FACTOR（0.75）构造一个空HashMappublic HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125; HashMap(int initialCapacity, float loadFactor)12345678910111213//使用指定的初始化容量initial capacity 和加载因子load factor构造一个空HashMappublic HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity);&#125; 最后调用了tableSizeFor 12345678910static final int tableSizeFor(int cap) &#123; //先移位再或运算，最终保证返回值是2的整数幂 int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; get方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 根据key的哈希值和key获取对应的节点 * getNode可分为以下几个步骤： * 1.如果哈希表为空，或key对应的桶为空，返回null * 2.如果桶中的第一个节点就和指定参数hash和key匹配上了，返回这个节点。 * 3.如果桶中的第一个节点没有匹配上，而且有后续节点 * 3.1如果当前的桶采用红黑树，则调用红黑树的get方法去获取节点 * 3.2如果当前的桶不采用红黑树，即桶中节点结构为链式结构，遍历链表，直到key匹配 * 4.找到节点返回null，否则返回null。 * * @param hash 指定参数key的哈希值 * @param key 指定参数key * @return 返回node，如果没有则返回null */ final Node&lt;K, V&gt; getNode(int hash, Object key) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; first, e; int n; K k; //如果哈希表不为空，而且key对应的桶上不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //如果桶中的第一个节点就和指定参数hash和key匹配上了 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) //返回桶中的第一个节点 return first; //如果桶中的第一个节点没有匹配上，而且有后续节点 if ((e = first.next) != null) &#123; //如果当前的桶采用红黑树，则调用红黑树的get方法去获取节点 if (first instanceof TreeNode) return ((TreeNode&lt;K, V&gt;) first).getTreeNode(hash, key); //如果当前的桶不采用红黑树，即桶中节点结构为链式结构 do &#123; //遍历链表，直到key匹配 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; //如果哈希表为空，或者没有找到节点，返回null return null; &#125; /** * 如果map中含有key为指定参数key的键值对，返回true * * @param key 指定参数key * @return 如果map中含有key为指定参数key的键值对，返回true * key. */ public boolean containsKey(Object key) &#123; return getNode(hash(key), key) != null; &#125; put方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * 将指定参数key和指定参数value插入map中，如果key已经存在，那就替换key对应的value * put(K key, V value)可以分为三个步骤： * 1.通过hash(Object key)方法计算key的哈希值。 * 2.通过putVal(hash(key), key, value, false, true)方法实现功能。 * 3.返回putVal方法返回的结果。 * * @param key 指定key * @param value 指定value * @return 如果value被替换，则返回旧的value，否则返回null。当然，可能key对应的value就是null */ public V put(K key, V value) &#123; // 倒数第二个参数false：表示允许旧值替换 // 最后一个参数true：表示HashMap不处于创建模式 return putVal(hash(key), key, value, false, true); &#125;/** * Map.put和其他相关方法的实现需要的方法 * putVal方法可以分为下面的几个步骤: * 1.如果哈希表为空，调用resize()创建一个哈希表。 * 2.如果指定参数hash在表中没有对应的桶，即为没有碰撞，直接将键值对插入到哈希表中即可。 * 3.如果有碰撞，遍历桶，找到key映射的节点 * 3.1桶中的第一个节点就匹配了，将桶中的第一个节点记录起来。 * 3.2如果桶中的第一个节点没有匹配，且桶中结构为红黑树，则调用红黑树对应的方法插入键值对。 * 3.3如果不是红黑树，那么就肯定是链表。遍历链表，如果找到了key映射的节点，就记录这个节点，退出循环。如果没有找到，在链表尾部插入节点。插入后，如果链的长度大于等于TREEIFY_THRESHOLD这个临界值，则使用treeifyBin方法把链表转为红黑树。 * 4.如果找到了key映射的节点，且节点不为null * 4.1记录节点的vlaue。 * 4.2如果参数onlyIfAbsent为false，或者oldValue为null，替换value，否则不替换。 * 4.3返回记录下来的节点的value。 * 5.如果没有找到key映射的节点（2、3步中讲了，这种情况会插入到hashMap中），插入节点后size会加1，这时要检查size是否大于临界值threshold，如果大于会使用resize方法进行扩容。 * * @param hash 指定参数key的哈希值 * @param key 指定参数key * @param value 指定参数value * @param onlyIfAbsent 如果为true，即使指定参数key在map中已经存在，也不会替换value * @param evict 如果为false，数组table在创建模式中 * @return 如果value被替换，则返回旧的value，否则返回null。当然，可能key对应的value就是null。 */ final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, i; //如果哈希表为空，调用resize()创建一个哈希表，并用变量n记录哈希表长度 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; /** * 如果指定参数hash在表中没有对应的桶，即为没有碰撞 * Hash函数，(n - 1) &amp; hash 计算key将被放置的槽位 * (n - 1) &amp; hash 本质上是hash % n，位运算更快 */ if ((p = tab[i = (n - 1) &amp; hash]) == null) //直接将键值对插入到map中即可 tab[i] = newNode(hash, key, value, null); else &#123;// 桶中已经存在元素 Node&lt;K, V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // 当前桶中无该键值对，且桶是红黑树结构，按照红黑树结构插入 else if (p instanceof TreeNode) e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); // 当前桶中无该键值对，且桶是链表结构，按照链表结构插入到尾部 else &#123; for (int binCount = 0; ; ++binCount) &#123; // 遍历到链表尾部 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 检查链表长度是否达到阈值，达到将该槽位节点组织形式转为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 链表节点的&lt;key, value&gt;与put操作&lt;key, value&gt;相同时，不做重复操作，跳出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 找到或新建一个key和hashCode与插入元素相等的键值对，进行put操作 if (e != null) &#123; // existing mapping for key // 记录e的value V oldValue = e.value; /** * onlyIfAbsent为false或旧值为null时，允许替换旧值 * 否则无需替换 */ if (!onlyIfAbsent || oldValue == null) e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 更新结构化修改信息 ++modCount; // 键值对数目超过阈值时，进行rehash if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null; &#125; 流程图 对table进行扩容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110/** * 对table进行初始化或者扩容。 * 如果table为null，则对table进行初始化 * 如果对table扩容，因为每次扩容都是翻倍，与原来计算（n-1）&amp;hash的结果相比，节点要么就在原来的位置，要么就被分配到“原位置+旧容量”这个位置 * resize的步骤总结为: * 1.计算扩容后的容量，临界值。 * 2.将hashMap的临界值修改为扩容后的临界值 * 3.根据扩容后的容量新建数组，然后将hashMap的table的引用指向新数组。 * 4.将旧数组的元素复制到table中。 * * @return the table */final Node&lt;K, V&gt;[] resize() &#123; //新建oldTab数组保存扩容前的数组table Node&lt;K, V&gt;[] oldTab = table; //获取原来数组的长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //原来数组扩容的临界值 int oldThr = threshold; int newCap, newThr = 0; //如果扩容前的容量 &gt; 0 if (oldCap &gt; 0) &#123; //如果原来的数组长度大于最大值(2^30) if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //扩容临界值提高到正无穷 threshold = Integer.MAX_VALUE; //无法进行扩容，返回原来的数组 return oldTab; //如果现在容量的两倍小于MAXIMUM_CAPACITY且现在的容量大于DEFAULT_INITIAL_CAPACITY &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //临界值变为原来的2倍 newThr = oldThr &lt;&lt; 1; &#125; else if (oldThr &gt; 0) //如果旧容量 &lt;= 0，而且旧临界值 &gt; 0 //数组的新容量设置为老数组扩容的临界值 newCap = oldThr; else &#123; //如果旧容量 &lt;= 0，且旧临界值 &lt;= 0，新容量扩充为默认初始化容量，新临界值为DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY newCap = DEFAULT_INITIAL_CAPACITY;//新数组初始容量设置为默认值 newThr = (int) (DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//计算默认容量下的阈值 &#125; // 计算新的resize上限 if (newThr == 0) &#123;//在当上面的条件判断中，只有oldThr &gt; 0成立时，newThr == 0 //ft为临时临界值，下面会确定这个临界值是否合法，如果合法，那就是真正的临界值 float ft = (float) newCap * loadFactor; //当新容量&lt; MAXIMUM_CAPACITY且ft &lt; (float)MAXIMUM_CAPACITY，新的临界值为ft，否则为Integer.MAX_VALUE newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float) MAXIMUM_CAPACITY ? (int) ft : Integer.MAX_VALUE); &#125; //将扩容后hashMap的临界值设置为newThr threshold = newThr; //创建新的table，初始化容量为newCap @SuppressWarnings(&#123;&quot;rawtypes&quot;, &quot;unchecked&quot;&#125;) Node&lt;K, V&gt;[] newTab = (Node&lt;K, V&gt;[]) new Node[newCap]; //修改hashMap的table为新建的newTab table = newTab; //如果旧table不为空，将旧table中的元素复制到新的table中 if (oldTab != null) &#123; //遍历旧哈希表的每个桶，将旧哈希表中的桶复制到新的哈希表中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K, V&gt; e; //如果旧桶不为null，使用e记录旧桶 if ((e = oldTab[j]) != null) &#123; //将旧桶置为null oldTab[j] = null; //如果旧桶中只有一个node if (e.next == null) //将e也就是oldTab[j]放入newTab中e.hash &amp; (newCap - 1)的位置 newTab[e.hash &amp; (newCap - 1)] = e; //如果旧桶中的结构为红黑树 else if (e instanceof TreeNode) //将树中的node分离 ((TreeNode&lt;K, V&gt;) e).split(this, newTab, j, oldCap); else &#123; //如果旧桶中的结构为链表,链表重排，jdk1.8做的一系列优化 Node&lt;K, V&gt; loHead = null, loTail = null; Node&lt;K, V&gt; hiHead = null, hiTail = null; Node&lt;K, V&gt; next; //遍历整个链表中的节点 do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123;// 原索引+oldCap if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 将链表转化为红黑树1234567891011121314151617181920212223242526272829final void treeifyBin(Node&lt;K, V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K, V&gt; e; //如果桶数组table为空，或者桶数组table的长度小于MIN_TREEIFY_CAPACITY，不符合转化为红黑树的条件 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //扩容 resize(); //如果符合转化为红黑树的条件，而且hash对应的桶不为null else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; // 红黑树的头、尾节点 TreeNode&lt;K, V&gt; hd = null, tl = null; //遍历链表 do &#123; //替换链表node为树node，建立双向链表 TreeNode&lt;K, V&gt; p = replacementTreeNode(e, null); // 确定树头节点 if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); //遍历链表插入每个节点到红黑树 if ((tab[index] = hd) != null) hd.treeify(tab); &#125; &#125; remove方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788/** * 删除hashMap中key映射的node * remove方法的实现可以分为三个步骤： * 1.通过 hash(Object key)方法计算key的哈希值。 * 2.通过 removeNode 方法实现功能。 * 3.返回被删除的node的value。 * * @param key 参数key * @return 如果没有映射到node，返回null，否则返回对应的value */ public V remove(Object key) &#123; Node&lt;K, V&gt; e; //根据key来删除node。removeNode方法的具体实现在下面 return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; /** * Map.remove和相关方法的实现需要的方法 * removeNode方法的步骤总结为: * 1.如果数组table为空或key映射到的桶为空，返回null。 * 2.如果key映射到的桶上第一个node的就是要删除的node，记录下来。 * 3.如果桶内不止一个node，且桶内的结构为红黑树，记录key映射到的node。 * 4.桶内的结构不为红黑树，那么桶内的结构就肯定为链表，遍历链表，找到key映射到的node，记录下来。 * 5.如果被记录下来的node不为null，删除node，size-1被删除。 * 6.返回被删除的node。 * * @param hash key的哈希值 * @param key key的哈希值 * @param value 如果 matchValue 为true，则value也作为确定被删除的node的条件之一，否则忽略 * @param matchValue 如果为true，则value也作为确定被删除的node的条件之一 * @param movable 如果为false，删除node时不会删除其他node * @return 返回被删除的node，如果没有node被删除，则返回null（针对红黑树的删除方法） */ final Node&lt;K, V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, index; //如果数组table不为空且key映射到的桶不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K, V&gt; node = null, e; K k; V v; //如果桶上第一个node的就是要删除的node if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) //记录桶上第一个node node = p; else if ((e = p.next) != null) &#123;//如果桶内不止一个node //如果桶内的结构为红黑树 if (p instanceof TreeNode) //记录key映射到的node node = ((TreeNode&lt;K, V&gt;) p).getTreeNode(hash, key); else &#123;//如果桶内的结构为链表 do &#123;//遍历链表，找到key映射到的node if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; //记录key映射到的node node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //如果得到的node不为null且(matchValue为false||node.value和参数value匹配) if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //如果桶内的结构为红黑树 if (node instanceof TreeNode) //使用红黑树的删除方法删除node ((TreeNode&lt;K, V&gt;) node).removeTreeNode(this, tab, movable); else if (node == p)//如果桶的第一个node的就是要删除的node //删除node tab[index] = node.next; else//如果桶内的结构为链表，使用链表删除元素的方式删除node p.next = node.next; ++modCount;//结构性修改次数+1 --size;//哈希表大小-1 afterNodeRemoval(node); return node;//返回被删除的node &#125; &#125; return null;//如果数组table为空或key映射到的桶为空，返回null。 &#125; clear123456789public void clear() &#123; Node&lt;K, V&gt;[] tab; modCount++; if ((tab = table) != null &amp;&amp; size &gt; 0) &#123; size = 0; for (int i = 0; i &lt; tab.length; ++i) tab[i] = null; &#125;&#125; 红黑树123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609static final class TreeNode&lt;K, V&gt; extends LinkedHashMap.Entry&lt;K, V&gt; &#123; TreeNode&lt;K, V&gt; parent; //节点的父亲 TreeNode&lt;K, V&gt; left; //节点的左孩子 TreeNode&lt;K, V&gt; right; //节点的右孩子 TreeNode&lt;K, V&gt; prev; //节点的前一个节点 boolean red; //true表示红节点，false表示黑节点 TreeNode(int hash, K key, V val, Node&lt;K, V&gt; next) &#123; super(hash, key, val, next); &#125; /** * 获取红黑树的根 */ final TreeNode&lt;K, V&gt; root() &#123; for (TreeNode&lt;K, V&gt; r = this, p; ; ) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; /** * 确保root是桶中的第一个元素 ，将root移到中中的第一个 */ static &lt;K, V&gt; void moveRootToFront(Node&lt;K, V&gt;[] tab, TreeNode&lt;K, V&gt; root) &#123; int n; if (root != null &amp;&amp; tab != null &amp;&amp; (n = tab.length) &gt; 0) &#123; int index = (n - 1) &amp; root.hash; TreeNode&lt;K, V&gt; first = (TreeNode&lt;K, V&gt;) tab[index]; if (root != first) &#123; Node&lt;K, V&gt; rn; tab[index] = root; TreeNode&lt;K, V&gt; rp = root.prev; if ((rn = root.next) != null) ((TreeNode&lt;K, V&gt;) rn).prev = rp; if (rp != null) rp.next = rn; if (first != null) first.prev = root; root.next = first; root.prev = null; &#125; assert checkInvariants(root); &#125; &#125; /** * 查找hash为h，key为k的节点 */ final TreeNode&lt;K, V&gt; find(int h, Object k, Class&lt;?&gt; kc) &#123; TreeNode&lt;K, V&gt; p = this; do &#123; int ph, dir; K pk; TreeNode&lt;K, V&gt; pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); return null; &#125; /** * 获取树节点，通过根节点查找 */ final TreeNode&lt;K, V&gt; getTreeNode(int h, Object k) &#123; return ((parent != null) ? root() : this).find(h, k, null); &#125; /** * 比较2个对象的大小 */ static int tieBreakOrder(Object a, Object b) &#123; int d; if (a == null || b == null || (d = a.getClass().getName(). compareTo(b.getClass().getName())) == 0) d = (System.identityHashCode(a) &lt;= System.identityHashCode(b) ? -1 : 1); return d; &#125; /** * 将链表转为二叉树 * * @return root of tree */ final void treeify(Node&lt;K, V&gt;[] tab) &#123; TreeNode&lt;K, V&gt; root = null; for (TreeNode&lt;K, V&gt; x = this, next; x != null; x = next) &#123; next = (TreeNode&lt;K, V&gt;) x.next; x.left = x.right = null; if (root == null) &#123; x.parent = null; x.red = false; root = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K, V&gt; p = root; ; ) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K, V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; moveRootToFront(tab, root); &#125; /** * 将二叉树转为链表 */ final Node&lt;K, V&gt; untreeify(HashMap&lt;K, V&gt; map) &#123; Node&lt;K, V&gt; hd = null, tl = null; for (Node&lt;K, V&gt; q = this; q != null; q = q.next) &#123; Node&lt;K, V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd; &#125; /** * 添加一个键值对 */ final TreeNode&lt;K, V&gt; putTreeVal(HashMap&lt;K, V&gt; map, Node&lt;K, V&gt;[] tab, int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; TreeNode&lt;K, V&gt; root = (parent != null) ? root() : this; for (TreeNode&lt;K, V&gt; p = root; ; ) &#123; int dir, ph; K pk; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; if (!searched) &#123; TreeNode&lt;K, V&gt; q, ch; searched = true; if (((ch = p.left) != null &amp;&amp; (q = ch.find(h, k, kc)) != null) || ((ch = p.right) != null &amp;&amp; (q = ch.find(h, k, kc)) != null)) return q; &#125; dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K, V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; Node&lt;K, V&gt; xpn = xp.next; TreeNode&lt;K, V&gt; x = map.newTreeNode(h, k, v, xpn); if (dir &lt;= 0) xp.left = x; else xp.right = x; xp.next = x; x.parent = x.prev = xp; if (xpn != null) ((TreeNode&lt;K, V&gt;) xpn).prev = x; moveRootToFront(tab, balanceInsertion(root, x)); return null; &#125; &#125; &#125; /** * Removes the given node, that must be present before this call. * This is messier than typical red-black deletion code because we * cannot swap the contents of an interior node with a leaf * successor that is pinned by &quot;next&quot; pointers that are accessible * independently during traversal. So instead we swap the tree * linkages. If the current tree appears to have too few nodes, * the bin is converted back to a plain bin. (The test triggers * somewhere between 2 and 6 nodes, depending on tree structure). */ final void removeTreeNode(HashMap&lt;K, V&gt; map, Node&lt;K, V&gt;[] tab, boolean movable) &#123; int n; if (tab == null || (n = tab.length) == 0) return; int index = (n - 1) &amp; hash; TreeNode&lt;K, V&gt; first = (TreeNode&lt;K, V&gt;) tab[index], root = first, rl; TreeNode&lt;K, V&gt; succ = (TreeNode&lt;K, V&gt;) next, pred = prev; if (pred == null) tab[index] = first = succ; else pred.next = succ; if (succ != null) succ.prev = pred; if (first == null) return; if (root.parent != null) root = root.root(); if (root == null || root.right == null || (rl = root.left) == null || rl.left == null) &#123; tab[index] = first.untreeify(map); // too small return; &#125; TreeNode&lt;K, V&gt; p = this, pl = left, pr = right, replacement; if (pl != null &amp;&amp; pr != null) &#123; TreeNode&lt;K, V&gt; s = pr, sl; while ((sl = s.left) != null) // find successor s = sl; boolean c = s.red; s.red = p.red; p.red = c; // swap colors TreeNode&lt;K, V&gt; sr = s.right; TreeNode&lt;K, V&gt; pp = p.parent; if (s == pr) &#123; // p was s&#x27;s direct parent p.parent = s; s.right = p; &#125; else &#123; TreeNode&lt;K, V&gt; sp = s.parent; if ((p.parent = sp) != null) &#123; if (s == sp.left) sp.left = p; else sp.right = p; &#125; if ((s.right = pr) != null) pr.parent = s; &#125; p.left = null; if ((p.right = sr) != null) sr.parent = p; if ((s.left = pl) != null) pl.parent = s; if ((s.parent = pp) == null) root = s; else if (p == pp.left) pp.left = s; else pp.right = s; if (sr != null) replacement = sr; else replacement = p; &#125; else if (pl != null) replacement = pl; else if (pr != null) replacement = pr; else replacement = p; if (replacement != p) &#123; TreeNode&lt;K, V&gt; pp = replacement.parent = p.parent; if (pp == null) root = replacement; else if (p == pp.left) pp.left = replacement; else pp.right = replacement; p.left = p.right = p.parent = null; &#125; TreeNode&lt;K, V&gt; r = p.red ? root : balanceDeletion(root, replacement); if (replacement == p) &#123; // detach TreeNode&lt;K, V&gt; pp = p.parent; p.parent = null; if (pp != null) &#123; if (p == pp.left) pp.left = null; else if (p == pp.right) pp.right = null; &#125; &#125; if (movable) moveRootToFront(tab, r); &#125; /** * 将结点太多的桶分割 * * @param map the map * @param tab the table for recording bin heads * @param index the index of the table being split * @param bit the bit of hash to split on */ final void split(HashMap&lt;K, V&gt; map, Node&lt;K, V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K, V&gt; b = this; // Relink into lo and hi lists, preserving order TreeNode&lt;K, V&gt; loHead = null, loTail = null; TreeNode&lt;K, V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; for (TreeNode&lt;K, V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K, V&gt;) e.next; e.next = null; if ((e.hash &amp; bit) == 0) &#123; if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; if (loHead != null) &#123; if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125; &#125; if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125; &#125; /* ------------------------------------------------------------ */ // 红黑树方法，都是从CLR中修改的 /** * 左旋转 * * @param root * @param p * @param &lt;K&gt; * @param &lt;V&gt; * @return */ static &lt;K, V&gt; TreeNode&lt;K, V&gt; rotateLeft(TreeNode&lt;K, V&gt; root, TreeNode&lt;K, V&gt; p) &#123; TreeNode&lt;K, V&gt; r, pp, rl; if (p != null &amp;&amp; (r = p.right) != null) &#123; if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; &#125; return root; &#125; /** * 右旋转 * * @param root * @param p * @param &lt;K&gt; * @param &lt;V&gt; * @return */ static &lt;K, V&gt; TreeNode&lt;K, V&gt; rotateRight(TreeNode&lt;K, V&gt; root, TreeNode&lt;K, V&gt; p) &#123; TreeNode&lt;K, V&gt; l, pp, lr; if (p != null &amp;&amp; (l = p.left) != null) &#123; if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; &#125; return root; &#125; /** * 保证插入后平衡 * * @param root * @param x * @param &lt;K&gt; * @param &lt;V&gt; * @return */ static &lt;K, V&gt; TreeNode&lt;K, V&gt; balanceInsertion(TreeNode&lt;K, V&gt; root, TreeNode&lt;K, V&gt; x) &#123; x.red = true; for (TreeNode&lt;K, V&gt; xp, xpp, xppl, xppr; ; ) &#123; if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; else if (!xp.red || (xpp = xp.parent) == null) return root; if (xp == (xppl = xpp.left)) &#123; if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; xppr.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.right) &#123; root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; else &#123; if (xppl != null &amp;&amp; xppl.red) &#123; xppl.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; if (x == xp.left) &#123; root = rotateRight(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125; &#125; /** * 删除后调整平衡 * * @param root * @param x * @param &lt;K&gt; * @param &lt;V&gt; * @return */ static &lt;K, V&gt; TreeNode&lt;K, V&gt; balanceDeletion(TreeNode&lt;K, V&gt; root, TreeNode&lt;K, V&gt; x) &#123; for (TreeNode&lt;K, V&gt; xp, xpl, xpr; ; ) &#123; if (x == null || x == root) return root; else if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; else if (x.red) &#123; x.red = false; return root; &#125; else if ((xpl = xp.left) == x) &#123; if ((xpr = xp.right) != null &amp;&amp; xpr.red) &#123; xpr.red = false; xp.red = true; root = rotateLeft(root, xp); xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr == null) x = xp; else &#123; TreeNode&lt;K, V&gt; sl = xpr.left, sr = xpr.right; if ((sr == null || !sr.red) &amp;&amp; (sl == null || !sl.red)) &#123; xpr.red = true; x = xp; &#125; else &#123; if (sr == null || !sr.red) &#123; if (sl != null) sl.red = false; xpr.red = true; root = rotateRight(root, xpr); xpr = (xp = x.parent) == null ? null : xp.right; &#125; if (xpr != null) &#123; xpr.red = (xp == null) ? false : xp.red; if ((sr = xpr.right) != null) sr.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateLeft(root, xp); &#125; x = root; &#125; &#125; &#125; else &#123; // symmetric if (xpl != null &amp;&amp; xpl.red) &#123; xpl.red = false; xp.red = true; root = rotateRight(root, xp); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl == null) x = xp; else &#123; TreeNode&lt;K, V&gt; sl = xpl.left, sr = xpl.right; if ((sl == null || !sl.red) &amp;&amp; (sr == null || !sr.red)) &#123; xpl.red = true; x = xp; &#125; else &#123; if (sl == null || !sl.red) &#123; if (sr != null) sr.red = false; xpl.red = true; root = rotateLeft(root, xpl); xpl = (xp = x.parent) == null ? null : xp.left; &#125; if (xpl != null) &#123; xpl.red = (xp == null) ? false : xp.red; if ((sl = xpl.left) != null) sl.red = false; &#125; if (xp != null) &#123; xp.red = false; root = rotateRight(root, xp); &#125; x = root; &#125; &#125; &#125; &#125; &#125; /** * 检测是否符合红黑树 */ static &lt;K, V&gt; boolean checkInvariants(TreeNode&lt;K, V&gt; t) &#123; TreeNode&lt;K, V&gt; tp = t.parent, tl = t.left, tr = t.right, tb = t.prev, tn = (TreeNode&lt;K, V&gt;) t.next; if (tb != null &amp;&amp; tb.next != t) return false; if (tn != null &amp;&amp; tn.prev != t) return false; if (tp != null &amp;&amp; t != tp.left &amp;&amp; t != tp.right) return false; if (tl != null &amp;&amp; (tl.parent != t || tl.hash &gt; t.hash)) return false; if (tr != null &amp;&amp; (tr.parent != t || tr.hash &lt; t.hash)) return false; if (t.red &amp;&amp; tl != null &amp;&amp; tl.red &amp;&amp; tr != null &amp;&amp; tr.red) return false; if (tl != null &amp;&amp; !checkInvariants(tl)) return false; if (tr != null &amp;&amp; !checkInvariants(tr)) return false; return true; &#125; &#125;&#125; JDK7和JDK8对区别 由 数组+链表 的结构改为 数组+链表+红黑树 。 拉链过长会严重影响hashmap的性能，所以1.8的hashmap引入了红黑树。 在链表元素数量超过8时改为红黑树，少于6时改为链表，中间7不改是避免频繁转换降低性能。 相对于链表，改为红黑树后碰撞元素越多查询效率越高。链表O(n)，红黑树O(logn)。 优化了高位运算的hash算法：h^(h&gt;&gt;&gt;16)将hashcode无符号右移16位，让高16位和低16位进行异或。 扩容后，元素要么是在原位置，要么是在原位置再移动2次幂的位置，且链表顺序不变。 不需要重新计算hash，只需要根据原来hash值新增的bit是1还是0分别放进两个链表lo和hi（非红黑树的情况）里，0的话索引没变，1的话索引变为原索引加原来的数组长度。因为用的尾插法所以新数组链表不会倒置，多线程下不会出现死循环。 put 方法链表头插改为尾插 总结 扩容是一个特别耗性能的操作，所以当程序员在使用HashMap的时候，估算map的大小，初始化的时候给一个大致的数值，避免map进行频繁的扩容。 负载因子是可以修改的，也可以大于1，但是建议不要轻易修改，除非情况非常特殊。 HashMap是线程不安全的，不要在并发的环境中同时操作HashMap，建议使用ConcurrentHashMap。 JDK1.8引入红黑树大程度优化了HashMap的性能。","categories":[{"name":"源码","slug":"源码","permalink":"https://gleaming.cn/categories/%E6%BA%90%E7%A0%81/"}],"tags":[]},{"title":"LinkedList源码分析","slug":"LindedList源码分析","date":"2021-04-23T10:55:30.000Z","updated":"2022-07-28T07:52:51.994Z","comments":true,"path":"LinkedList-source-code/","link":"","permalink":"https://gleaming.cn/LinkedList-source-code/","excerpt":"","text":"LinkedList特性LinkedList内部使用双向链表作为存储结构,LinkedLIst可以理解为链表的扩展对象,简单封装了常用和非常用的操作链表的方法,以及在通过索引获取元素时的简单优化.LinkedList的特点如下： 根据索引在链表中检索数据的时间复杂度时O(n). 在链表的头部和尾部写入或删除元素效率高 实现了List和Duque的全部功能,可以把它当作一个集合或队列使用 链表中允许保存NULL 链表是非线程安全 源码重要成员变量1234567891011121314151617181920212223//链表中元素的数量transient int size = 0;///链表头节点transient Node&lt;E&gt; first;//链表尾节点transient Node&lt;E&gt; last;// 链表元素的定义private static class Node&lt;E&gt; &#123; // 存储的元素 E item; // 后继结点 Node&lt;E&gt; next; // 前驱结点 Node&lt;E&gt; prev; // 前驱结点、存储的元素和后继结点作为参数的构造方法 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; Node的数据结构是一个双向链表 构造函数123456789//默认构造函数public LinkedList()&#123;&#125;//根据哟个Collection初始化链表public LinkedList(Collection&lt;? entends E&gt; c)&#123; this(); addAll(c);&#125; 常用方法add、addFirst与addLast12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 在链表头部添加元素public void addFirst(E e) &#123; linkFirst(e);&#125;// 在链表尾部添加元素public void addLast(E e) &#123; linkLast(e);&#125;// 添加方法，默认添加到尾部public boolean add(E e) &#123; linkLast(e); return true;&#125;private void linkFirst(E e) &#123; // 添加e之前的第一个元素 final Node&lt;E&gt; f = first; // 创建一个node节点，prev=null,next=f final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); first = newNode; // 判断是否是第一个添加到链表中的元素 // 如果是则last也等于该元素 // 如果不是，则与f进行关联 if (f == null) last = newNode; else f.prev = newNode; // 长度+1 size++; modCount++;&#125;void linkLast(E e) &#123; // 添加e之前的最后一个元素 final Node&lt;E&gt; l = last; // 创建一个node节点，prev=l,next=null final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; // 判断是否是第一个添加到链表中的元素 // 如果是则first也等于该元素 // 如果不是，则与l进行关联 if (l == null) first = newNode; else l.next = newNode; // 长度+1 size++; modCount++;&#125; add(int index, E element)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 在指定位置添加元素public void add(int index, E element) &#123; // 判断索引正确 checkPositionIndex(index); // 如果在链表最后面添加，则调用linkLast将元素添加到尾部 // 否则先通过node(index)获取到元素，然后将element添加到该元素前面 if (index == size) linkLast(element); else linkBefore(element, node(index));&#125;// 索引越界会返回IndexOutOfBoundsException异常private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;// index在0-size之间private boolean isPositionIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;// 根据索引查找Node节点Node&lt;E&gt; node(int index) &#123; // index &lt; (size &gt;&gt; 1)最多只遍历一般的Node节点 // 如果index在链表的前半段，则从前向后遍历 // 否则从后向前遍历 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125;void linkBefore(E e, Node&lt;E&gt; succ) &#123; // 涉及到了3个Node的关系 final Node&lt;E&gt; pred = succ.prev; // 创建一个新的Node，prev=[index].prev，next=[index] final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // index对应的Node的prev=新Node succ.prev = newNode; // 判断是否在first位添加了e，如果是，则把first=新Node if (pred == null) first = newNode; // 否则[index-1].next等于新Node else pred.next = newNode; size++; modCount++;&#125; 在指定位置add一个元素,其性能要比在头和尾add一个元素满. 原因在于需要遍历链表,找到index位置的元素,然后将要添加的元素、index位置的元素、以及index-1位置的元素重新建立关系. 源码中的优化: 如果index==size,直接添加到尾部,不进行遍历链表. 遍历时,先判断index在链表的前部还是后部,然后采取从前向后遍历还是从后向前遍历. addAll123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 从链表尾部添加c中的所有元素，构造函数使用的就是这个addAllpublic boolean addAll(Collection&lt;? extends E&gt; c) &#123; return addAll(size, c);&#125;// 从index位置之后，添加c中的所有元素public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; // 检查索引是否越界 checkPositionIndex(index); // 判断c中是否有元素，如果没有则返回false Object[] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; // 添加c之前的准备工作，准备好上一个node和下一个node Node&lt;E&gt; pred, succ; if (index == size) &#123; // 如果从尾部添加c // 则保存last到pred变量，作为要添加节点的上一个Node // succ变量为null，从尾部添加没有next succ = null; pred = last; &#125; else &#123; // 如果从链表其他部分添加c // 则获取index位置的元素，设置位下一个Node // pred为index位置的元素的prev succ = node(index); pred = succ.prev; &#125; // 循环a数组开始添加到链表 for (Object o : a) &#123; @SuppressWarnings(&quot;unchecked&quot;) E e = (E) o; // 创建要添加的node Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, null); if (pred == null) first = newNode; // 是否要更新first变量 else pred.next = newNode; // 添加node到链表 // 更新上一个node的变量 pred = newNode; &#125; if (succ == null) &#123; // 如果是从链表尾部添加，则last变量=最后一个node last = pred; &#125; else &#123; // 否则将最后一个添加的node的next设置为succ，succ的prev=最后一个node pred.next = succ; succ.prev = pred; &#125; // 更新链表长度 size += numNew; modCount++; return true;&#125; set(int index, E element)12345678910public E set(int index, E element)&#123; // 验证索引越界 checkElementIndex(index); // 获取指定位置的元素 Node&lt;E&gt; x = node(index); // 更新值，并返回旧值 E oldVal = x.item; x.item = element; return oldVal;&#125; removeFirst,removeLast123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657// 删除第一个元素public E removeFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return unlinkFirst(f);&#125;// 删除最后一个元素public E removeLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return unlinkLast(l);&#125;private E unlinkFirst(Node&lt;E&gt; f) &#123; final E element = f.item; // 获取f的下一个元素保存到next变量 final Node&lt;E&gt; next = f.next; // 置空，便于GC回收 f.item = null; f.next = null; // help GC // 把first设置成next，并且判断next是否为null // 如果next为null，则说明删除了f后，链表中没有元素了，那么把last设置成null // 否则，把next的prev设置成null，代表他是第一个元素 first = next; if (next == null) last = null; else next.prev = null; // 链表长度-1 size--; modCount++; return element;&#125;private E unlinkLast(Node&lt;E&gt; l) &#123; final E element = l.item; // 获取上一个元素保存到prev变量 final Node&lt;E&gt; prev = l.prev; // 置空，便于GC回收 l.item = null; l.prev = null; // help GC // 把last设置成prev，并且判断prev是否为null // 如果prev为null，则说明删除了l后，链表中没有元素了，那么把first设置成null // 否则，把prev的next设置成null，代表他是第最后一个元素 last = prev; if (prev == null) first = null; else prev.next = null; // 链表长度-1 size--; modCount++; return element;&#125; remove(int index)12345678910111213141516171819202122232425262728293031323334353637// 删除指定索引位置的元素public E remove(int index) &#123; // 判断索引是否越界 checkElementIndex(index); return unlink(node(index));&#125;E unlink(Node&lt;E&gt; x) &#123; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; // 判断要删除的node是否是第一个node // 如果是就更新first变量 // 否则断开这个node与前一个node的关联关系 if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; // 判断要删除的node是否是最后一个node // 如果是就更新last变量 // 否则断开这个node与后一个node的关联关系 if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; // 便于GC x.item = null; size--; modCount++; return element;&#125; 在指定位置删除元素,依旧会遍历找到index对应的元素,然后断开这个元素的前后node.对这个元素是否时头节点和尾节点进行了优化. remove(Object o)1234567891011121314151617181920public boolean remove(Object o) &#123; // 判断要删除的元素是否是null，如果是，则用 == null进行判断，然后调用unlink if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; // 如果不是null，则用item.eq判断，然后调用unlink for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false;&#125; 根据对象删除元素比根据index删除元素的效率可能还要低,因为没办法判断index &lt; (size&gt;&gt;1),就只能从前向后一直遍历. removeFirstOccurrence和removeLastOccurrence1234567891011121314151617181920212223// 从链表前段开始删除元素，其实就是调用的remove(o)，因为remove(o)是正向循环public boolean removeFirstOccurrence(Object o) &#123; return remove(o);&#125;// 从链表尾部删除元素，做法与remove(o)相反，是反向循环查找元素public boolean removeLastOccurrence(Object o) &#123; if (o == null) &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false;&#125; 这两个方法是JDK1.6提供的,弥补了remove(o)方法无法从链表尾部开始循环带来的极端情况下,时间复杂度为O(n)的问题 clear123456789101112131415161718public void clear() &#123; // Clearing all of the links between nodes is &quot;unnecessary&quot;, but: // - helps a generational GC if the discarded nodes inhabit // more than one generation // - is sure to free memory even if there is a reachable Iterator // 循环清空链表 for (Node&lt;E&gt; x = first; x != null; ) &#123; Node&lt;E&gt; next = x.next; x.item = null; x.next = null; x.prev = null; x = next; &#125; // 设置first和last变量=null，链表长度=0 first = last = null; size = 0; modCount++;&#125; getFirst,getLast,get123456789101112131415161718192021// 获取链表的第一个元素，如果为null会抛出异常public E getFirst() &#123; final Node&lt;E&gt; f = first; if (f == null) throw new NoSuchElementException(); return f.item;&#125;// 获取链表的最后一个元素，如果为null会抛出异常public E getLast() &#123; final Node&lt;E&gt; l = last; if (l == null) throw new NoSuchElementException(); return l.item;&#125;// 获取指定位置的元素，判断索引是否越界public E get(int index) &#123; checkElementIndex(index); return node(index).item;&#125; 如果这是一个空链表,这两个方法获取元素会抛出异常 注意,代码中判断的是fitst和last,并不是元素的item,所以链表中是可以保存null的. 其他函数contains1234public boolean contains(Object o) &#123; // 调用indexOf(o)方法 return indexOf(o) != -1;&#125; size123public int size()&#123; return size;&#125; indexOf12345678910111213141516171819public int indexOf(Object o) &#123; int index = 0; // 为null的判断 if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) return index; index++; &#125; &#125; else &#123; // 不为空的判断 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) return index; index++; &#125; &#125; return -1;&#125; 从前往后循环,查找匀速出现的第一次索引. lastIndexOf123456789101112131415161718// 类似indexOf，只不过是从后向前查找public int lastIndexOf(Object o) &#123; int index = size; if (o == null) &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (x.item == null) return index; &#125; &#125; else &#123; for (Node&lt;E&gt; x = last; x != null; x = x.prev) &#123; index--; if (o.equals(x.item)) return index; &#125; &#125; return -1;&#125; 从后往前循环,查找元素出现的最后一次索引. Deque方法Linkedlist不仅实现了链表,还实现了Deque(双端队列)接口. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 返回头元素，不存在返回nullpublic E peek() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item;&#125;// 返回头元素，不存在抛出异常public E element() &#123; return getFirst();&#125;// 弹出头元素，不存在返回nullpublic E poll() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : unlinkFirst(f);&#125;// 弹出头元素，不存在抛出异常public E remove() &#123; return removeFirst();&#125;// 添加元素到头部public boolean offer(E e) &#123; return add(e);&#125;// 添加元素到头部public boolean offerFirst(E e) &#123; addFirst(e); return true;&#125;// 添加元素到尾部public boolean offerLast(E e) &#123; addLast(e); return true;&#125;// 返回头元素，不存在返回nullpublic E peekFirst() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : f.item;&#125;// 返回尾元素，不存在返回nullpublic E peekLast() &#123; final Node&lt;E&gt; l = last; return (l == null) ? null : l.item;&#125;// 弹出头元素，不存在返回nullpublic E pollFirst() &#123; final Node&lt;E&gt; f = first; return (f == null) ? null : unlinkFirst(f);&#125;// 弹出尾元素，不存在返回nullpublic E pollLast() &#123; final Node&lt;E&gt; l = last; return (l == null) ? null : unlinkLast(l);&#125;// 添加元素到头部public void push(E e) &#123; addFirst(e);&#125;// 弹出头部元素，不存在抛出异常public E pop() &#123; return removeFirst();&#125; 总结 LinkedList内部使用双向链表作为数据结构存储数据，一切符合链表的特性都对它生效； LinkedList从头部，尾部添加删除元素的时间复杂度是O(1)，在中间位置插入或删除元素时不会产生ArrayList的扩容问题，但需要遍历到指定Node； LinkedList通过index检索元素进行了index &lt; (size &gt;&gt; 1)的优化，但通过object检索元素并没有优化； LinkedList基于双向链表这种数据结构，对双端队列操作进行了实现； 所以，LinkedList适合在频繁的写入和删除，但检索相对较少的场景。因为写入和删除不会进行扩容，若在头部和尾部写入或删除元素，不会进行检索，时间复杂度是O(1)，就算进行检索，经过了index &lt; (size &gt;&gt; 1)的优化，时间复杂度最多会是O(n/2)。","categories":[{"name":"源码","slug":"源码","permalink":"https://gleaming.cn/categories/%E6%BA%90%E7%A0%81/"}],"tags":[]},{"title":"ArrayList源码解析","slug":"ArrayList源码解析","date":"2021-04-21T13:33:17.000Z","updated":"2022-07-28T07:52:51.992Z","comments":true,"path":"ArrayList-source-code/","link":"","permalink":"https://gleaming.cn/ArrayList-source-code/","excerpt":"","text":"ArrayList的特性ArrayList内部使用数组作为存储结构,ArrayList可以理解为数组的扩展对象,封装了常用和非常用的操作数组的方法.以及当数组长度不足时,自动扩容数组.ArrayList的特点如下: 根据索引查询数据的时间复杂度为O(1); 在数组中间部分根据索引写入或删除元素效率低; 写入数组时若数组长度不足则会出发扩容; ArrayList允许保存NULL值; ArrayList是非线程安全的设计; 源码重要成员变量12345678910111213141516//ArrayList默认的容量长度private static final int DEFAULT_CAPACITY = 10;//定义一个空数组对象private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;//当ArrayList中没有任何对象时，使用该变量赋值，是一个默认的缺省空数组对象//用于无参构造函数为elementData赋值，或判断数组是否是缺省值private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;//ArrayList中真正保存数据的数组transient Object[] elementData; // non-private to simplify nested class access//数组中保存的对象的个数private int size; elementData:ArrayList中真正储存数据的对象,也证明ArrayList内部是由数组实现的; DEFAULT_CAPACITY:它表示数组在一般情况下,初次扩容的长度; ArrayList默认初始化的长度并不是0; 构造函数无参构造函数12345public ArrayList() &#123; // 用缺省的DEFAULTCAPACITY_EMPTY_ELEMENTDATA来赋值给elementData数组 // 变量定义：static final DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 这说明通过ArrayList list = new ArrayList()创建一个ArrayList时，初始化长度是0，而不是10。 根据指定容量创建1234567891011121314// initialCapacity是new ArrayList(5)时传递的指定容量public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; // 根据initialCapacity实例化elementData对象 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; // 如果传进来的是0，那么则用EMPTY_ELEMENTDATA初始化，这里要跟DEFAULTCAPACITY_EMPTY_ELEMENTDATA做好区分 // 变量定义：static final EMPTY_ELEMENTDATA = &#123;&#125; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125;&#125; 根据其他类型集合来初始化1234567891011121314// Collection是多种集合（List,Set,Queue）的的顶级接口// 也就是说，实现Collection接口的对象都可以用来初始化ArrayListpublic ArrayList(Collection&lt;? extends E&gt; c) &#123; elementData = c.toArray(); if ((size = elementData.length) != 0) &#123; // c.toArray might (incorrectly) not return Object[] (see 6260652) // 调用Arrays.copyOf为elementData赋值 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 如果参数长度为0，则用EMPTY_ELEMENTDATA赋值elementData this.elementData = EMPTY_ELEMENTDATA; &#125;&#125; 数组扩容主动扩容12345678public void ensureCapacity(int minCapacity) &#123; // 获取最小的扩容长度，0或者10 int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) ? 0 : DEFAULT_CAPACITY; // 当主动扩容传入的参数大于最小扩容长度时进行扩容，否则不进行扩容 if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125;&#125; 上述代码为主动扩容,可以通过list.ensureCapacity(参数)由使用者进行主动扩容. 因为扩容涉及到数组的拷贝,所以源码中加入了判断,当要扩容的长度小于最小扩容长度,则不进行扩容. 自动扩容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private void ensureCapacityInternal(int minCapacity) &#123; // 先调用calculateCapacity方法，获取要扩容的长度 // 后调用ensureExplicitCapacity方法，进行扩容 ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 如果数组是初始化数组，则取DEFAULT_CAPACITY和minCapacity中较大的数作为扩容长度 // 也就是说，初始化数组的长度是0，如果第一次添加对象，那么minCapacity将是1，数组会扩容到10 // 所以可以认为，数组初始化是0，当添加第一个对象时，扩容到10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; // 记录数组的修改次数，这个变量在AbstractList类中，是ArrayList的父类 modCount++; // 确认是否进行扩容，也就是说数组elementData的长度不足以存放对象了 // 调用grow进行真正的扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;// elementData的最大长度// 是一个Integer的最大值-8的长度// 之所以减8是因为一些虚拟机对数组的实现，需要用减掉的这个8来保存一些头信息private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;private void grow(int minCapacity) &#123; // 获取旧的容量 int oldCapacity = elementData.length; // 将容量扩大1.5倍, &gt;&gt;1效率更高 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 做一些逻辑判断，若要扩容的长度小于最小扩容长度，也就是比传入的参数小，则扩容长度取传入的参数 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 若要扩容的长度大于Integer.MAX -8,这种极端形况下调用hugeCapacity进行处理 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 扩容！调用Arrays.copyOf方法 elementData = Arrays.copyOf(elementData, newCapacity);&#125;private static int hugeCapacity(int minCapacity) &#123; // 如果minCapacity等于负数，有可能是内存溢出了，则抛出OOM异常 if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); // 否则如果确实是大于了Integer.MAX - 8，但在Integer.MAX内，则返回Integer.MAX // 但能否扩容成功，要根据JVM虚拟机的实现来看了 return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;&#125; 自动扩容为原数组长度的1.5倍,用&gt;&gt;1效率更高; 扩容的最大长度,理想状态下应该是Integer.MAX-8的长度,以为一些JVM实现数组需要保存头信息,会占用数组的长度; 当极端情况下超过了Integer.MAX-8,则扩容到Integer.MAX; 常用方法get方法123456789101112131415161718192021public E get(int index) &#123; // 检测索引是否越界 rangeCheck(index); // 从数组中找到数据并返回，时间复杂度的O(1) return elementData(index);&#125;private void rangeCheck(int index) &#123; // 检测索引是否越界，如果越界则抛出异常 if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private String outOfBoundsMsg(int index) &#123; return &quot;Index: &quot;+index+&quot;, Size: &quot;+size;&#125;E elementData(int index) &#123; // 返回数组中制定索引的数据 return (E) elementData[index];&#125; get方法的时间复杂度为$\\mathrm O(1)$ set方法12345678public E set(int index, E element) &#123; // 检测索引越界 rangeCheck(index); // 获取索引对应的值，将索引位置的值设置成element，然后返回旧值，时间复杂度是O(1) E oldValue = elementData(index); elementData[index] = element; return oldValue; &#125; set方法的时间复杂度为$\\mathrm O(1)$ add方法1234567891011public boolean add(E e) &#123; // 将ArrayList长度+1，然后调用内部扩容函数 // 若数组还有空间则不会进行扩容 ensureCapacityInternal(size + 1); // 保存到数组最后面 elementData[size++] = e; return true;&#125;if (minCapacity - elementData.length &gt; 0) grow(minCapacity); 如果elementData.length的长度小于minCapacity(也就是size+1),那么会将数组扩容1.5倍. 不触发扩容的情况下add(E e)的时间复杂度是O(1) 123456789101112131415161718public void add(int index, E element) &#123; // 判断索引是否越界 rangeCheckForAdd(index); // 触发扩容 ensureCapacityInternal(size + 1); // 将index位置后的元素统一向后移动 System.arraycopy(elementData, index, elementData, index + 1, size - index); // 将数组保存到索引位置，并将长度+1 elementData[index] = element; size++;&#125;private void rangeCheckForAdd(int index) &#123; // 索引不能大于ArrayList的size，并且不能小于0 if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125; 调用add(int index, E element)效率并不高，每次add一个元素，都要移动该元素后面的所有元素。最坏的情况下时间复杂度是O(n) remove方法12345678910111213141516public E remove(int index) &#123; // 检查索引越界 rangeCheck(index); // 记录修改次数 modCount++; // 获取要被删除的元素，准备在方法结束时返回给调用者 E oldValue = elementData(index); // 这里做了一个判断，如果删除的不是最后一个元素，则需要将index后的元素向前移动 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 将最后一个元素位置置为null，便于GC回收 elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 与add(int index, E element)类似，如果从中间删除时，数组会将index后的元素统一向前移动。 从数组中间位置删除元素效率低的原因就在于此，最坏的情况下时间复杂度是O(n) 123456789101112131415161718192021222324252627282930public boolean remove(Object o) &#123; if (o == null) &#123; // 如果对象是null，则找到第一个null元素的索引，并从数组中删除，查找方式是从前到后 for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; // 如果对象不是null，则找到对象的索引，从数组中删除，查找方式也是从前到后 for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125;private void fastRemove(int index) &#123; // 记录数组操作次数 modCount++; // 如果删除的元素不在数组尾部，则需要向前移动数组 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); // 最后以为元素设置为null，便于GC回收 elementData[--size] = null; // clear to let GC do its work&#125; remove(Object o)的做法是先循环找到o对应的index，然后跟remove(int index)执行差不多逻辑，将index后面的数据向前移动。 remove(Object o)，从前向后查找元素，并将后面的元素向前移动，所以它的时间复杂度是是O(n) 12345678910111213141516171819202122232425262728293031323334353637383940414243public boolean removeAll(Collection&lt;?&gt; c) &#123; // c不能为null Objects.requireNonNull(c); // 核心方法在这里，传入false代表删除elementData和c中重复的元素 return batchRemove(c, false);&#125;public boolean retainAll(Collection&lt;?&gt; c) &#123; // c不能为null Objects.requireNonNull(c); // 核心方法在这里，传入true代表保留elementData和c中重复的元素，删除不通的元素，可以理解为取交集 return batchRemove(c, true);&#125;private boolean batchRemove(Collection&lt;?&gt; c, boolean complement) &#123; final Object[] elementData = this.elementData; int r = 0, w = 0; boolean modified = false; // 使用try-finally结构，支持快速失败 try &#123; // 将两个数组中重复的元素，被后一个元素前移，替换掉 for (; r &lt; size; r++) if (c.contains(elementData[r]) == complement) elementData[w++] = elementData[r]; &#125; finally &#123; // 删除数组尾部无用的元素 if (r != size) &#123; System.arraycopy(elementData, r, elementData, w, size - r); w += size - r; &#125; if (w != size) &#123; // clear to let GC do its work for (int i = w; i &lt; size; i++) elementData[i] = null; modCount += size - w; size = w; modified = true; &#125; &#125; return modified;&#125; removeAll方法是用来删除两个数组中都存在的数据； retainAll方法是用来删除两个数组中不一致的数据； 核心方法是batchRemove方法中，确保时间复杂度的一个不确定因素是c.contains方法，如果该方法的时间复杂度是O(1)，那么整个batchRemove的时间复杂度将会是O(n)。 其他方法1234567public int size() &#123; return size;&#125;public boolean isEmpty() &#123; return size == 0;&#125; 123456789101112131415161718192021222324252627public int indexOf(Object o) &#123; // 从前到后查找 if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) if (o.equals(elementData[i])) return i; &#125; return -1;&#125;public int lastIndexOf(Object o) &#123; // 从后到前查找 if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1;&#125; indexOf和lastIndexOf的时间复杂度都是O(n) 1234public boolean contains(Object o) &#123; // 调用indexOf来判断是否存在 return indexOf(o) &gt;= 0;&#125; 这里就验证了batchRemove方法中c.contains会确定该方法的时间复杂度的说法。如果removeAll传入的参数也是一个ArrayList，那么removeAll方法是时间复杂度将会是$\\mathrm O(\\mathrm n\\hat{}2)$ trimToSize123456789public void trimToSize() &#123; modCount++; // 如果elementData的长度大于ArrayList的size，则说明有剩余空间，可以进行裁剪 if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125;&#125; 虽然ArrayList没有自动缩容的功能,但提供了方法,可以让使用者主动调用,进行空间的释放. 总结 ArrayList内部是由数组实现的； ArrayList在创建时候可以指定数组容量； ArrayList的get和set方法的时间复杂度是O(1)，添加元素到尾部的时间复杂度是O(1)； ArrayList内部数组容量不足时会自动扩容，扩容比例是1.5倍； ArrayList从数组中间删除或添加元素性能不高； 所以，ArrayList的适用与顺序添加元素到数组中，并可以通过索引检索数据的场景，如果可以确定元素个数还可以减少扩容次数。","categories":[{"name":"源码","slug":"源码","permalink":"https://gleaming.cn/categories/%E6%BA%90%E7%A0%81/"}],"tags":[]},{"title":"JVM新生代为什么需要两个Survivor空间","slug":"JVM新生代为什么需要两个Survivor空间","date":"2021-02-13T13:30:52.000Z","updated":"2022-07-28T07:52:51.993Z","comments":true,"path":"Why-two-survivor-spaces/","link":"","permalink":"https://gleaming.cn/Why-two-survivor-spaces/","excerpt":"","text":"我们知道，目前主流的虚拟机实现都采用了分代收集的思想，把整个堆区划分为新生代和老年代；新生代又被划分成 Eden 空间、 From Survivor 和 To Survivor 三块区域。 那么问题就来了，为什么必须是两个Survivor空间呢？其实要回答这个问题，我们首先要知道为什么需要Survivor空间？为什么不是一个Survivor空间？ 为什么不是0个Survivor空间？这个问题其实等价于：为什么需要Survivor空间。如果没有Survivor空间，垃圾回收过程为：一遍新生代GC后，所有活着的对象全部进入老年代，即使它在接下来的几次GC过程中极有可能会被回收掉。这样的话老年代会很快被填满，Full GC 的频率被大大提高。老年代一般比新生代大很多，对它进行垃圾回收会消耗比较长的时间；如果收集的频率又很快的话，那就大大降低了效率。基于这种考虑，虚拟机引入了幸存区的概念。如果对象在某次新生代GC后仍然存活，让它暂时进入幸存区；以后每熬过一次GC，就让对象的年龄加一，知道年龄达到某个设定的值(比如15)，JVM才会将它转入老年代。 总之，设置Survivor空间的目的是让那些中等寿命尽量在Minor GC时被干掉，最终在总体上减少垃圾回收对用户产生的影响。 为什么不是1个Survivor空间？回答这个问题一般有一个前提，就是新生代一般哦度再用复制算法进行垃圾回收。原始的复制算法是把一块内存分成两个部分，GC时把存活的对象从一块空间(From space)复制到另一块空间(To space)，再把原来的内存(From space)清理干净，最后调换From space和To space的逻辑角色。 在 HotSpot 虚拟机里， Eden 空间和 Survivor 空间默认的比例是 8:1 。我们来看看在只有一个 Survivor 空间的情况下，这个 8:1 会有什么问题。此处为了方便说明，我们假设新生代一共为 9 MB 。对象优先在 Eden 区分配，当 Eden 空间满 8 MB 时，触发第一次 Minor GC 。比如说有 0.5 MB的对象存活，那这 0.5 MB 的对象将由 Eden 区向 Survivor 区复制。这次 Minor GC 过后， Eden 区被清理干净， Survivor 区被占用了 0.5 MB ，还剩 0.5 MB 。到这里一切都很美好，但问题马上就来了：从现在开始所有对象将会在这剩下的 0.5 MB 的空间上被分配，很快就会发现空间不足，于是只好触发下一次 Minor GC 。可以看出在这种情况下，当 Survivor 空间作为对象“出生地”的时候，很容易触发 Minor GC ，这种 8:1 的不对称分配不但没能在总体上降低 Minor GC 的频率，还会把 gc 的时间间隔搞得很不平均。把 Eden : Survivor 设成 1 : 1 也一样，每当对象总大小满 5 MB 的时候都必须触发一次 Minor GC ，唯一的变化是 gc 的时间间隔相对平均了。 为什么2个 Survivor 空间可以达到要求？问题很清楚了，无论 Eden 和 Survivor 的比例怎么设置，在只有一个 Survivor 的情况下，总体上看在新生代空间满一半的时候就会触发一次 Minor GC 。那有没有提升的空间呢？比如说永远在新生代空间满 80% 的时候才触发 Minor GC ？ 事实上是可以做到的：我们可以设两个 Survivor 空间（ From Survivor 和 To Survivor ）。比如，我们把 Eden : From Survivor : To Survivor 空间大小设成 8 : 1 : 1 ，对象总是在 Eden 区出生， From Survivor 保存当前的幸存对象， To Survivor 为空。一次 gc发生后： Eden 区活着的对象 ＋ From Survivor 存储的对象被复制到 To Survivor ； 清空 Eden 和 From Survivor ； 颠倒 From Survivor 和 To Survivor 的逻辑关系： From 变 To ， To 变 From 。 可以看出，只有在 Eden 空间快满的时候才会触发 Minor GC 。而 Eden 空间占新生代的绝大部分，所以 Minor GC 的频率得以降低。当然，使用两个 Survivor 这种方式我们也付出了一定的代价，如 10% 的空间浪费、复制对象的开销等。","categories":[{"name":"JVM","slug":"JVM","permalink":"https://gleaming.cn/categories/JVM/"}],"tags":[{"name":"JVM","slug":"JVM","permalink":"https://gleaming.cn/tags/JVM/"}]},{"title":"Java内存区域探究","slug":"Java内存区域探究","date":"2021-01-12T14:27:50.000Z","updated":"2022-07-28T07:52:51.993Z","comments":true,"path":"Research-on-JAVA-memory-area/","link":"","permalink":"https://gleaming.cn/Research-on-JAVA-memory-area/","excerpt":"","text":"JVM内存结构概述Java虚拟机在执行Java程序的过程中会把它所管理的内存划分为若干个不同的区域。在Java 7中，Java虚拟机所管理的内存如下图所示 在Java 8中，内存结构发生了变化。主要变化是移除了方法区，新增了元数据区的概念。 按线程划分 程序计数器（Program Counter Register） 程序计数器是当前线程正在执行的字节码的地址。程序计数器是线程私有的，每一个线程都有一个独立的程序计数器。 在Java虚拟机中，多线程是靠CPU时间片轮转来调度的，也就是说一个线程可能没有结束就被挂起。而他再次执行时，就要从挂起的地方继续执行。 程序计数器是唯一一个不会出现OOM的区域。 虚拟机栈（Java stack） 生命周期和线程相同，是线程私有的，目的是保证线程中的局部变量不会被别的线程放问到。 每个方法从调用到执行完毕对应着一个栈帧的入栈出栈，所以栈帧不需要垃圾回收 栈帧储存：局部变量、操作数栈、动态链接、方法出入口等信息。 StackOverflowError异常：线程请求的栈深度大于虚拟机允许的深度 OutOfMemoryError异常：虚拟机栈扩展时无法申请到足够的内存 本地方法栈（Native Method Stack） 作用类似虚拟机栈，区别是本地方法栈为Native方法服务 同样会抛出StackOverflowError异常和OutOfMemortError异常 堆（Heap）对于大多数应用来说，Java堆是Java虚拟机所管理的内存中最大的一块。对于大多数刚刚创建的Java应用俩说，如果运行卡顿，第一时间想到的可能是通过JVM挑优提高**-Xmx和-Xms**的数值。 所有线程共享的内存区域，虚拟机启动时创建，用来储存实例对象。 可以是物理上不连续的内存空间。 默认空间分配 新生代（1/3的堆空间） Eden区（8/10） From区（1/10） To区（1/10） 老年代（2/3的堆空间） OutOfMemoryError异常：无法满足内存分配需求且堆无法扩展时 方法区（Method Area）概念方法区（Method Area）是各个线程共享的内存区域，它用于储存已经被虚拟机加载的信息： 非静态的属性 非静态方法的元数据 运行时常量池 方法和构造函数编译后的代码 类加载器初始化或者实例对象初始化用到的特殊方法 在理解它的构成之前，我们先了解一下两个概念：规范和实现。 针对Java虚拟机的实现有专门的《Java虚拟机规范》，在遵守规范的前提下，不同的厂商会对虚拟机进行不同的实现。就好比开发的过程中定义了接口，具体的接口实现大家可以根据不同的业务需求进行实现。 我们通常使用的Java SE都是Sun JDK和OpenJDK所提供，这也是应用最广泛的版本。而该版本使用的VM是HotSpot VM。通常情况下，我们讲的Java虚拟机指的就是HotSpot的版本。 虽然Java虚拟机规范将方法区描述为堆的逻辑部分，但是它却有一个别名叫做Non-堆（非堆），目的应该是将Java堆分开。 很多人把方法区叫做永久代（Permanent Generation）。这是因为在HotSpot中选择将GC分代手机扩展至方法区，或者说用永久代来实现方法区，实际上两者并不等价。将GC分代扩展至方法区后，HotSpot的垃圾收集器就可以像管理Java堆一样去管理这部分的内存，能够省区专门为方法区编写内存管理代码的工作。永久代和堆是相互隔离的，但它们使用的物理内存是连续的。永久代的垃圾收集是和老年代捆绑在一起的，因此无论谁满了，都会触发永久代和老年代的垃圾收集，这就产生了性能问题。而且常量池等一些类的信息也存在方法区内，而类及方法的信息难以确定它的大小。如果加载比较多的类很有可能造成内存溢出。而如果为方法区指定较大的内存的话，势必会压缩老年代内存空间，又容易造成老年代的溢出。 但在Java7中永久代中存储的部分数据已经开始转移到Java Heap或Native Momery中了。比如：符号引用(Symbols)转移到了Native Memory；字符串常量池(interned Strings)转移到了Java Heap；类的静态变量(class statics)转移到了Java Heap。 但在Java8已经取消了方法区这一个概念，而是用元空间来接管方法区的工作。 Java8改动对于Java8，HotSopt取消了永久代，元空间登上舞台，方法区存在于元空间。同时，元空间不再和堆连续，而是存在于本地内存(Native Memory)。当Java Heap空间不足是会触发GC，但Native Memory空间不够时不会触发GC。 元空间存在于本地内存，意味着只要本地内存足够，它不会出现像永久代中OOM的错误。默认情况下元空间是可以无限使用本地内存的，但为了不让它膨胀，JVM同样提供了参数来限制它的使用。 元空间元空间的特点 充分利用了 Java 语言规范中的好处：类及相关的元数据的生命周期与类加载器的一致。 每个加载器有专门的存储空间 只进行线性分配 不会单独回收某个类 省掉了 GC 扫描及压缩的时间 元空间里的对象的位置是固定的 如果 GC 发现某个类加载器不再存活了，会把相关的空间整个回收掉 元空间的内存分配模型 绝大多数的类元数据的空间都从本地内存中分配 用来描述类元数据的类也被删除了 分元数据分配了多个虚拟内存空间 给每个类加载器分配一个内存块的列表。块的大小取决于类加载器的类型；sun / 反射 / 代理对应的类加载器的块会小一些 归还内存块，释放内存块列表 一旦元空间的数据被清空了，虚拟内存的空间会被回收掉 减少碎片的策略 改动后的优势如果你理解了元空间的概念，很容易发现 GC 的性能得到了提升。 Full GC 中，元数据指向元数据的那些指针都不用再扫描了。很多复杂的元数据扫描的代码（尤其是 CMS 里面的那些）都删除了。 元空间只有少量的指针指向 Java 堆。这包括：类的元数据中指向 java/lang/Class 实例的指针；数组类的元数据中，指向 java/lang/Class 集合的指针。 没有元数据压缩的开销 减少了根对象的扫描（不再扫描虚拟机里面的已加载类的字典以及其它的内部哈希表） 减少了 Full GC 的时间 G1 回收器中，并发标记阶段完成后可以进行类的卸载 Hotspot 中的元数据现在存储到了元空间里。mmap 中的内存块的生命周期与类加载器的一致。 类指针压缩空间（Compressed class pointer space）目前仍然是固定大小的，但它的空间较大 可以进行参数的调优，不过这不是必需的。 未来可能会增加其它的优化及新特性。比如， 应用程序类数据共享；新生代 GC 优化，G1 回收器进行类的回收；减少元数据的大小，以及 JVM 内部对象的内存占用量。","categories":[{"name":"Java","slug":"Java","permalink":"https://gleaming.cn/categories/Java/"}],"tags":[]},{"title":"操作系统复习","slug":"操作系统复习","date":"2020-12-16T11:36:04.000Z","updated":"2022-07-28T07:52:51.995Z","comments":true,"path":"OS-Review/","link":"","permalink":"https://gleaming.cn/OS-Review/","excerpt":"","text":"基本定义 并发：是指两个或多个事件在同一时间间隔内发生。 并行：是指两个或多个事件在同一时刻发生。 并发条件：读集与写集是空集、写集和写集是空集。 并发执行特征：失去了封闭性和可再现性。 共享性：是指计算机系统中的各种硬、软件资源都可以为多个用户同时使用。 管态：操作系统的管理程序在执行时CPU所处的状态，又称系统态。 目态：用户程序在执行时CPU所处的状态，又称用户态。 虚拟性：一方面指把物理上的一个实体变成逻辑上的多个对应物，另一方面指虚拟出来的对应物只不过是用户主观上的一种错觉。 进程特性：动态性、并发性、独立性、异步性、结构特性。 进程与程序的区别： 从定义上看，进程是程序处理数据的过程，而程序是一组指令的有序集合； 进程具有动态性、并发性、独立性和异步性等，而程序不具有这些特性； 从进程结构特性来看，他包含程序（以及数据和PCB）； 进程和程序并非一一对应； 进程的三种状态：就绪，执行，阻塞。 临界区：进程中访问临界资源的那段程序代码成为临界区或临界段。 临界区的使用原则：空则让进，忙则等待，等则有限，等则让权。 上锁和开锁的原语 上锁原语：LOCK(W) L1:如果 W=1 那么转向L1； 否则 W=1 返回 开锁原语：UNLOCK(W) W=0; 返回 信号量基本物理含义：当信号量值大于零时表示系统中某类资源的数目；当信号量值小于零时，其绝对值为系统中因请求该类资源而被阻塞的进程数目。 信号量的值的意义： 正数：资源数； 负数：等待资源进程数； 同步、互斥的区别：进程互斥是让各个进程竞争共享资源，资源的使用是各自独立的，相互之间无必然联系；进程同步时并发进程对共享资源的使用必须按照某种逻辑顺序来进行。 死锁定于：两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象。 产生死锁的必要条件： 互斥条件 占有并请求条件。 不可剥夺条件 循环等待条件。 物理地址：内存单元的地址编号，又称绝对地址或实地址。 逻辑地址：用户程序中使用的地址，又称相对地址或虚地址。 静态重定位：在程序执行过程之前由装入程序完成的重定位过程。 动态重定位：在程序执行过程中，由硬件地址变换机构实现重定位的过程。 虚拟内存：一种利用虚拟储存器来逻辑扩充物理内存的技术。 快表：一组相关联寄存器，具有并行能力。用来存在当前最长访问的那些页面的页表目内容。 二级页表：为节约内存，氛围二级页表结构。第一级用１ｋ个表相，每项４个字节，占４KB内存；第二级再用１０位，这样二级表组合起来即可达到１MB个表项。 中断：是指计算机在执行期间，系统内发生了某一急需处理的事件，使得CPU暂时终止当前正在执行的程序而转区执行相应的事件处理程序，待处理完毕后再返回导刚才暂停程序的被中断处继续执行。 I/O通道：是指专门用于负责输入/输出工作的处理机，是大型机必备的为CPU减负的设备。 缓冲技术：在CPU和外设之间设立缓冲区，用以暂存CPU和外设之间交换的数据，从而缓和CPU和外设速度不匹配所产生的矛盾。 I/O控制方式：程序直接控制方式、中断控制方式、DMA控制方式、通道控制方式。 文件物理结构：从系统角度看到的文件信息的组织形式。 算法银行家算法假定系统中有五个进程{P0、P1、P2、P3、P4}和三类资源{A、B、C}，各种资源的数量分别是10、5、7，在T0时刻的资源分配情况如下图所示： 1）判断在T0时刻的安全性。 MAX：进程所需的资源 Allocation：系统已经分配给进程的资源数 Need：进程还需要的资源数 Need = MAX - Allocation Available：系统剩余的资源数 Available =Total - Allocation Work：当前系统所剩资源 work+allocation：计算机处理完当前进程后所剩资源 所以在T0时刻有安全序列{P1，P3，P4，P2，P0}，所以T0时刻是安全的。 2）P1请求资源：P1发出请求向量Request（1，0，2），系统按照银行家算法进行检查。 判断步骤： 先判断请求是否小于等于所需：Request（1，0，2）&lt;= Need(1，2，2) 判断请求的是否小于等于系统还剩的：Request（1，0，2）&lt;=Allocation（3，3，2） 根据请求资源量进行分配（更改表） 列表计算 更改表： 结果： 所以P1可以请求成功。 页面置换算法缺页：当前物理页面中不存在的页面 缺页次数：出现缺页的次数 缺页率：缺页次数/总页数 先进先出算法 FIFO 置换最先调入内存的页面，即置换在内存中驻留时间最久的页面。 按照进入内存的先后次序排列成队列，从队尾进入，从队首删除。但是该算法会淘汰经常访问的页面，不适应进程实际运行的规律，目前已经很少使用。 P 2 3 2 1 5 2 4 5 3 2 5 2 0 2 2 2 2 5 5 5 5 3 3 3 3 1 3 3 3 3 2 2 2 2 2 5 5 2 1 1 1 4 4 4 4 4 2 F=9 Y Y Y Y Y Y Y Y Y 缺页率：9/12*100%=75% 最佳置换算法 OPT 被淘汰的页面将是在未来最长的时间内不再被访问的页面 缺页中断率最低，但是该算法需要依据以后各业的使用情况，而当一个进程还未运行完成是，很难估计哪一个页面是以后不再使用或在最长时间以后才会用到的页面。所以该算法是不能实现的。 P: 2 3 2 1 5 2 4 5 3 2 5 2 M=3 2 2 2 2 2 2 4 4 4 4 4 4 3 3 3 3 3 3 3 3 2 2 2 1 5 5 5 5 5 5 5 5 F=6 Y Y Y Y Y Y 缺页率：6/12*100%=50% 最近最少使用算法LRU 置换最近一段时间以来最长时间未访问过的页面 P 2 3 2 1 5 2 4 5 3 2 5 2 0 2 2 2 2 2 2 2 2 3 3 3 3 1 3 3 3 5 5 5 5 5 5 5 5 2 1 1 1 4 4 4 2 2 2 F=7 Y Y Y Y Y Y Y 缺页率：7/12*100%=58.3% 计算物理地址在采用页式存储管理的系统中，某进程的逻辑地址空间为4页（每页2048字节），已知该进程的页面映像表（页表）如下： 页号 块号 0 2 1 4 2 6 3 8 计算有效逻辑地址4865所对应的物理地址。 地址转换：绝对地址=块号×块长+块内地址 求页号： d = 4865%2048=2……769 对页表：找到块号 6 算地址：物理地址=6*2048+769=13057 磁盘调度一个磁盘驱动器有150个柱面，考虑一个磁盘序列，它按照到达时间顺序分别是35、52、37、17、80、120、135、104，如果读写磁头最初位于柱面90，请使用FCFS、SSTF、SCAN、CSCAN算法求总寻道长度和平均寻道长度。 先来先服务算法 FCFS磁头移动顺序： ​ 90 -&gt; 35 -&gt; 52 -&gt; 37 -&gt; 17 -&gt; 80 -&gt; 120 -&gt; 135 -&gt; 104 总寻道长度 = (90-35) + (52-35) + (52-37) + (37-17) + (80-17) + (120-80) + (135-120) + (135-104) = 256 平均寻道长度 = 总长/移动次数 = 256/8 = 32; 最短寻道时间优先 SSTF先把磁盘序列的时间按照从小到大的顺序排列，磁头从开始柱面移动到距离开始柱面最近的柱面，然后从这个柱面移动到到离这个柱面最近的柱面，以此类推，直到访问所有的柱面。 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 17 35 37 52 80 90 104 120 135 磁头移动顺序： ​ 90 -&gt; 80 -&gt; 104 -&gt; 120 -&gt; 135 -&gt; 52 -&gt; 37 -&gt; 35 -&gt; 17 扫描算法 SCAN17 35 37 52 80 90 104 120 135 磁头从初始柱面向左（右）扫描，一直到头，然后向右（左）扫描。 磁头移动顺序： 向左：90 -&gt; 80 -&gt; 52 -&gt; 37 -&gt; 35 -&gt; 17 -&gt; 104 -&gt; 120 -&gt; 135 向右：90 -&gt; 104 -&gt; 120 -&gt; 135 -&gt; 80 -&gt; 52 -&gt; 37 -&gt; 35 -&gt; 17 循环扫描算法 C-SCAN17 35 37 52 80 90 104 120 135 磁头移动顺序： 向左：90 -&gt; 80 -&gt; 52 -&gt; 37 -&gt; 35 -&gt; 17 -&gt; 135 -&gt; 120 -&gt; 104 向右：90 -&gt; 104 -&gt; 120 -&gt; 135 -&gt; 17 -&gt; 35 -&gt; 37 -&gt; 52 -&gt; 80 向左（右）到头，然后因为是圆形有150个柱面，所以直接转了一圈，计算总寻道长度的时候注意 进程调度从P1到P4有四个进程，每个进程的到达时间和运行时间如下表所示： 进程 到达时间 执行时间 P1 0 8 P2 1 4 P3 2 9 P4 3 5 求FCFS和SJF的平均等待时间、平均周转时间和平均带权周转时间。 先来先服务算法 FCFS按照进程到达的先后次序 ： ​ P1 -&gt; P2 -&gt; P3 -&gt; P4 进程 到达时间 执行时间 开始时间 结束时间 等待时间 周转时间 带权周转时间 P1 0 8 0 8 0 8 1 P2 1 4 8 12 7 11 2.75 P3 2 9 12 21 10 19 2.11 P4 3 5 21 26 18 23 4.6 等待时间 = 开始时间 - 到达时间 周转时间 = 结束时间 - 到达时间 带权周转时间 = 周转时间/执行时间 平均等待时间 = 等待时间/进程数 平均周转时间 = 周转时间/进程数 平均带权周转时间 = 带权周转时间/进程数 短作业优先调度算法 SJF按照进程的长短，进程越短越优先 非抢占 P1 -&gt; P2 -&gt; P4 -&gt; P3 进程 到达时间 执行时间 开始时间 结束时间 等待时间 周转时间 带权周转时间 P1 0 8 0 8 0 8 1 P2 1 4 8 12 7 11 2.75 P4 3 5 12 17 9 14 2.8 P3 2 9 17 26 15 24 2.66 抢占 P1 -&gt; P2 -&gt; P4 -&gt; P1 -&gt; P3 进程 到达时间 执行时间 开始时间 结束时间 等待时间 周转时间 带权周转时间 P1 0 8/7 0/10 1/17 P2 1 4 1 5 P3 2 9 17 26 P4 3 5 5 10","categories":[{"name":"复习","slug":"复习","permalink":"https://gleaming.cn/categories/%E5%A4%8D%E4%B9%A0/"}],"tags":[{"name":"OS","slug":"OS","permalink":"https://gleaming.cn/tags/OS/"}]},{"title":"人工智能复习提纲","slug":"人工智能复习提纲","date":"2020-12-09T09:14:54.000Z","updated":"2022-07-28T07:52:51.994Z","comments":true,"path":"AI-Review-outline/","link":"","permalink":"https://gleaming.cn/AI-Review-outline/","excerpt":"","text":"第一章1. 什么是人工智能 点击查看答案 所谓人工智能就是用人工的方法在机器（计算机）上实现的智能，也称为机器智能。 2. 人工智能研究的基本内容有那些？ 点击查看答案 1. 知识表示 2. 机器感知 3. 机器思维 4. 机器学习 5. 机器行为 3. 人工智能有那些主要的研究领域？ 点击查看答案 1 自动定理证明 、2 博弈、3 模式识别、 4 机器视觉、5 自然语言理解、 6 智能信息检索、7 数据挖掘与知识发现、8 专家系统、9 自动程序设计、10 机器人 、 11 组合优化问题、12 人工神经网络、 13 分布式人工智能与多智能体、14 智能控制、 15 智能仿真、16 智能CAD、17 智能CAI 、18 智能管理与智能决策、19 智能多媒体系统、 20 智能操作系统、21 智能计算机系统、22 智能通信、23 智能网络系统、24 人工生命 第二章1. 什么是知识、命题、产生式？ 点击查看答案 知识：把有关信息关联在一起所形成的信息结构称为知识 命题：命题是一个非真即假的陈述句 产生式：产生式是由条件和结果组成的指令 2. 掌握谓词公式的五个连接词 $\\neg$ : 非 或 否定，表示否定位于它之后的命题 $\\vee$ ：析取，表示被它连接的二个命题具有“或”关系 $\\wedge$ : 合取，表示被它连接的二个命题具有“与”关系 $\\rightarrow$ : 蕴涵 或 条件，$P\\rightarrow Q$ 表示“$P$ 蕴涵$Q$”，即表示“如果$P$，则$Q$”。其中，$P$称为条件的前件，$Q$称为条件的后件。 $\\leftrightarrow$ : 等价 或 双条件，$P \\leftrightarrow Q$ 表示“$P$当且仅当$Q$” 3. 掌握存在量词、全程量词 全程量词$(\\forall x)$: 表示“对于个体域中的所有（或任一个）个体 $x$ ” 存在量词$(\\exists x)$: 表示“对于个体域中存在个体$x$” 4. 自然语句改写成相应的谓词公式例 2.1 用一阶谓词公式表示“每个储蓄钱的人都得到利息”。 解 定义谓词：$save(x)$表示$x$储蓄钱，$interest(x)$表示$x$获得利息。 则“每个储蓄钱的人都得到利息”可以表示为：$(\\forall x)(save(x)\\rightarrow interest(x)) $ 习题2.4 用谓词逻辑表达描述一下列推理： （1）如果张三比李四大，那么李四比张三小。 （2）甲和乙结婚列，则甲为男，乙为女或者甲为女，乙为男 （3）如果一个人是老实人，他就不会说谎；张三说了谎，所以张三不是老实人。 点击查看答案 （1）Older(x,y)：x 比 y 大。Older(Zhang, Li) → ¬Older(Li, Zhang) （2）Man(x): x 为男；¬Man(x): x 为女；Marry(x, y)：x 和 y 结婚。 Marry(甲，乙)→(Man(甲) ∧ ¬Man(乙)) ∨ (Man(乙) ∧ ¬Man(甲)) （3）Honest(x)：x 是老实人；Lie(x)：x 说谎。 Honest(x) → ¬Lie(x);Lie(Zhang) → ¬Honest(Zhang) 5. 熟练运用产生式表达法1. 确定性规则知识的产生式表示：$IF$$P$$THEN$$Q$ 或者 $P\\rightarrow Q$ 例如： IF 动物会飞 AND 会下蛋 THEN 该动物是鸟 2. 不确定性规则知识的产生式表示：$IF$$P$$THEN$$Q$（置信度）或者$P\\rightarrow Q（置信度）$ 3. 确定性事实知识的产生式表示：一般用三元组表示：$(对象，属性，值)$ 或者 $(关系，对象1，对象2)$ 例如：老李的年龄是 40 岁，表示为$（Li，Age，40）$ 。老李和老王是朋友，表示为$（Friend, Li, Wang）$ 4. 不确定性事实知识的产生式表示：一般用四元组表示：$(对象，属性，值，置信度)$或者$(关系，对象1，对象2，置信度)$ 例如：老李的年龄很可能是 40 岁，表示为$（Li，Age，40，0.8）$ 。老李和老王是朋友，表示为$（Friend, Li, Wang，0.2）$ 习题2.5 产生式表达异或$(XOR)$逻辑。 点击查看答案 IF x1 = 0 AND x2 = 0 THEN y = 0 IF x1 = 1 AND x2 = 0 THEN y = 1 IF x1 = 0 AND x2 = 1 THEN y = 1 IF x1 = 1 AND x2 = 1 THEN y = 0 第三章1. 什么是子句、空子句？ 点击查看答案 子句：任何文字的析取式称为子句，任何文字本身也是子句。 空子句：不含有任何文字的子句。 空子句是永假的 2. 什么是确定性推理？ 点击查看答案 所谓确定性推理是指推理是所用的知识与证据都是确定的，推出的结论也是正确的，其真值或者为真或者为假，没有第三种情况出现。 3. 掌握谓词公式化为子句集的方法。谓词公式化为子句集的一般步骤：例3.3：将下列谓词公式化为子句集 ​ $(\\forall x){[\\neg P(x) \\vee \\neg Q(x)]\\rightarrow (\\exists y)[S(x,y)\\wedge Q(x)]}\\wedge(\\forall x)[P(x)\\vee B(x)]$ 消去蕴涵符号 $(\\forall x){\\neg [\\neg P(x) \\vee \\neg Q(x)]\\vee (\\exists y)[S(x,y)\\wedge Q(x)]}\\wedge(\\forall x)[P(x)\\vee B(x)]$ 把否定符号移到每个谓词的前面 $(\\forall x){ [ P(x) \\wedge Q(x)]\\vee (\\exists y)[S(x,y)\\wedge Q(x)]}\\wedge(\\forall x)[P(x)\\vee B(x)]$ 变量标准化 $(\\forall x){ [ P(x) \\wedge Q(x)]\\vee (\\exists y)[S(x,y)\\wedge Q(x)]}\\wedge(\\forall w)[P(w)\\vee B(w)]$ 消去存在量词设 y 的 Skolem 函数为 $f(x)$,则 $(\\forall x){ [ P(x) \\wedge Q(x)]\\vee [S(x,f(x))\\wedge Q(x)]}\\wedge(\\forall w)[P(w)\\vee B(w)]$ 化为前束型 $(\\forall x)(\\forall w){ {\\lbrack P(x)\\wedge Q(X)\\rbrack\\vee\\lbrack S(x,f(x))\\wedge Q(x)\\rbrack}\\wedge\\lbrack P(w)\\vee B(w)\\rbrack}$ 化为 Skolem 标准形 $(\\forall x)(\\forall w){ Q(x) \\wedge [S(x,f(x))\\vee P(x)]\\wedge[P(w)\\vee B(w)]}$ 略去全程量词 ${ Q(x) \\wedge [S(x,f(x))\\vee P(x)]\\wedge[P(w)\\vee B(w)]}$ 消去合取词，把母式用子句集表示 ${ Q(x) , S(x,f(x))\\vee P(x),P(w)\\vee B(w)}$ 子句变量标准化，即使每个子句中的变量符号不同 ${ Q(x) , S(y,f(y))\\vee P(y),P(w)\\vee B(w)}$ 例3.4 P63 习题3.4（3） p74 4. 熟练运用鲁滨逊归结原理进行归结反演以及求解问题例3.9 p68 例3.10 p69 例3.11 p70 例3.12 p71 习题3.7 3.10 P75 第四章1. 什么是不确定性推理、可信度、C-F模型？ 点击查看答案 不确定性推理： 从不确定性的初始证据出发，通过运用不确定性知识，最终推出具有一定程度的不确定性但是却是合理或者近乎合理的结论的思维过程。 可信度： 根据经验对一个事物或现象为真的信息程度称为可信度。 C-F模型： 基于可信度表示的不确定性推理的基本方法。 2. 熟练运用组合证据不确定性的算法合取 取小 析取 取大 3. 熟练运用结论不确定性的合成算法例4.1 P82 4. 掌握概率分配函数、信任函数、似然函数、概率分配函数的正交和P84-P6 5. 了解模糊集合的定义、表达方法P90-P91 6. 什么是隶属函数​ 隶属集合中所有的元素的隶属度全体构成模糊集合的隶属函数 7. 模糊集合的运算 模糊集合的包含关系 ​ 若$\\mu_A(x)\\geq\\mu_B(x)$，则$A\\ni B$ 模糊集合的相等关系 ​ 若$\\mu_A(x)=\\mu_B(x)$，则$A=B$ 模糊集合的交并补运算 交运算 $A\\cap B$ $\\mu_{A\\cap B}(x)=min\\lbrace\\mu_A(x),\\mu_B(x)\\rbrace=\\mu_A(x)\\wedge\\mu_B(x)$ 并运算 $A\\cup B$ $\\mu_{A\\cup B}(x)=max\\lbrace\\mu_A(x),\\mu_B(x)\\rbrace=\\mu_A(x)\\vee\\mu_B(x)$ 补运算 $\\overline A$ $\\mu_\\overline A(x)=1-\\mu_A(x)$ 模糊集合的代数运算 代数积： $ \\mu_{AB}(x)=\\mu_A(x)\\mu_B(x) $ 代数和 $\\mu_{A+B}(x)=\\mu_A(x)+\\mu_B(x)-\\mu_{AB}(x)$ 有界和 $\\mu_{A\\oplus B}(x)=min\\lbrace 1,\\mu_A(x)+\\mu_B(x)\\rbrace=1\\wedge\\lbrack\\mu_A(x)+\\mu_B(x)\\rbrack$ 有界积 $\\mu_{A\\otimes B}(x)=max\\lbrace 1,\\mu_A(x)+\\mu_B(x) \\rbrace=0\\vee\\lbrack\\mu_A(x)+\\mu_B(x)-1\\rbrack$ 8. 模糊关系的叉积表示P95 9. 模糊关系的合成P96 10. 模糊决策的三种方法P99 最大隶属度法 在模糊向量中，取隶属度最大的量作为推理结果。 加权平均判决法$$U=\\frac{\\displaystyle\\sum_{i=1}^n\\mu(u_i)u_i}{\\displaystyle\\sum_{i=1}^n\\mu(u_i)}$$例如：$$U’ = 0.1/2+0.6/3 + 0.5/4 + 0.4/5 + 0.2/6$$ $$U=\\frac{0.1\\times2+0.6\\times3+0.5\\times4+0.4\\times5+0.2\\times6}{0.1+0.6+0.5+0.4+0.2}$$ 中位数法 中位数法就是把模糊集的中位数作为系统控制变量。 当论域为有限离散点时，中位数$u^*$可以用下列公式求取$$\\sum_{u_1}^{u^\\ast}\\mu(u_i)=\\sum_{u^\\ast+1}^{u_n}\\mu(u_i);$$例如： $U’=0.1/-4+0.5/-3+0.1/-2+0.0/-1+0.1/0+0.2/1+0.4/2+0.5/3+0.1/4);$ 当$u^*=6$ 时，$\\sum_{u_1}^{u_6}\\mu(u_i)=\\sum_{u_7}^{u_9}\\mu(u_i)=1$，所以中位数为$u^* = u_6 = 1$，则$U=1$。 第五章1. 搜索的基本问题与主要过程搜索中需要解决的基本问题： 点击查看答案 1. 是否一定能找到一个解。 2. 找到的解是否是最佳解。 3. 时间与空间复杂性如何。 4. 是否终止运行或是否会陷入一个死循环。 搜索的主要过程： 点击查看答案 1. 从初始或目的状态出发，并将它作为当前状态。 2. 扫描操作算子集，将适用当前状态的一些操作算子作用于当前状态而得到新的状态，并建立指向其父结点的指针 。 3. 检查所生成的新状态是否满足结束状态，如果满足，则得到问题的一个解，并可沿着有关指针从结束状态反向到达开始状态，给出一解答路径；否则，将新状态作为当前状态，返回第(2)步再进行搜索。 2. 什么是状态空间 点击查看答案 利用状态变量和操作符号，表示系统或问题的有关知识的符号体系，状态空间是一个四元组： (S , O , S (0) , G) S ：状态集合。 O：操作算子的集合。 S ( ( (0))) ：包含问题的初始状态是 S 的非空子集。 G ：若干具体状态或满足某些性质的路径信息描述。 3. 什么是回溯决策 点击查看答案 从初始状态出发，不停地、试探性地寻找路径，直到它到达目的或“不可解结点”，即“死胡同”为止。若它遇到不可解结点就回溯到路径中最近的父结点上，查看该结点是否还有其他的子结点未被扩展。若有，则沿这些子结点继续搜索；如果找到目标，就成功退出搜索，返回解题路径。 4. 宽度优先搜索策略和深度优先搜索策略宽度优先搜索策略是按照下图所示的次序来搜索状态的，由$S$0生成状态1，2，然后扩展状态1，生成状态3，4，5，接着扩展状态2，生成状态6，7，8，该层扩展完后，再进入下一层，对状态3进行扩展，如此一层层地扩展下去，直到搜索到目的状态（如果目的状态存在）。 在实际的宽度优先搜索时，为了保存空间搜索的轨迹，用到了两个表：open表和closed表。open表与回溯算法中的NPS表相似，包含已经生成出来但其子状态未被搜索的状态。open表中状态的排列次序就是搜索的次序。closed表记录了已被生成扩展过的状态，它相当于回溯算法中PS表和NSS表的合并。 深度优先搜索策略是按下图所示的次序来搜索状态的。搜索从$S$0出发，沿一个方向一直扩展下去，如状态1，2，3，直到达到一定的深度（这里假定为3层）。如果未找到目的状态或无法再扩展时便回溯到另一条路径（状态4）继续搜索；若还未找到目的状态或无法再扩展时，再回溯到另一条路径（状态5，6）搜索…… 在深度优先搜索中，当搜索到某一个状态时，它所有的子状态以及子状态的后裔状态都必须先于该状态的兄弟状态被搜索。 为了保证找到解，应选择合适的深度限制值，或采取不断加大深度限制值的办法，反复搜索，直到找到解。 5. 启发信息和估价函数启发信息： 点击查看答案 在具体求解中，能够利用与该问题有关的信息来简化搜索过程，称此类信息为启发信息。 启发信息的分类： 点击查看答案 1. 陈述性启发信息 2. 过程性启发信息 3. 控制性启发信息 估价函数： 点击查看答案 估价函数的任务就是估计待搜索结点的“有希望”程度，并依次给它们排定次序（在open表中）。 估价函数 f(n) ：从初始结点经过 结点到达目的 结点的路径的最小代价估计值，其一般形式是f(n)=g(n)+h(n) 一般地，在 f(n) 中，g(n) 的比重越大，越倾向于宽度优先搜索方式，而h(n)的比重越大，表示启发性能越强。 ### 6. A搜索算法和A*搜索算法，并能解决八数码问题 A算法是基于估价函数的一种加权启发式图搜索算法，具体步骤如下： 把附有$f(S_0)$的初始结点$S_0$放入open表； 若open表为空，则搜索失败，退出； 移出open表中第一个结点N放入closed表中，并顺序编号n； 若目标结点把y附有$f(S_0)$的初始$S_g=N$，则搜索成功，结束； 若N不可扩展，则转步骤2； 扩展N，生成一组附有$f(x)$的子结点，对这组子结点做如下处理： ​ a.考察是否有已在open表或closed表中存在的结点。若有则再考察其中有无N的先辈结点，若有则删除之，对于其余结点也删除之，但由于它们又被第二次生成，因此需要考虑是否修改已经存在于open表中的这些结点及其后裔的返回指针和$f(x)$的值。修改原则是：选$f(x)$值小的路径走。 ​ b.为其余子结点配上指向N的返回指针后放入open表中，并对open表按$f(x)$值以升序排序，转步骤2。 A* 算法：定义$h^*(n)$为状态n到目的状态的最优路径的代价，则当A搜索算法的启发函数$h(n)$小于等于$h^*(n)$，即满足$$h(n)\\leq h^*(n)，对所有结点n$$时，被称为A*搜索算法。 八数码问题: P124 第六章1. 什么是进化算法 点击查看答案 进化算法是基于自然选择和自然遗传等生物进化机制的一种搜索算法 。 2. 什么是基本遗传算法？理解编码、初始群体的设定、适应度函数、选择、交叉、变异 点击查看答案 基本遗传算法是模拟达尔文生物进化论的自然选择和遗传学机理的生物进化过程的计算模型，是一种通过模拟自然进化过程搜索最优解的方法。 编码等见书135页。 3. 遗传算法的一般步骤、遗传算法的基本流程图一般步骤： 使用随机方法或者其他方法，产生一个有$N$个染色体的初始群体$pop(1)$,$t:=1$; 对群体中的每一个染色体$pop_i(t)$,计算其适应值$f_i=fitness(pop_i(t))$ 若满足停止条件，则算法停止；否则，以概率$p_i=f_i/\\sum_{j=1}^Nf_j$从$pop(t)$中随机选择一些染色体构成一个新种群$newpop(t+1)={pop_j(t)\\vert j=1,2,\\cdots,N}$ 以概率$p_c$进行交叉产生一些新的染色体，得到一个新的群体$crosspop(t+1)$。 以一个较小的概率$p_m$使染色体的一个基因发生变异，形成$mutpop(t+1)$;t:=t+1,成为一个新的群体$pop(t+1)$;返回Step 2。 流程图： 4. 什么是群智能算法 点击查看答案 群智能算法：受动物群体智能启发的算法。 5. 粒子群智能优化算法的基本原理 点击查看答案 PSO初始化为一群随机粒子，然后通过迭代找到最优解。在每一次迭代中，粒子通过跟踪两个“极值”来更新自己。第一个就是粒子本身所找到的最优解，这个解称为个体极值。另个一是整个种群目前找到的最优解，这个解称为全局极值。 6. 粒子群算法流程 初始化每个粒子。即在允许范围内随机设置每个粒子的初始位置和速度。 评价每个粒子的适应度。计算每个粒子的目标函数 设置每个粒子的$p^i$。对每个粒子，将其适应度与其经历过的最好的位置$p^i$进行比较，如果优于$p^i$，则将其作为该粒子的最好位置$p^i$。 设置全局最优值$p^g$。对每个粒子，将其适应度与群体经历过的最好位置$p^g$进行比较，如果优于$p^g$，则将其最为当前群体的最好位置$p^g$。 更新粒子的速度和位置。根据式(7.1)更新粒子的速度和位置。 检查终止条件。如果未达到设定条件(预设误差或者迭代的次数)，则返回第二步。 第七章1. 什么是专家系统？专家系统的特点定义： 点击查看答案 专家系统一种智能的计算机程序，他运用知识和推理来解决只有专家才能解决的复杂问题。 特点： 点击查看答案 1. 具有专家水平的专业知识 2. 能进行有效的推理 3. 具有启发性 4. 具有灵活性 5. 具有透明性 6. 具有交互性 2. 专家系统的工作原理 点击查看答案 专家系统的核心是知识库和推理机，其工作过程是根据知识库中的知识和用户提供的事实进行推理，不断地由已知的事实推出未知的结论即中间结果，并将中间结果放到数据库中，作为已知的新事实进行推理，从而把求解的问题由未知状态转化为已知状态。在专家系统的运行过程中，会不断地通过人机接口与用户进行交互，向用户提问，并向用户作出解释。 3. 专家系统的一般结构 点击查看答案 完整的专家系统一般应该包括 人机接口、推理机、知识库、数据库、知识获取机构、解释机构 六部分。 4. 什么是机器学习？了解机器学习的分类​ 定义： 点击查看答案 机器学习使计算机能模拟人的学习行为，自动的通过学习来获取知识和技能，不断改善性能，实现自我完善。 ​ 分类： 点击查看答案 1. 按学习方法分类 2. 按学习能力分类（监督学习、强化学习、非监督学习） 3. 按推理方式分类 4. 按综合属性分类 第八章1. 什么是人工神经网络？ 点击查看答案 一个用大量简单处理单元经广泛连接而组成的人工网络，是对人脑或者生物神经网络若干基本特性的抽象和模拟。 2. 了解神经网络的结构和工作方式结构： 前馈型（ 前向型） 反馈型 工作方式： 点击查看答案 同步（并行）方式：任一时刻神经网络中所有神经元同时调整状态。 异步（串行）方式：任一时刻只有一个神经元调整状态，而其它神经元的状态保持不变。 3. 掌握BP神经网络结构以及BP学习算法结构： 多层前馈型 BP学习算法： P214 BP 学习算法是通过反学习过程使误差最小，因此选择目标函数为$minJ=\\frac12\\sum_{j=1}^{P_m}{(y_j^m-y_{xj})}^2$即选择神经网络权值使输出期望出$y_{sj}$与神经网络实际输出$y_j^m$之差的平方和最小。 4. 了解BP算法程序流程图 5. 掌握离散型Hopfield神经网络P221 6. 会利用Hopfield神经网络设计分类器P238 例8.2","categories":[{"name":"复习","slug":"复习","permalink":"https://gleaming.cn/categories/%E5%A4%8D%E4%B9%A0/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://gleaming.cn/tags/AI/"}]},{"title":"Java实现十大经典排序算法","slug":"Java实现十大经典排序算法","date":"2020-11-28T08:31:32.000Z","updated":"2022-07-28T07:52:51.993Z","comments":true,"path":"Ten-classic-sorting-algorithms/","link":"","permalink":"https://gleaming.cn/Ten-classic-sorting-algorithms/","excerpt":"","text":"排序算法分类 内部排序：指在排序期间，元素全部存放在内存中的排序，常见的内部排序算法有：冒泡排序、选择排序、插入排序、希尔排序、归并排序、快速排序、堆排序、基数排序等。 外部排序：指在排序期间，元素无法完全全部同时存放在内存中，必须在排序的过程中根据要求不断地在内、外存之间移动的排序。 比较类排序：：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 常见的非比较类排序算法有：基数排序、计数排序、桶排序。 一般情况下，内部排序算法在执行过程中都要进行两种操作：比较和移动。通过比较两个关键字的大小，确定对应元素的前后关系，然后通过移动元素以达到有序。但是，并非所有的内部排序算法都要基于比较操作。 每种排序算法都有各自的优缺点，适合在不同的环境下使用，就其全面性能而言，很难提出一种被认为是最好的算法。通常可以将排序算法分为插入排序、交换排序、选择排序、归并排序和基数排序五大类，内部排序算法的性能取决于算法的时间复杂度和空间复杂度，而时间复杂度一般是由比较和移动的次数决定的。 一、冒泡排序 (Bubble Sort)1.原理每次检查相邻的两个元素，如果前面的元素与后面的元素满足给定的排序条件，就将相邻两个元素交换。当没有相邻元素需要交换时，排序就完成了。 经过$i$次扫面后，数列的末尾$i$项必然是最大的$i$项，因此冒泡排序最多需要扫描$n-1$遍数组就能完成扫描。 2.步骤 ① 比较相邻的元素。如果第一个比第二个大，就交换它们两个； ② 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； ③ 针对所有的元素重复步骤 ① ~ ②，除了最后一个元素，直到排序完成。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O(\\mathrm N^2)$最佳时间复杂度：$\\mathrm O(\\mathrm N)$最差时间复杂度：$\\mathrm O(\\mathrm N^2)$空间复杂度：$\\mathrm O(\\mathrm 1)$排序方式：In-place稳定性：稳定 5.代码12345678910public void bubbleSort(int arr[]) &#123; for (int i = 0; i &lt; arr.length; i++) for (int j = 0; j &lt; arr.length - 1; j++) if (arr[j - 1] &gt; arr[j]) &#123; int temp; temp = arr[j - 1]; arr[j - 1] = arr[j]; arr[j] = temp; &#125; &#125; 冒泡排序还有一种优化算法，就是立一个 flag，当某一趟序列遍历中元素没有发生交换，则证明该序列已经有序，就不再进行后续的排序。动画演示里就是改进后的算法，改进后的代码如下： 123456789101112131415public void bubbleSort(int arr[]) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; boolean flag = true; for (int j = 0; j &lt; arr.length - 1; j++) if (arr[j - 1] &gt; arr[j]) &#123; int temp; temp = arr[j - 1]; arr[j - 1] = arr[j]; arr[j] = temp; flag = false; &#125; if (flag) return; &#125; &#125; 二、选择排序 (Selection Sort)1.原理每次找出第$i$小的元素 (也就是$A_{i..n}$中最小的元素)，然后将这个元素与数组第$i$个位置上的元素交换。 2.步骤 ① 首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置； ② 再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾； ③ 重复步骤 ②，直到所有元素均排序完毕。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O(\\mathrm N^2)$最佳时间复杂度：$\\mathrm O(\\mathrm N^2)$最差时间复杂度：$\\mathrm O(\\mathrm N^2)$空间复杂度：$\\mathrm O(\\mathrm 1)$排序方式：In-place稳定性：不稳定 5.代码1234567891011public void selectionSort(int arr[]) &#123; for (int i = 0; i &lt; arr.length - 1; i++) &#123; int min = i; for (int j = i + 1; j &lt; arr.length; j++) if (arr[min] &gt; arr[j]) min = j; int temp = arr[min]; arr[min] = arr[i]; arr[i] = temp; &#125; &#125; 三、插入排序 (Insertion Sort)1.原理将排列元素划分为“已排序”和”未排序”两部分，每次从”未排序的”元素中选择一个插入到”已排序”的元素中的正确位置。 一个与插入排序相同的操作是打扑克牌时，从牌桌上抓一张牌。按牌面大小插到手牌后，再抓下一张牌。 2.步骤 ① 从第一个元素开始，该元素可以认为已经被排序； ② 取出下一个元素，在已经排序的元素序列中从后向前扫描； ③ 如果该元素（已排序的）大于新元素，将该元素往右移到下一位置，重复该步骤，直到找到已排序的元素小于或者等于新元素的位置； ④ 将新元素插入到步骤 ③ 找到的位置的后面； ⑤ 重复步骤 ② ~ ④。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O(\\mathrm N^2)$最差时间复杂度：$\\mathrm O(\\mathrm N^2)$空间复杂度：$\\mathrm O(\\mathrm 1)$排序方式：In-place稳定性：稳定 5.代码12345678910111213141516171819public void insertSort(int arr[]) &#123; if(arr == null || arr.length &lt;= 0)&#123; return; &#125; for(int i = 1; i &lt; arr.length; i++)&#123; int tmp = arr[i]; int j; for(j = i-1; j &gt;= 0; j--)&#123; //如果比tmp大把值往后移动一位 if(arr[j] &gt; tmp)&#123; arr[j+1] = arr[j]; &#125; else&#123; break; &#125; &#125; arr[j+1] = tmp; &#125; &#125; 四、希尔排序 (Shell Sort)1.原理排序对不相邻的记录进行比较和移动： 将待排序序列分为若干子序列（每个子序列的元素在原始数组中间距相同） 对这些子序列进行插入排序； 减小每个子序列中元素之间的间距，重复上述过程直至间距减少为 1。 2.步骤 ① n 为数组长度，首先取一个整数$d1=n/2$，将元素分为$d1$ 个组，每组相邻量元素之间距离为$d1-1$，在各组内进行直接插入排序； ② 取第二个整数$d2=d1/2$，重复步骤 ① 分组排序过程，直到$di=1$，即所有元素在同一组内进行直接插入排序。 PS：希尔排序每趟并不使某些元素有序，而是使整体数据越来越接近有序；最后一趟排序使得所有数据有序。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$最佳时间复杂度：最差时间复杂度：$\\mathrm O(\\mathrm N^2)$空间复杂度：$\\mathrm O(\\mathrm 1)$稳定性：不稳定复杂性：较复杂 5.代码12345678910111213public void shellSort(int arr[]) &#123; int gap = 1; int j; while (gap &lt; arr.length / 3) gap = gap * 3 + 1; for (; gap &gt; 0; gap /= 3) for (int i = gap; i &lt; arr.length; i++) &#123; int temp = arr[i]; for (j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= gap) arr[j + gap] = arr[j]; arr[j + gap] = temp; &#125; &#125; 五、归并排序 (Merge Sort)1.原理归并排序分为三个步骤： 将数列划分为两部分； 递归地分别对两个子序列进行归并排序； 合并两个子序列。 不难发现，归并排序的前两步都很好实现，关键是如何合并两个子序列。注意到两个子序列在第二步中已经保证了都是有序的了，第三步中实际上是想要把两个 有序 的序列合并起来。 2.步骤 归并的基本步骤 ① 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列； ② 设定两个指针，最初位置分别为两个已经排序序列的起始位置； ③ 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置； ④ 重复步骤 ③ 直到某一指针达到序列尾； ⑤ 将另一序列剩下的所有元素直接复制到合并序列尾。 归并排序的步骤 ① 分解：将列表越分越小，直至分成一个元素，终止条件：一个元素是有序的。 ② 合并：不断将两个有序列表进行归并，列表越来越大，直到所有序列归并完毕。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$最佳时间复杂度：$\\mathrm O(\\mathrm N)$最差时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$空间复杂度：$\\mathrm O(\\mathrm N)$排序方式：In-place稳定性：稳定 5.代码迭代法： 12345678910111213141516171819202122232425262728293031public void mergeSort(int arr[]) &#123; int len = arr.length; int[] result = new int[len]; int block, start; // 原版代码的迭代次数少了一次，没有考虑到奇数列数组的情况 for (block = 1; block &lt; len * 2; block *= 2) &#123; for (start = 0; start &lt; len; start += 2 * block) &#123; int low = start; int mid = (start + block) &lt; len ? (start + block) : len; int high = (start + 2 * block) &lt; len ? (start + 2 * block) : len; //两个块的起始下标及结束下标 int start1 = low, end1 = mid; int start2 = mid, end2 = high; //开始对两个block进行归并排序 while (start1 &lt; end1 &amp;&amp; start2 &lt; end2) &#123; result[low++] = arr[start1] &lt; arr[start2] ? arr[start1++] : arr[start2++]; &#125; while (start1 &lt; end1) &#123; result[low++] = arr[start1++]; &#125; while (start2 &lt; end2) &#123; result[low++] = arr[start2++]; &#125; &#125; int[] temp = arr; arr = result; result = temp; &#125; result = arr; &#125; 递归法： 123456789101112131415161718192021222324static void mergeSortRecursive(int[] arr, int[] result, int start, int end) &#123; if (start &gt;= end) return; int len = end - start, mid = (len &gt;&gt; 1) + start; int start1 = start, end1 = mid; int start2 = mid + 1, end2 = end; mergeSortRecursive(arr, result, start1, end1); mergeSortRecursive(arr, result, start2, end2); int k = start; while (start1 &lt;= end1 &amp;&amp; start2 &lt;= end2) result[k++] = arr[start1] &lt; arr[start2] ? arr[start1++] : arr[start2++]; while (start1 &lt;= end1) result[k++] = arr[start1++]; while (start2 &lt;= end2) result[k++] = arr[start2++]; for (k = start; k &lt;= end; k++) arr[k] = result[k]; &#125; public static void mergeSort(int[] arr) &#123; int len = arr.length; int[] result = new int[len]; mergeSortRecursive(arr, result, 0, len - 1); &#125; 六、快速排序 (Quick sort)1.原理通过分治的方式来将一个数组排序。 快速分为三个过程： 将数列划分为两部分 (要保证相对大小关系); 递归到连个子序列中分别进行快速排序; 不用合并，因为此时数列已经完全有序。 和归并排序不同，第一步并不是直接分成前后两个序列，而是在分的过程中要保证相对大小关系。具体来说，第一步是要把数列分成两个部分，然后保证前一个子数列中的数都小于后一个子数列中的数。为了保证平均时间复杂度，一般是随机选择一个数$m$来当作两个子数列的分界。 之后，维护一前一后两个指针$p$和$q$，依次考虑当前的数是否放在了应该放的位置 (前还是后)。如果当前的数没放对，比如说如果后面的指针$q$遇到了一个比$m$小的数，那么可以交换$p$和$q$位置上的数，再把$p$向后移一位。当前的数的位置全放对后，再移动指针继续处理，直到两个指针相遇。 2.步骤 ① 从数列中挑出一个元素，称为 “基准值”; ② 重新排序数列，所有元素比基准值小的放在基准值的左边，比基准值大的放在基准值的右边（相同的数可以到任一边）。在这个分区退出之后，该基准值就处于数列的中间位置。这个称为分区（partition）操作，也可以称为一次归位操作，归位操作的过程见下动图； ③ 递归地把小于基准值元素的子数列和大于基准值元素的子数列按照步骤 ① ② 排序。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$最佳时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$最差时间复杂度：$\\mathrm O(\\mathrm N^2)$空间复杂度：根据实现方式的不同而不同 5.代码12345678910111213141516171819202122232425262728293031class quickSort &#123; int[] arr; private void swap(int x, int y) &#123; int temp = arr[x]; arr[x] = arr[y]; arr[y] = temp; &#125; private void quick_sort_recursive(int start, int end) &#123; if (start &gt;= end) return; int mid = arr[end]; int left = start, right = end - 1; while (left &lt; right) &#123; while (arr[left] &lt;= mid &amp;&amp; left &lt; right) left++; while (arr[right] &gt;= mid &amp;&amp; left &lt; right) right--; swap(left, right); &#125; if (arr[left] &gt;= arr[end]) swap(left, end); else left++; quick_sort_recursive(start, left - 1); quick_sort_recursive(left + 1, end); &#125; public void sort(int[] arrin) &#123; arr = arrin; quick_sort_recursive(0, arr.length - 1); &#125;&#125; 七、堆排序 (Heap Sort)1.原理堆排序是指利用堆这种数据结构所设计的一种排序算法。堆是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 堆：一种特殊的完全二叉树结构 大根堆：一棵完全二叉树，满足任一节点都比其孩子节点大 小根堆：一棵完全二叉树，满足任一节点都比其孩子节点小 2.步骤 ① 构建堆：将待排序序列构建成一个堆 H[0……n-1]，从最后一个非叶子结点开始，从左至右，从下至上进行调整。根据升序或降序需求选择大顶堆或小顶堆； ② 此时的堆顶元素，为最大或者最小元素； ③ 把堆顶元素和堆尾元素互换，调整堆，重新使堆有序； ④ 此时堆顶元素为第二大元素； ⑤ 重复以上步骤，直到堆变空。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$最佳时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$最差时间复杂度：$\\mathrm O({\\mathrm{Nlog}}_2\\mathrm N)$稳定性：不稳定 5.代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import java.util.Arrays;public class HeapSort &#123; private int[] arr; public HeapSort(int[] arr)&#123; this.arr = arr; &#125; /** * 堆排序的主要入口方法，共两步。 */ public void sort()&#123; /* * 第一步：将数组堆化 * beginIndex = 第一个非叶子节点。 * 从第一个非叶子节点开始即可。无需从最后一个叶子节点开始。 * 叶子节点可以看作已符合堆要求的节点，根节点就是它自己且自己以下值为最大。 */ int len = arr.length - 1; int beginIndex = (len - 1) &gt;&gt; 1; for(int i = beginIndex; i &gt;= 0; i--)&#123; maxHeapify(i, len); &#125; /* * 第二步：对堆化数据排序 * 每次都是移出最顶层的根节点A[0]，与最尾部节点位置调换，同时遍历长度 - 1。 * 然后从新整理被换到根节点的末尾元素，使其符合堆的特性。 * 直至未排序的堆长度为 0。 */ for(int i = len; i &gt; 0; i--)&#123; swap(0, i); maxHeapify(0, i - 1); &#125; &#125; private void swap(int i,int j)&#123; int temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; &#125; /** * 调整索引为 index 处的数据，使其符合堆的特性。 * * @param index 需要堆化处理的数据的索引 * @param len 未排序的堆（数组）的长度 */ private void maxHeapify(int index,int len)&#123; int li = (index &lt;&lt; 1) + 1; // 左子节点索引 int ri = li + 1; // 右子节点索引 int cMax = li; // 子节点值最大索引，默认左子节点。 if(li &gt; len) return; // 左子节点索引超出计算范围，直接返回。 if(ri &lt;= len &amp;&amp; arr[ri] &gt; arr[li]) // 先判断左右子节点，哪个较大。 cMax = ri; if(arr[cMax] &gt; arr[index])&#123; swap(cMax, index); // 如果父节点被子节点调换， maxHeapify(cMax, len); // 则需要继续判断换下后的父节点是否符合堆的特性。 &#125; &#125; 八、计数排序 (Counting Sort)1.原理使用一个额外的数组 $C$，其中第$i$ 个元素是待排序数组$A$中值等于$i$的元素的个数，然后根据数组$C$来将$A$中的元素排到正确的位置。 他的工作过程分为三个步骤： 计算每个数出现了几次； 求出每个数的前缀和； 利用出现次数的前缀和，从右至左计算每个数的排名。 2.步骤 ① 找到待排序列表中的最大值 k，开辟一个长度为 k+1 的计数列表，计数列表中的值都为 0。 ② 遍历待排序列表，如果遍历到的元素值为 i，则计数列表中索引 i 的值加1。 ③ 遍历完整个待排序列表，计数列表中索引 i 的值 j 表示 i 的个数为 j，统计出待排序列表中每个值的数量。 ④ 创建一个新列表（也可以清空原列表，在原列表中添加），遍历计数列表，依次在新列表中添加 j 个 i，新列表就是排好序后的列表，整个过程没有比较待排序列表中的数据大小。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$最佳时间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$最差时间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$空间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$ 5.代码123456789101112131415161718192021222324252627282930313233public class CountingSort &#123; public static void main(String[] argv) &#123; int[] A = CountingSort.countingSort(new int[]&#123;16, 4, 10, 14, 7, 9, 3, 2, 8, 1&#125;); Utils.print(A); &#125; public static int[] countingSort(int[] A) &#123; int[] B = new int[A.length]; // 假设A中的数据a&#x27;有，0&lt;=a&#x27; &amp;&amp; a&#x27; &lt; k并且k=100 int k = 100; countingSort(A, B, k); return B; &#125; private static void countingSort(int[] A, int[] B, int k) &#123; int[] C = new int[k]; // 计数 for (int j = 0; j &lt; A.length; j++) &#123; int a = A[j]; C[a] += 1; &#125; Utils.print(C); // 求计数和 for (int i = 1; i &lt; k; i++) &#123; C[i] = C[i] + C[i - 1]; &#125; Utils.print(C); // 整理 for (int j = A.length - 1; j &gt;= 0; j--) &#123; int a = A[j]; B[C[a] - 1] = a; C[a] -= 1; &#125; &#125;&#125; 九、桶排序 (Bucket Sort)1.原理桶排序又叫箱排序，工作的原理是将数组分到有限数量的桶子里。每个桶子再个别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序）。桶排序是鸽巢排序的一种归纳结果。 桶排序也是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。为了使桶排序更加高效，我们需要做到这两点： 在额外空间充足的情况下，尽量增大桶的数量； 使用的映射函数能够将输入的 N 个数据均匀的分配到 K 个桶中。 同时，对于桶中元素的排序，选择何种比较排序算法对于性能的影响至关重要。 最快情况：当输入的数据可以均匀的分配到每一个桶中； 最慢情况：当输入的数据被分配到了同一个桶中。 2.步骤 ① 创建一个定量的数组当作空桶子； ② 遍历序列，把元素一个一个放到对应的桶子去； ③ 对每个不是空的桶子进行排序； ④ 从不是空的桶子里把元素再放回原来的序列中。 3.动画演示 4.性能分析 平均时间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$最佳时间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$最差时间复杂度：$\\mathrm O(\\mathrm N^2)$空间复杂度：$\\mathrm O(\\mathrm N\\ast\\mathrm K)$稳定性：稳定 5.代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class BucketSort &#123; /** * 桶排序 * @param arr * @return */ public static double[] bucketSort(double[] arr)&#123; //1.计算出最大值和最小值，求出两者的差值 double min = arr[0]; double max = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; if (max &lt; arr[i])&#123; max = arr[i]; &#125; if (arr[i] &lt; min)&#123; min = arr[i]; &#125; &#125; double d = max - min; //2.初始化桶 int bucketNum = arr.length; List&lt;LinkedList&lt;Double&gt;&gt; bucketList = new ArrayList&lt;&gt;(bucketNum); for (int i = 0; i &lt; bucketNum; i++) &#123; bucketList.add(new LinkedList&lt;&gt;()); &#125; //3.遍历数组中的元素，把所有元素都放入对应的桶当中 for (int i = 0; i &lt; arr.length; i++) &#123; //计算当前元素应该放在哪个桶里面 int num = (int)((arr[i] - min) / (d / (bucketNum - 1))); bucketList.get(num).add(arr[i]); &#125; //4.对每个桶里面的元素进行排序 for (int i = 0; i &lt; bucketNum; i++) &#123; Collections.sort(bucketList.get(i)); &#125; //5.输出全部元素 int k = 0; for(LinkedList&lt;Double&gt; doubles : bucketList)&#123; for (Double aDouble : doubles) &#123; arr[k] = aDouble; k++; &#125; &#125; return arr; &#125;&#125; 十、基数排序 (Radix Sort)1.原理基数排序属于分配式排序，是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 基数排序、计数排序、桶排序三种排序算法都利用了桶的概念，但对桶的使用方法上是有明显差异的： 基数排序：根据键值的每位数字来分配桶； 计数排序：每个桶只存储单一键值； 桶排序：每个桶存储一定范围的数值。 2.步骤 ① 取数组中的最大数，并取得位数； ② 从最低位开始，依次进行一次排序； ③ 从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 3.动画演示 4.性能分析 时间复杂度：$\\mathrm O(\\mathrm N\\ast\\mathrm K)$空间复杂度：$\\mathrm O(\\mathrm N+\\mathrm K)$稳定性：稳定 5.代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class RadixSort&#123; public static void radixSort(int[] arr) &#123; int maxValue = arr[0]; int minValue = arr[0]; for (int i = 1; i &lt; arr.length; i++) &#123; maxValue = Math.max(maxValue, arr[i]); minValue = Math.min(minValue, arr[i]); &#125; // 如果最小值为负数，则数组中的数全部减去最小值，这样能保证数组中最小数是0 if (minValue &lt; 0) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] -= minValue; &#125; // 最大值也需要减，不然可能出现位数发生变化 maxValue -= minValue; &#125; int length = String.valueOf(maxValue).length(); // 表示10个桶 int[][] bucket = new int[10][arr.length]; // 记录每个桶实际存放几个数据，比如：bucketElementCount[0]记录着bucket[0]桶放入数据个数 int[] bucketElementCount = new int[10]; for (int i = 0, j = 1; i &lt; length; i++, j *= 10) &#123; for (int k = 0; k &lt; arr.length; k++) &#123; int element = arr[k] / j % 10; bucket[element][bucketElementCount[element]] = arr[k]; bucketElementCount[element]++; &#125; int index = 0; // 遍历每个桶的数量，放入原数组 for (int n = 0; n &lt; bucketElementCount.length; n++) &#123; if (bucketElementCount[n] != 0) &#123; for (int t = 0; t &lt; bucketElementCount[n]; t++) &#123; arr[index++] = bucket[n][t]; &#125; &#125; // 需要把bucketElementCount置为0，不然会出错 bucketElementCount[n] = 0; &#125; &#125; if (minValue &lt; 0) &#123; for (int i = 0; i &lt; arr.length; i++) &#123; arr[i] += minValue; &#125; &#125; &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"算法/排序","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F/"}],"tags":[{"name":"排序","slug":"排序","permalink":"https://gleaming.cn/tags/%E6%8E%92%E5%BA%8F/"}]},{"title":"快速幂","slug":"快速幂","date":"2020-09-26T11:02:22.000Z","updated":"2022-07-28T07:52:51.994Z","comments":true,"path":"Fast-Power/","link":"","permalink":"https://gleaming.cn/Fast-Power/","excerpt":"","text":"算法描述快速幂是一个$\\theta$($logn$)的时间内计算$x^{n}$的小技巧，而暴力的计算需要$\\theta$($n$)的时间。而这个技巧也常常用在非计算的场景，因为它可以应用在任何具有结合律的运算中。计算$\\alpha$的n次方就是将n个$\\alpha$乘在一起，然而当$\\alpha$，n太大的时候，这种方法就不太实用。不过我们知道$a^{b+c}=a^b\\ast a^c$,$a^{2b}=a^b\\ast a^b$.二进制取幂的想法是，我们将取幂的任务按照指数的二进制表示来分割成更小的任务。 举个例子$3^{13}=3^{(1101)_2}=3^8\\ast3^4\\ast3^1$. 代码实现递归算法12345678long long binpow(long long a, long long b) &#123; if (b == 0) return 1; long long res = binpow(a, b / 2); if (b % 2) return res * res * a; else return res * res;&#125; 非递归12345678910long long binpow(long long a, long long b) &#123; long long res = 1; while (b &gt; 0) &#123; if (b &amp; 1) res = res * a; a = a * a; b &gt;&gt;= 1; &#125; return res;&#125; 模板：Luogu 应用模意义下取幂计算$x^n mod$$m$ 这是一个非常常见的应用，既然我们知道取模运算不会干涉乘法运算，因此我们只需要在计算的过程中取模即可。 12345678910long long binpow(long long a, long long b, long long m) &#123; a %= m; long long res = 1; while (b &gt; 0) &#123; if (b &amp; 1) res = res * a % m; a = a * a % m; b &gt;&gt;= 1; &#125; return res;&#125; 计算斐波那契数列计算斐波那契数列的第n项 根据斐波那契数列的递推式$F_n=F_{n-1}+F_{n-2}$，我们可以构建一个2*2的矩阵来表示从$F_i$,$F_{i+1}$,$F_{i+2}$的变换。于是在计算这个矩阵的n次幂的时候，我们使用快速幂可以在更快的时间计算出结果。 高精度快速幂 题目大意：从文件中输入 P（1000&lt;P&lt;3100000），计算 的最后 100 位数字（用十进制高精度数表示），不足 100 位时高位补 0。麦森数：题目链接 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;bits/stdc++.h&gt;using namespace std;int a[505], b[505], t[505], i, j;int mult(int x[], int y[]) // 高精度乘法&#123; memset(t, 0, sizeof(t)); for (i = 1; i &lt;= x[0]; i++) &#123; for (j = 1; j &lt;= y[0]; j++) &#123; if (i + j - 1 &gt; 100) continue; t[i + j - 1] += x[i] * y[j]; t[i + j] += t[i + j - 1] / 10; t[i + j - 1] %= 10; t[0] = i + j; &#125; &#125; memcpy(b, t, sizeof(b));&#125;void ksm(int p) // 快速幂&#123; if (p == 1) &#123; memcpy(b, a, sizeof(b)); return; &#125; ksm(p / 2); mult(b, b); if (p % 2 == 1) mult(b, a);&#125;int main() &#123; int p; scanf(&quot;%d&quot;, &amp;p); a[0] = 1; a[1] = 2; b[0] = 1; b[1] = 1; ksm(p); for (i = 100; i &gt;= 1; i--) &#123; if (i == 1) &#123; printf(&quot;%d\\n&quot;, b[i] - 1); &#125; else printf(&quot;%d&quot;, b[i]); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"数学","slug":"算法/数学","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/"}],"tags":[{"name":"数学","slug":"数学","permalink":"https://gleaming.cn/tags/%E6%95%B0%E5%AD%A6/"}]},{"title":"Java学习路线","slug":"Java学习路线","date":"2020-09-12T06:06:16.000Z","updated":"2022-07-28T07:52:51.993Z","comments":true,"path":"Java-learning-route/","link":"","permalink":"https://gleaming.cn/Java-learning-route/","excerpt":"","text":"路线阶段一Java基础，常用类、集合、IO，GUI编程，网络编程，多线程基础，注解和反射，JVM入门。 阶段二前端基础，HTML，CSS，JavaScript，jQuery。 阶段三MySQL数据库。 阶段四JavaWeb。 阶段五首先学习MyBatis，Spring，SpringMVC。 阶段六微服务入门，首先是Vue以及前端进阶、webpack、Nodejs，然后Spring Boot，文档交互Swagger，微服务治理SpringCloud。 关于面试对于面试我了解的并不多，简单的写一下我现在所知道的，后续我会持续修改。 首先学习MyBatis，Spring，SpringMVC。 缓存：Redis。 搜索增强：ES(ElasticSearch) 后续的话：JVM进阶，框架底层源码分析，23种设计模式，数据结构，常用算法，计算机网络，计算机系统等等。 总结这是我已知的最简路线了，学习Java的路要比着长的多，我们要慢慢探索，希望自己能找到心想工作，共勉~","categories":[{"name":"Java","slug":"Java","permalink":"https://gleaming.cn/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://gleaming.cn/tags/Java/"},{"name":"学习","slug":"学习","permalink":"https://gleaming.cn/tags/%E5%AD%A6%E4%B9%A0/"}]},{"title":"Hello World","slug":"Hello-World","date":"2020-09-11T11:02:22.000Z","updated":"2022-07-28T07:52:51.993Z","comments":true,"path":"Hello-World/","link":"","permalink":"https://gleaming.cn/Hello-World/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"K8S","slug":"K8S","permalink":"https://gleaming.cn/categories/K8S/"},{"name":"春招","slug":"春招","permalink":"https://gleaming.cn/categories/%E6%98%A5%E6%8B%9B/"},{"name":"复习","slug":"复习","permalink":"https://gleaming.cn/categories/%E5%A4%8D%E4%B9%A0/"},{"name":"Redis","slug":"Redis","permalink":"https://gleaming.cn/categories/Redis/"},{"name":"源码","slug":"源码","permalink":"https://gleaming.cn/categories/%E6%BA%90%E7%A0%81/"},{"name":"JVM","slug":"JVM","permalink":"https://gleaming.cn/categories/JVM/"},{"name":"Java","slug":"Java","permalink":"https://gleaming.cn/categories/Java/"},{"name":"算法","slug":"算法","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/"},{"name":"排序","slug":"算法/排序","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/%E6%8E%92%E5%BA%8F/"},{"name":"数学","slug":"算法/数学","permalink":"https://gleaming.cn/categories/%E7%AE%97%E6%B3%95/%E6%95%B0%E5%AD%A6/"}],"tags":[{"name":"Go","slug":"Go","permalink":"https://gleaming.cn/tags/Go/"},{"name":"数据库","slug":"数据库","permalink":"https://gleaming.cn/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"网络","slug":"网络","permalink":"https://gleaming.cn/tags/%E7%BD%91%E7%BB%9C/"},{"name":"JVM","slug":"JVM","permalink":"https://gleaming.cn/tags/JVM/"},{"name":"OS","slug":"OS","permalink":"https://gleaming.cn/tags/OS/"},{"name":"AI","slug":"AI","permalink":"https://gleaming.cn/tags/AI/"},{"name":"排序","slug":"排序","permalink":"https://gleaming.cn/tags/%E6%8E%92%E5%BA%8F/"},{"name":"数学","slug":"数学","permalink":"https://gleaming.cn/tags/%E6%95%B0%E5%AD%A6/"},{"name":"Java","slug":"Java","permalink":"https://gleaming.cn/tags/Java/"},{"name":"学习","slug":"学习","permalink":"https://gleaming.cn/tags/%E5%AD%A6%E4%B9%A0/"}]}